{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bbb004-5a22-4f65-b400-cb2bc3edf217",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017705b0-281d-4565-abe1-852ebc5ca888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else: \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "    global spark\n",
    "    global sc\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "        spark.stop()\n",
    "        del spark\n",
    "        del sc\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06343f5-a638-4f7f-ade5-5f564a068bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/10/10 17:48:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/10 17:48:35 WARN Utils: Service 'sparkDriver' could not bind on port 7077. Attempting port 7078.\n",
      "25/10/10 17:48:35 WARN Utils: Service 'sparkDriver' could not bind on port 7078. Attempting port 7079.\n",
      "25/10/10 17:48:36 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/10/10 17:48:36 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4050\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1760071715154</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7079</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>32</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-5337068df9b249dcb54180a5db833018</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-4677fa99cc730cf4</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1760071715270</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2025-09-21T20:54:03Z&se=2026-12-31T04:09:03Z&spr=https&sv=2024-11-04&sr=c&sig=5V91JeJe9mD%2FuPKUQ3LCErJh%2FwP0gNYoyl8MMx5pdkM%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d99403-8a11-4bfe-aaf3-63492d4726c9",
   "metadata": {},
   "source": [
    "### -  –––––––––––––––––––– Assignment 2 begins here ––––––––––––––––––––- - ###\n",
    "\n",
    "- MSD containers:\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/` \n",
    "\n",
    "- MY containers:\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31c204-813f-4003-9d21-c3b9c57a065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Imports\n",
    "\n",
    "# group 1: from imports (alphabetical by module)\n",
    "from datetime            import datetime\n",
    "from IPython.display     import display, Image\n",
    "from math                import acos, atan2, cos, radians, sin, sqrt\n",
    "from matplotlib.ticker   import FuncFormatter, MaxNLocator\n",
    "from pathlib             import Path\n",
    "from pyspark.sql         import DataFrame, DataFrame as SparkDF\n",
    "from pyspark.sql         import functions as F, types as T\n",
    "from pyspark.sql.types   import *\n",
    "from pyspark.sql.utils   import AnalysisException\n",
    "from pyspark.sql.window  import Window\n",
    "from rich.console        import Console\n",
    "from rich.tree           import Tree\n",
    "from time                import perf_counter\n",
    "from typing              import List, Optional, Tuple\n",
    "\n",
    "# group 2: import ... as ... (alphabetical)\n",
    "import itertools         as it\n",
    "import matplotlib.dates  as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "\n",
    "# group 3: import statements (alphabetical)\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import platform\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "console = Console()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb102f08-d26f-4475-8561-7f63862da060",
   "metadata": {},
   "source": [
    "#The following shows the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812c8751-d59a-423b-8f39-d5e6a540944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.1\n",
      "___________________________________PATHS___________________________________\n",
      "WASBS_DATA          : wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/\n",
      "WASBS_USER          : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overall time metric\n",
    "notebook_run_time = time.time()\n",
    "\n",
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "#USERNAME    = \"dew59\"\n",
    "WASBS_DATA  = \"wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/\"\n",
    "WASBS_USER  = f\"wasbs://campus-user@madsstorage002.blob.core.windows.net/{username}-A2/\"\n",
    "\n",
    "#WASBS_USER          = \"wasbs://campus-user@madsstorage002.blob.core.windows.net/{}\".format(USERNAME)\n",
    "#WASBS_YEAR_SIZE     = \"{}/years_size_metrics.parquet/\".format(WASBS_USER)\n",
    "\n",
    " \n",
    "#stations_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/{stations_write_path}'\n",
    "#common_data_path    = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/'\n",
    "#stations_read_name  =  inventory_read_name = \"\"\n",
    "#stations_read_name  =  inventory_read_name = \"\"\n",
    " \n",
    "\n",
    "print(\"Spark:\", spark.version)\n",
    "print(\"_\" * 35 + \"PATHS\" + \"_\" * 35)\n",
    "print(\"WASBS_DATA          :\", WASBS_DATA)\n",
    "print(\"WASBS_USER          :\", WASBS_USER) \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "839b56fb-a403-4903-ba60-b0f33495ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________HELPER / DIAGNOSTIC FUNCTIONS___________________________________\n"
     ]
    }
   ],
   "source": [
    "# HELPER AND DIAGNOSTIC FUNCTIONS\n",
    "\n",
    "notebook_run_time = time.time()\n",
    "print(\"_\" * 35 + \"HELPER / DIAGNOSTIC FUNCTIONS\" + \"_\" * 35)\n",
    "\n",
    "def hprint(text: str=\"\", l=50):\n",
    "    \"\"\"Print formatted section header\"\"\"\n",
    "    n = len(text)\n",
    "    n = abs(n - l) // 2\n",
    "    print(\"\\n\" + \"_\" * n + text + \"_\" * n)\n",
    "\n",
    "def cleanup_parquet_files(cleanup=False):\n",
    "    \"\"\"Clean up existing parquet files in user directory.\n",
    "    \n",
    "    Args:\n",
    "        cleanup (bool): When True, actually DELETES FILES. \n",
    "                        When False, only LISTS files.\n",
    "    \"\"\"\n",
    "    hprint(\"Clean up existing parquet files\")\n",
    "\n",
    "    print(\"[cleanup] Listing files BEFORE cleanup:\")\n",
    "    get_ipython().system(f'hdfs dfs -ls {WASBS_USER}/*.parquet')\n",
    "    \n",
    "    if cleanup:\n",
    "        print(\"\\n[cleanup] Deleting all parquet folders...\")\n",
    "        get_ipython().system(f'hdfs dfs -rm -r -f {WASBS_USER}/*.parquet')\n",
    "        \n",
    "        print(\"\\n[info] Listing files AFTER cleanup:\")\n",
    "        get_ipython().system(f'hdfs dfs -ls {WASBS_USER}/*.parquet')\n",
    "        print(\"\\n[cleanup] Parquet file cleanup complete - ready to restart Processing run with clean schema\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n[info] To actually delete files, call: cleanup_parquet_files(cleanup=True)\")\n",
    "\n",
    "def normalise_ids(df: DataFrame, col: str = \"ID\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    # Single source of truth for ID normalisation \n",
    "    Upper + trim + distinct on the given ID column.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] normalise_ids() on column: {col}\")\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "    return df.select(F.upper(F.trim(F.col(col))).alias(\"ID\")).distinct()\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "\n",
    "def df_as_html(df, n: int = 5, right_align: bool = False, show_index: bool = False):\n",
    "    \"\"\"\n",
    "    HTML preview via pandas with no truncation. If right_align=True,\n",
    "    only numeric columns are right-justified; everything else is \n",
    "    explicitly left-aligned.\n",
    "    \"\"\"\n",
    "    pdf = df.limit(n).toPandas()\n",
    "    print(\"[INFO] Converting Spark → pandas for HTML display (rows:\", len(pdf), \")\")\n",
    "    print(\"[INFO] right_align (numeric columns):\", right_align)\n",
    "\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\", None,   \n",
    "        \"display.max_columns\", None,    \n",
    "        \"display.width\", None            \n",
    "    ):\n",
    "        styler = pdf.style if show_index else pdf.style.hide(axis=\"index\")\n",
    "\n",
    "        #   table alignment: left for both headers and cells\n",
    "        styler = styler.set_table_styles(\n",
    "            [\n",
    "                {\"selector\": \"th\", \"props\": [(\"text-align\", \"left\")]},\n",
    "                {\"selector\": \"td\", \"props\": [(\"text-align\", \"left\")]},\n",
    "            ],\n",
    "            overwrite=True,  # make this the baseline\n",
    "        )\n",
    "         \n",
    "        if right_align:\n",
    "            numeric_cols = list(pdf.select_dtypes(include=[\"number\"]).columns)\n",
    "            print(\"[INFO] Right-aligning numeric columns:\", numeric_cols)\n",
    "            if numeric_cols:\n",
    "                styler = styler.set_properties(subset=numeric_cols,\n",
    "                                               **{\"text-align\": \"right\"})\n",
    "        display(styler)\n",
    "\n",
    "def show_df(df, n: int = 10, name: str = \"\", right_align: bool = False):\n",
    "    \"\"\"\n",
    "    Print schema, \n",
    "    show an HTML sample,\n",
    "    and row count.\n",
    "    \"\"\"\n",
    "    hprint()\n",
    "    print(\"name : \",name)\n",
    "    df.printSchema()\n",
    "    print(\"[check] sample:\")\n",
    "    df_as_html(df, n=n, right_align=right_align)\n",
    "\n",
    "def write_parquet(df, dir_as_path: str, df_name:str = \"\"):    \n",
    "    funct_time = time.time()\n",
    "    path = _normalise_dir(dir_as_path)\n",
    "    print(f\"[file] write_parquet  : {path}\")\n",
    "    try:      \n",
    "        show_df(df,df_name)\n",
    "    except Exception as e:\n",
    "        print(\"[catch] sample failed:\", e)\n",
    "        os.system(f'hdfs dfs -rm -r -f \"{path}\"')   # idempotent cleanup\n",
    "    df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
    "    os.system(f'hdfs dfs -ls -R \"{path}\"')\n",
    "    funct_time = time.time() - funct_time \n",
    "    print(f\"[time] write_parquet (min)   : {funct_time/60:5.2f}\")\n",
    "    print(f\"[time] write_parquet (sec)   : {funct_time:5.2f}\")\n",
    "\n",
    "def has_parquet(dir_as_path: str) -> bool:\n",
    "    path   = _normalise_dir( dir_as_path)\n",
    "    marker = path + '_SUCCESS'\n",
    "    #print(\"\\n[check] dir_path:\", dir_path)\n",
    "    #print(\"\\n[check] path    :\", path)\n",
    "    print(\"\\n[check] marker  :\", marker)\n",
    "    rc = os.system(f'hdfs dfs -test -e \"{marker}\"')\n",
    "    print(\"[check] rc:\", rc, \"->\", (\"exists\" if rc == 0 else \"missing\"))\n",
    "    return (rc == 0)\n",
    "\n",
    "def _to_spark(df_like, schema=None):\n",
    "    \"\"\"\n",
    "    Return a Spark DataFrame  .\n",
    "    \"\"\"\n",
    "    if isinstance(df_like, SparkDF):\n",
    "        return df_like\n",
    "    return spark.createDataFrame(df_like, schema=schema) if schema else spark.createDataFrame(df_like)\n",
    "\n",
    "def ensure_dir(path: str) -> str:\n",
    "    \"\"\"\n",
    "    ensures that path is a path \n",
    "    and not representing a file;\n",
    "    add trailing slash if needed\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        raise ValueError(\"Path is None\")\n",
    "    path = _normalise_dir(path)\n",
    "#   print(\"ensure_dir -> \",path)\n",
    "    return path\n",
    "\n",
    "def _normalise_dir(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure trailing slash so we point to\n",
    "    the dataset directory (not a file)\n",
    "    \"\"\"\n",
    "    return s if s.endswith(\"/\") else s + \"/\"\n",
    "\n",
    "def _success_exists(target_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check for the Hadoop/Spark _SUCCESS marker;  \n",
    "    \"\"\"\n",
    "    jvm = spark._jvm\n",
    "    hconf = spark._jsc.hadoopConfiguration()\n",
    "    try:\n",
    "        uri = jvm.java.net.URI(target_dir)\n",
    "        fs = jvm.org.apache.hadoop.fs.FileSystem.get(uri, hconf)\n",
    "        success = jvm.org.apache.hadoop.fs.Path(target_dir + \"_SUCCESS\")\n",
    "        exists = fs.exists(success)\n",
    "        print(f\"[status] _SUCCESS check at: {target_dir}_SUCCESS -> {exists}\")\n",
    "        return bool(exists)\n",
    "    except Exception as e:\n",
    "        print(f\"[status] _SUCCESS check failed ({e}); attempting read-probe …\")\n",
    "        try:\n",
    "            spark.read.parquet(target_dir).limit(1).count()\n",
    "            print(f\"[dewstatus59] read-probe succeeded at: {target_dir}\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"[status] read-probe failed ({e2}); treating as not existing.\")\n",
    "            return False\n",
    "\n",
    "def _count_unique_ids(df: DataFrame) -> int:\n",
    "    return normalise_ids(df).count()\n",
    "\n",
    " \n",
    "# Where to save diagnostics (use your username as requested)\n",
    "\n",
    "# Back-compat aliases hack to account for non-disciplined naming un-convention\n",
    "# hack \n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    "\n",
    "#print(\"[TEST] Using _canon_ids:\", _canon_ids(stations).count())\n",
    "#print(\"[TEST] Using canon_ids :\", canon_ids(stations).count())\n",
    "#print(\"[TEST] Using _ids      :\", _ids(stations).count())\n",
    "\n",
    "# : pairwise city distances in km using Spark built-ins \n",
    "def pairwise_city_distances_spark(cities, radius_km=6371.0):\n",
    "    \"\"\"\n",
    "    cities: list[tuple[str, float, float]] -> [(name, lat_deg, lon_deg), ...]\n",
    "    returns: Spark DataFrame with columns:\n",
    "             city_a, city_b, haversine_km, slc_km, delta_km, delta_pct\n",
    "    \"\"\"\n",
    "  #  from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise RuntimeError(\"No active Spark session.\")\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "        ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.city\").alias(\"city_a\"),\n",
    "                       F.col(\"b.city\").alias(\"city_b\"),\n",
    "                       F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\")))\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\"));  lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav_km = R * c_term\n",
    "\n",
    "    cos_val = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cos_val = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cos_val))\n",
    "    slc_km = R * F.acos(cos_val)\n",
    "\n",
    "    delta_km  = F.abs(hav_km - slc_km)\n",
    "    delta_pct = F.when(hav_km == 0, F.lit(0.0)).otherwise(delta_km / hav_km * 100.0)\n",
    "\n",
    "    out_df = (pairs\n",
    "              .withColumn(\"haversine_km\", F.round(hav_km, 2))\n",
    "              .withColumn(\"slc_km\",       F.round(slc_km, 2))\n",
    "              .withColumn(\"delta_km\",     F.round(delta_km, 4))\n",
    "              .withColumn(\"delta_pct\",    F.round(delta_pct, 6))\n",
    "              .select(\"city_a\", \"city_b\", \"haversine_km\", \"slc_km\", \"delta_km\", \"delta_pct\")\n",
    "              .orderBy(\"haversine_km\"))\n",
    "    return out_df\n",
    "\n",
    "# --- Timing helpers for Spark & pure Python (no extra deps)\n",
    "\n",
    "def benchmark_python_distances(cities, radius_km=6371.0, repeats=50000):\n",
    "    \"\"\"\n",
    "    cities: [(name, lat_deg, lon_deg), ...]  (3 cities => 3 pairs)\n",
    "    repeats: loop count to make timings stable\n",
    "    returns: dict with seconds for haversine/slc\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(cities)):\n",
    "        for j in range(i+1, len(cities)):\n",
    "            (_, lat1, lon1), (_, lat2, lon2) = cities[i], cities[j]\n",
    "            pairs.append((lat1, lon1, lat2, lon2))\n",
    "\n",
    "    # haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            dφ, dλ = (φ2 - φ1), (λ2 - λ1)\n",
    "            a = sin(dφ/2)**2 + cos(φ1)*cos(φ2)*sin(dλ/2)**2\n",
    "            c = 2*atan2(sqrt(a), sqrt(1 - a))\n",
    "            _ = radius_km * c\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # spherical law of cosines (SLC)\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            cosv = sin(φ1)*sin(φ2) + cos(φ1)*cos(φ2)*cos(λ2 - λ1)\n",
    "            cosv = max(-1.0, min(1.0, cosv))\n",
    "            _ = radius_km * acos(cosv)\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"python_haversine_sec\": t1 - t0,\n",
    "        \"python_slc_sec\":       t3 - t2,\n",
    "        \"repeats\": repeats,\n",
    "        \"pairs\": len(pairs),\n",
    "    }\n",
    "\n",
    "def _parse_ls_bytes(line): \n",
    "    parts = line.split()\n",
    "    if len(parts) < 8:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[4])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def _parse_du_bytes(line):\n",
    "    parts = line.split()\n",
    "    if len(parts) < 2:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[0])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def du_bytes(path):\n",
    "    lines = get_ipython().getoutput(f'hdfs dfs -du \"{path}\"')\n",
    "    total = 0\n",
    "    for ln in lines:\n",
    "        parts = ln.split()\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                total += int(parts[0])\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return total\n",
    "    \n",
    "def benchmark_spark_distances(cities, radius_km=6368.6, repeats=3):\n",
    "    \"\"\"\n",
    "    Uses Spark built-ins only. Measures full execution\n",
    "    time by forcing an action.\n",
    "    \n",
    "    returns: dict with seconds for haversine/slc and\n",
    "    row counts used.\n",
    "    \n",
    "    For the radius:\n",
    "    \n",
    "    The Earth is slightly flattened, so the geocentric \n",
    "    radius depends on latitude.  For context: \n",
    "    \n",
    "    * equatorial radius = 6,378.137 km; \n",
    "    * polar radius      = 6,356.752 km \n",
    "    \n",
    "    Across New Zealand's latitudes (≈36–47°S), using the\n",
    "    WGS-84 ellipsoid, you get roughly:\n",
    "\n",
    "    Auckland (37°S):       ~6,370.4 km\n",
    "    Christchurch (43.5°S): ~6,368.0 km\n",
    "    Dunedin (45.9°S):      ~6,367.2 km\n",
    "    __________________________________\n",
    "    Wellington (41°S):     ~6,369.0 km\n",
    "    mean                  ≈ 6,368.6 km\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        from pyspark.sql import SparkSession, functions as F, types as T\n",
    "    except Exception:\n",
    "        return None  # no Spark therefore save cannot run in vs code\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        return None\n",
    "\n",
    "    # build pairs once and cache\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "    ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\"))\n",
    "               .cache())\n",
    "    _ = pairs.count()\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\")); lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    # Haversine expr\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav    = R * c_term\n",
    "\n",
    "    # SLC expr\n",
    "    cosv = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cosv = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cosv))\n",
    "    slc = R * F.acos(cosv)\n",
    "\n",
    "    # time Haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(hav.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # time SLC\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(slc.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"spark_pairs\": pairs.count(),\n",
    "        \"spark_repeats\": repeats,\n",
    "        \"spark_haversine_sec\": t1 - t0,\n",
    "        \"spark_slc_sec\":       t3 - t2,\n",
    "    }\n",
    "\n",
    "\n",
    "def list_hdfs_csvgz_files(hdfs_path = WASBS_DATA, debug=False):\n",
    "    \"\"\"\n",
    "    Lists .csv.gz files from an HDFS directory, extracting year and file size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hdfs_path : str\n",
    "        The HDFS path to list, e.g. 'wasbs://campus-data@...'\n",
    "    debug : bool, optional\n",
    "        If True, prints intermediate parsing steps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuple\n",
    "        A list of (year, size) tuples for each .csv.gz file.\n",
    "    \"\"\"\n",
    "    cmd = f\"hdfs dfs -ls {hdfs_path}\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    lines = result.stdout.strip().split(\"\\n\")\n",
    "    rows = []\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if debug:\n",
    "            print(\"Parts:\", parts)\n",
    "        if len(parts) < 6:\n",
    "            continue\n",
    "        try:\n",
    "            size = int(parts[2])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        path = parts[-1]\n",
    "        if path.endswith(\".csv.gz\"):\n",
    "            try:\n",
    "                year = int(path.split(\"/\")[-1].replace(\".csv.gz\", \"\"))\n",
    "                rows.append((year, size))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    if debug:\n",
    "        print(\"_____________________________________________________\")\n",
    "        print(\"Sample parsed rows:\", rows[:5])\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def explore_hdfs_directory_tree(root_path, max_depth=2, show_sizes=True):\n",
    "    \"\"\"\n",
    "    Explore and visualise any HDFS or WASBS directory tree.\n",
    "    Works with any file types (not just .parquet).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_path : str\n",
    "        HDFS/WASBS path to explore.\n",
    "    max_depth : int\n",
    "        Maximum depth to traverse.\n",
    "    show_sizes : bool\n",
    "        Whether to display file sizes in MB.\n",
    "    \"\"\"\n",
    "\n",
    "    console = Console()\n",
    "\n",
    "    def build_tree(path, tree, depth=0):\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Run the HDFS ls command\n",
    "            cmd = ['hdfs', 'dfs', '-ls', path]\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "\n",
    "            lines = result.stdout.strip().split('\\n')\n",
    "            if not lines:\n",
    "                tree.add(\"[dim]Empty directory[/dim]\")\n",
    "                return\n",
    "\n",
    "            # Skip 'Found N items' header\n",
    "            if lines[0].startswith(\"Found\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.split()\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "\n",
    "                permissions, _, _, size, date, time_str, _, name = parts[-8:]\n",
    "                item_name = name.split(\"/\")[-1] or name.split(\"/\")[-2]\n",
    "\n",
    "                if permissions.startswith(\"d\"):\n",
    "                    # Directory node\n",
    "                    subtree = tree.add(f\"[bold cyan]{item_name}/[/bold cyan]\")\n",
    "                    if depth + 1 < max_depth:\n",
    "                        build_tree(name, subtree, depth + 1)\n",
    "                else:\n",
    "                    # File node\n",
    "                    display_name = item_name\n",
    "                    if show_sizes and size.isdigit():\n",
    "                        size_mb = int(size) / (1024 ** 2)\n",
    "                        display_name += f\" ({size_mb:.2f} MB)\"\n",
    "                    tree.add(display_name)\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            tree.add(f\"[red]Error accessing {path}: {e}[/red]\")\n",
    "        except Exception as e:\n",
    "            tree.add(f\"[red]Unexpected error: {e}[/red]\")\n",
    "\n",
    "    # Start visualisation\n",
    "    console.print(\"=\" * 60)\n",
    "    console.print(f\"[bold white]DIRECTORY TREE FOR:[/bold white] [cyan]{root_path}[/cyan]\")\n",
    "    console.print(\"=\" * 60)\n",
    "\n",
    "    tree = Tree(f\"[green]{root_path}[/green]\")\n",
    "    build_tree(root_path, tree)\n",
    "    console.print(tree)\n",
    "    console.print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "def explore_hdfs_directory_tree(root_path, max_depth=3, show_sizes=True):\n",
    "    console = Console()\n",
    "\n",
    "    def build_tree(path, tree, depth=0):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"hdfs\", \"dfs\", \"-ls\", path],\n",
    "                capture_output=True, text=True, check=True\n",
    "            )\n",
    "            lines = [ln for ln in result.stdout.strip().split(\"\\n\") if ln and not ln.startswith(\"Found\")]\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.split()\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "\n",
    "                perms, size, name = parts[0], parts[4], parts[-1]\n",
    "                item_name = name.split(\"/\")[-1] or name.split(\"/\")[-2]\n",
    "\n",
    "                if perms.startswith(\"d\"):\n",
    "                    subtree = tree.add(f\"[bold cyan]{item_name}/[/bold cyan]\")\n",
    "                    build_tree(name, subtree, depth + 1)\n",
    "                else:\n",
    "                    size_mb = int(size)/(1024*1024) if size.isdigit() else 0\n",
    "                    label = f\"{item_name} ({size_mb:.2f} MB)\" if show_sizes else item_name\n",
    "                    tree.add(label)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            tree.add(f\"[red]Error accessing {path}: {e}[/red]\")\n",
    "\n",
    "    # ✅ Header and recursive tree printing belong *inside* the function\n",
    "    console.print(\"=\" * 60)\n",
    "    console.print(f\"[bold white]DIRECTORY TREE FOR:[/bold white] [cyan]{root_path}[/cyan]\")\n",
    "    console.print(\"=\" * 60)\n",
    "    tree = Tree(f\"[green]{root_path}[/green]\")\n",
    "    build_tree(root_path, tree)\n",
    "    console.print(tree)\n",
    "    console.print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def list_hdfs_all(hdfs_path):\n",
    "    \"\"\"List all files and directories under a given HDFS/WASBS path.\"\"\"\n",
    "    cmd = f\"hdfs dfs -ls -R {hdfs_path}\"  # -R for recursive\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    output = result.stdout.strip()\n",
    "    \n",
    "    if not output:\n",
    "        print(f\"[INFO] No files or directories found in {hdfs_path}\")\n",
    "    else:\n",
    "        print(f\"Listing for {hdfs_path}:\\n\")\n",
    "        print(output)\n",
    "\n",
    "\n",
    "def build_directory_tree_df(root_path=None, max_depth=3):\n",
    "    \"\"\"\n",
    "    build directory tree from hdfs/wasbs path and return as spark dataframe.\n",
    "    \n",
    "    parameters:\n",
    "        root_path (str): wasbs path to explore (defaults to WASBS_DATA)\n",
    "        max_depth (int): maximum depth to traverse\n",
    "        \n",
    "    returns:\n",
    "        spark dataframe with columns: level, path, name, type, size, parent_path\n",
    "    \"\"\"\n",
    "    if root_path is None:\n",
    "        root_path = WASBS_DATA\n",
    "        \n",
    "    print(f\"[info] building directory tree from: {root_path}\")\n",
    "    print(f\"[info] max depth: {max_depth}\")\n",
    "    \n",
    "    tree_data = []\n",
    "    \n",
    "    def explore_path(current_path, current_level, parent_path):\n",
    "        if current_level > max_depth:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"hdfs\", \"dfs\", \"-ls\", current_path],\n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            lines = result.stdout.strip().split(\"\\n\")\n",
    "            if lines and lines[0].startswith(\"Found\"):\n",
    "                lines = lines[1:]\n",
    "                \n",
    "            for line in lines:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                    \n",
    "                parts = line.split()\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "                    \n",
    "                permissions = parts[0]\n",
    "                size_str = parts[4]\n",
    "                full_path = parts[-1]\n",
    "                \n",
    "                # extract item name\n",
    "                item_name = full_path.rstrip('/').split('/')[-1]\n",
    "                if not item_name:\n",
    "                    item_name = full_path.split('/')[-2]\n",
    "                \n",
    "                # determine type and size\n",
    "                is_dir = permissions.startswith('d')\n",
    "                item_type = \"dir\" if is_dir else \"file\"\n",
    "                size_bytes = 0 if is_dir else (int(size_str) if size_str.isdigit() else 0)\n",
    "                \n",
    "                # add to tree data\n",
    "                tree_data.append({\n",
    "                    \"level\": current_level,\n",
    "                    \"path\": full_path,\n",
    "                    \"name\": item_name,\n",
    "                    \"type\": item_type,\n",
    "                    \"size\": size_bytes,\n",
    "                    \"parent_path\": parent_path\n",
    "                })\n",
    "                \n",
    "                # recurse into directories\n",
    "                if is_dir and current_level < max_depth:\n",
    "                    explore_path(full_path, current_level + 1, current_path)\n",
    "                    \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"[error] failed to access {current_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[error] unexpected error at {current_path}: {e}\")\n",
    "    \n",
    "    # start exploration from root\n",
    "    explore_path(root_path, 0, None)\n",
    "    \n",
    "    print(f\"[info] collected {len(tree_data)} items from directory tree\")\n",
    "    \n",
    "    # convert to spark dataframe\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"level\", T.IntegerType(), False),\n",
    "        T.StructField(\"path\", T.StringType(), False),\n",
    "        T.StructField(\"name\", T.StringType(), False),\n",
    "        T.StructField(\"type\", T.StringType(), False),\n",
    "        T.StructField(\"size\", T.LongType(), False),\n",
    "        T.StructField(\"parent_path\", T.StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = spark.createDataFrame(tree_data, schema=schema)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_tree_to_parquet(df, output_path):\n",
    "    \"\"\"\n",
    "    save directory tree dataframe to parquet.\n",
    "    \n",
    "    parameters:\n",
    "        df: spark dataframe with tree structure\n",
    "        output_path: wasbs path for output (should be in WASBS_USER)\n",
    "    \"\"\"\n",
    "    print(f\"[info] saving tree to: {output_path}\")\n",
    "    \n",
    "    # ensure trailing slash\n",
    "    if not output_path.endswith('/'):\n",
    "        output_path += '/'\n",
    "    \n",
    "    try:\n",
    "        df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        print(f\"[info] tree saved successfully to: {output_path}\")\n",
    "        \n",
    "        # verify with hdfs ls\n",
    "        result = subprocess.run(\n",
    "            [\"hdfs\", \"dfs\", \"-ls\", output_path],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        print(f\"[info] parquet contents:\\n{result.stdout}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[error] failed to save tree: {e}\")\n",
    "\n",
    "\n",
    "def display_tree_as_text(df, show_sizes=True):\n",
    "    \"\"\"\n",
    "    display directory tree dataframe in text format matching reference pdf.\n",
    "    \n",
    "    parameters:\n",
    "        df: spark dataframe with tree structure\n",
    "        show_sizes: whether to show file sizes in bytes\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DIRECTORY TREE STRUCTURE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # collect data sorted by level and path\n",
    "    tree_data = df.orderBy(\"level\", \"path\").collect()\n",
    "    \n",
    "    # build hierarchical display\n",
    "    path_to_children = {}\n",
    "    for row in tree_data:\n",
    "        parent = row.parent_path\n",
    "        if parent not in path_to_children:\n",
    "            path_to_children[parent] = []\n",
    "        path_to_children[parent].append(row)\n",
    "    \n",
    "    def print_tree(path, level=0, prefix=\"\", is_last=True):\n",
    "        \"\"\"recursively print tree structure\"\"\"\n",
    "        children = path_to_children.get(path, [])\n",
    "        \n",
    "        for i, child in enumerate(children):\n",
    "            is_last_child = (i == len(children) - 1)\n",
    "            \n",
    "            # determine connector characters\n",
    "            if level == 0:\n",
    "                connector = \"└── \" if is_last_child else \"├── \"\n",
    "                extension = \"    \" if is_last_child else \"│   \"\n",
    "            else:\n",
    "                connector = prefix + (\"└── \" if is_last_child else \"├── \")\n",
    "                extension = prefix + (\"    \" if is_last_child else \"│   \")\n",
    "            \n",
    "            # format item name\n",
    "            item_display = child.name\n",
    "            if child.type == \"dir\":\n",
    "                item_display += \"/\"\n",
    "            elif show_sizes and child.size > 0:\n",
    "                item_display += f\" ({child.size})\"\n",
    "            \n",
    "            # print the item\n",
    "            print(connector + item_display)\n",
    "            \n",
    "            # recurse for directories\n",
    "            if child.type == \"dir\":\n",
    "                print_tree(child.path, level + 1, extension, is_last_child)\n",
    "    \n",
    "    # start from root (items with no parent)\n",
    "    root_items = path_to_children.get(None, [])\n",
    "    for i, root_item in enumerate(root_items):\n",
    "        is_last = (i == len(root_items) - 1)\n",
    "        print(\"└── \" + root_item.name + (\"/\" if root_item.type == \"dir\" else \"\"))\n",
    "        if root_item.type == \"dir\":\n",
    "            print_tree(root_item.path, 1, \"    \" if is_last else \"│   \", is_last)\n",
    "    \n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "\n",
    "def create_struct_type_from_attributes(attributes_list):\n",
    "    \"\"\"\n",
    "    create spark structtype schema from attributes list\n",
    "    \n",
    "    args:\n",
    "        attributes_list: list of tuples [(column_name, data_type), ...]\n",
    "        \n",
    "    returns:\n",
    "        structtype: spark schema object\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    \n",
    "    for col_name, col_type in attributes_list:\n",
    "        # map attribute data types to spark types\n",
    "        if col_type.lower() == 'string':\n",
    "            spark_type = StringType()\n",
    "        elif col_type.lower() in ['real', 'numeric']:\n",
    "            spark_type = DoubleType()\n",
    "        else:\n",
    "            # default to string for unknown types\n",
    "            spark_type = StringType()\n",
    "            print(f\"[warning] unknown type '{col_type}' for column '{col_name}', defaulting to StringType\")\n",
    "        \n",
    "        # create structfield\n",
    "        fields.append(StructField(col_name, spark_type, True))\n",
    "    \n",
    "    return StructType(fields)\n",
    "\n",
    "\n",
    "def rename_audio_columns(df, dataset_code, keep_msd_trackid=True):\n",
    "    \"\"\"\n",
    "    rename dataframe columns using 2-letter + 3-digit format\n",
    "    \n",
    "    args:\n",
    "        df: spark dataframe to rename\n",
    "        dataset_code: 2-letter code ('AO', 'LP', 'SP', 'TI')\n",
    "        keep_msd_trackid: if true, don't rename MSD_TRACKID column\n",
    "        \n",
    "    returns:\n",
    "        tuple: (renamed_df, mapping_dict)\n",
    "            renamed_df: dataframe with new column names\n",
    "            mapping_dict: {original_name: new_name}\n",
    "    \"\"\"\n",
    "    rename_map = {}\n",
    "    feature_num = 1\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if keep_msd_trackid and col == 'MSD_TRACKID':\n",
    "            # preserve join key\n",
    "            rename_map[col] = col\n",
    "        else:\n",
    "            # create 2-letter + 3-digit code\n",
    "            new_name = f\"{dataset_code}{feature_num:03d}\"\n",
    "            rename_map[col] = new_name\n",
    "            feature_num += 1\n",
    "    \n",
    "    # apply renaming\n",
    "    renamed_df = df.select([F.col(old).alias(new) for old, new in rename_map.items()])\n",
    "    \n",
    "    return renamed_df, rename_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a36c42b0-9d6d-4eb4-89df-590eb55cb051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________Clean up existing parquet files_________\n",
      "[cleanup] Listing files BEFORE cleanup:\n",
      "Found 2 items\n",
      "-rw-r--r--   1 dew59 supergroup          0 2025-10-08 08:29 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2/msd_directory_tree.parquet/_SUCCESS\n",
      "-rw-r--r--   1 dew59 supergroup        727 2025-10-08 08:29 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2/msd_directory_tree.parquet/part-00000-7069e22c-78cd-449b-967d-81d921730363-c000.snappy.parquet\n",
      "\n",
      "[info] To actually delete files, call: cleanup_parquet_files(cleanup=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# USE SPARINGLY - these are for diagnostics only\n",
    "# Set cleanup=True to actually delete files, or False to just list them \n",
    "# LEAVE cleanup=False after running this cell once! \n",
    "# if they have been created and are correct, change cleanup=False for quicker runs. \n",
    "cleanup_parquet_files(cleanup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586fca6e-683f-4426-b354-175f8eeb5000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___________started at: 2025.10.10 17:48___________\n",
      "Found 4 items\n",
      "drwxrwxrwx   -          0 1970-01-01 12:00 wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/audio\n",
      "drwxrwxrwx   -          0 1970-01-01 12:00 wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/genre\n",
      "drwxrwxrwx   -          0 1970-01-01 12:00 wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/main\n",
      "drwxrwxrwx   -          0 1970-01-01 12:00 wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/tasteprofile\n",
      "12.9 G  12.9 G  wasbs://campus-data@madsstorage002.blob.core.windows.net/msd\n",
      "Found 1 items\n",
      "drwxr-xr-x   - dew59 supergroup          0 2025-10-08 08:29 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2/msd_directory_tree.parquet\n",
      "727  727  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2\n"
     ]
    }
   ],
   "source": [
    "# overall time metric\n",
    "start_notebook = time.time() \n",
    "start_time = datetime.fromtimestamp(start_notebook).strftime(\"%Y.%m.%d %H:%M\")\n",
    "\n",
    "hprint(f\"started at: {start_time}\")\n",
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "#!hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/msd/\n",
    "!hdfs dfs -ls    -h {WASBS_DATA} \n",
    "!hdfs dfs -du -s -h {WASBS_DATA} \n",
    "!hdfs dfs -ls    -h {WASBS_USER} \n",
    "!hdfs dfs -du -s -h {WASBS_USER} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c741463-9dfb-4c39-ba88-1d35afcecedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw result: ['13854359805  13854359805  wasbs://campus-data@madsstorage002.blob.core.windows.net/msd']\n",
      "\n",
      "firstpass size (bytes): 13854359805\n",
      "firstpass size (MB)   : 13212.547\n",
      "\n",
      "[time]   Cell time (sec)   :  4.21\n",
      "[time]   Cell time (min)   :  0.07\n"
     ]
    }
   ],
   "source": [
    "cell_time = time.time() \n",
    "result = get_ipython().getoutput(f\"hdfs dfs -du -s {WASBS_DATA}\") \n",
    "\n",
    "print(\"Raw result:\", result)\n",
    "print()\n",
    "data_size_bytes = int(result[0].split()[0])\n",
    "print(\"firstpass size (bytes):\", data_size_bytes)\n",
    "print(f\"firstpass size (MB)   : {data_size_bytes / (1024**2):.3f}\")\n",
    " \n",
    "lines = get_ipython().getoutput(f\"hdfs dfs -ls {WASBS_DATA}\")\n",
    "print()\n",
    "#other_size_bytes = 0\n",
    "#for line in lines:\n",
    "#    parts = line.split()\n",
    "#    if len(parts) >= 6 and parts[0].startswith('-'):   # file, not directory\n",
    "#        size = int(parts[2])                           # file size is parts[2] in your env\n",
    "#        other_size_bytes += size\n",
    "#print()\n",
    "#print(\"_____________________________________________________\") \n",
    "#print(f\"[result] daily size (bytes): {daily_size_bytes:,d}\")\n",
    "#print(f\"[result] daily size (MB)   : {daily_size_bytes / (1024**2):.2f}\")\n",
    "#print(f\"[result] meta-data (bytes) : {other_size_bytes:,d}\")\n",
    "#print(f\"[result] meta-data (MB)    : {other_size_bytes / (1024**2):.2f}\")\n",
    "\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc5edd-f44d-41c9-8add-129638a53a8f",
   "metadata": {},
   "source": [
    "### Q1 - Directory Tree Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab232eb-ade2-4400-b722-11708d97ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] directory tree image exists, skipping generation\n",
      "[display] reading from disk: ../report/supplementary/msd_directory_tree.png\n",
      "[display] showing directory tree from: ../report/supplementary/msd_directory_tree.png\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(png_path):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[display] showing directory tree from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpng_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m     display(\u001b[43mImage\u001b[49m(filename\u001b[38;5;241m=\u001b[39mpng_path))\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[warning] directory tree image not available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "# Q1(a) - Get the file structure and display directory tree\n",
    "\n",
    "png_path = '../report/supplementary/msd_directory_tree.png'\n",
    "\n",
    "# ensure directory exists\n",
    "os.makedirs(os.path.dirname(png_path), exist_ok=True)\n",
    "\n",
    "# check if png already exists\n",
    "if os.path.exists(png_path):\n",
    "    print(f\"[info] directory tree image exists, skipping generation\")\n",
    "    print(f\"[display] reading from disk: {png_path}\")\n",
    "else:\n",
    "    print(f\"[info] directory tree image not found, generating...\")\n",
    "    \n",
    "    # build directory tree dataframe\n",
    "    tree_df = build_directory_tree_df(WASBS_DATA, max_depth=3)\n",
    "    \n",
    "    # save to parquet in wasbs_user\n",
    "    tree_parquet_path = f\"{WASBS_USER}msd_directory_tree.parquet/\"\n",
    "    tree_df.write.mode(\"overwrite\").parquet(tree_parquet_path)\n",
    "    print(f\"[saved] tree dataframe: {tree_parquet_path}\")\n",
    "    \n",
    "    # create rich console visualisation and save as png\n",
    "    try:\n",
    "        # create rich tree visualisation\n",
    "        console_tree = Console(record=True, width=120)\n",
    "        tree = Tree(f\"[green]{WASBS_DATA}[/green]\")\n",
    "        \n",
    "        # build tree from dataframe\n",
    "        tree_data = tree_df.orderBy(\"level\", \"path\").collect()\n",
    "        path_to_node = {None: tree}\n",
    "        \n",
    "        for row in tree_data:\n",
    "            parent_node = path_to_node.get(row.parent_path, tree)\n",
    "            if row.type == \"dir\":\n",
    "                node = parent_node.add(f\"[bold cyan]{row.name}/[/bold cyan]\")\n",
    "                path_to_node[row.path] = node\n",
    "            else:\n",
    "                size_mb = row.size / (1024**2) if row.size > 0 else 0\n",
    "                parent_node.add(f\"{row.name} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        # export to svg then convert to png\n",
    "        console_tree.print(tree)\n",
    "        svg_output = console_tree.export_svg(title=\"MSD Directory Tree\")\n",
    "        \n",
    "        # save svg temporarily\n",
    "        svg_path = png_path.replace('.png', '.svg')\n",
    "        with open(svg_path, 'w') as f:\n",
    "            f.write(svg_output)\n",
    "        \n",
    "        # convert svg to png using cairosvg or imagemagick\n",
    "        try:\n",
    "            import cairosvg\n",
    "            cairosvg.svg2png(url=svg_path, write_to=png_path, dpi=150)\n",
    "        except ImportError:\n",
    "            # fallback: use matplotlib to create simple tree visualisation\n",
    "            fig, ax = plt.subplots(figsize=(14, 10))\n",
    "            ax.axis('off')\n",
    "            tree_text = \"\\n\".join([f\"{'  ' * row.level}{row.name}{'/' if row.type == 'dir' else ''}\" \n",
    "                                   for row in tree_data[:50]])  # limit to 50 items\n",
    "            ax.text(0.05, 0.95, tree_text, fontsize=8, family='monospace', va='top')\n",
    "            plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "            plt.close()\n",
    "        \n",
    "        print(f\"[saved] directory tree image: {png_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[error] failed to generate tree image: {e}\")\n",
    "        print(f\"[fallback] creating simple text-based image...\")\n",
    "        \n",
    "        # simple fallback: create text-based visualisation\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "        ax.axis('off')\n",
    "        ax.text(0.5, 0.5, \"Directory tree visualisation\\n(see parquet file for details)\", \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"[saved] fallback image: {png_path}\")\n",
    "\n",
    "# always display from disk (whether just created or already existed)\n",
    "if os.path.exists(png_path):\n",
    "    print(f\"[display] showing directory tree from: {png_path}\")\n",
    "    display(Image(filename=png_path))\n",
    "else:\n",
    "    print(f\"[warning] directory tree image not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1(b) - Parse the structure file and calculate summary statistics\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Q1(b) - Summary Statistics\")\n",
    "\n",
    "import re\n",
    "\n",
    "# Parse hdfs ls -R output to extract size and path\n",
    "def parse_ls_line(line):\n",
    "    \"\"\"Parse a single line from hdfs ls -R output\"\"\"\n",
    "    # Format: permissions replication user group size date time path\n",
    "    # Example: -rw-r--r--   3 hdfs supergroup   1051 2024-01-15 10:30 /path/to/file\n",
    "    parts = line.split()\n",
    "    if len(parts) < 8:\n",
    "        return None\n",
    "    \n",
    "    permissions = parts[0]\n",
    "    size_str = parts[4]\n",
    "    path = parts[-1]\n",
    "    \n",
    "    # Only process files (not directories)\n",
    "    if not permissions.startswith('d'):\n",
    "        try:\n",
    "            size = int(size_str)\n",
    "            return {'size': size, 'path': path, 'is_dir': False}\n",
    "        except ValueError:\n",
    "            return None\n",
    "    else:\n",
    "        return {'size': 0, 'path': path, 'is_dir': True}\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Read and parse the data structure file\n",
    "try:\n",
    "    with open(\"data_structure.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    file_count = 0\n",
    "    dir_count = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        parsed = parse_ls_line(line.strip())\n",
    "        if parsed:\n",
    "            if parsed['is_dir']:\n",
    "                dir_count += 1\n",
    "            else:\n",
    "                file_count += 1\n",
    "                total_size += parsed['size']\n",
    "    \n",
    "    print(f\"\\n[summary] directories: {dir_count}\")\n",
    "    print(f\"[summary] files: {file_count}\")\n",
    "    print(f\"[summary] total size: {total_size:,} bytes ({total_size/(1024**3):.2f} GB)\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"[error] data_structure.txt not found. Please run the previous cell first.\")\n",
    "except Exception as e:\n",
    "    print(f\"[error] Failed to parse structure file: {e}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2318c1-9ef3-4db9-a5cb-7d028114864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    " \n",
    "#stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47253f6c-7c2c-47f1-a55c-a16210d00c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff6b9d78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q2 - Exploring the Audio Dataset\n",
    "\n",
    "In this section, we will examine the audio feature datasets, understand their schemas, and develop a systematic approach to working with their column names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b40b20",
   "metadata": {},
   "source": [
    "### Q2(a) - Load Audio Feature Attribute Names and Types\n",
    "\n",
    "The audio feature datasets are stored in two locations:\n",
    "- **Attributes directory**: Contains CSV files defining column names and data types\n",
    "- **Features directory**: Contains the actual feature data (partitioned CSV directories)\n",
    "\n",
    "Each attribute file follows the format: `attribute_name,type`\n",
    "\n",
    "We'll examine these attribute files to understand how they can be used to define schemas for loading the feature datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) - Examine attribute files to understand column naming\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Q2(a) - Audio Feature Attributes Analysis\")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# List all audio feature datasets (reference approach - use available files)\n",
    "try:\n",
    "    # dynamically discover what datasets actually exist\n",
    "    files_df = spark.read.text(f\"{WASBS_DATA}audio/features/\")\n",
    "    available_files = [row.value.split('/')[-1].replace('.csv', '') for row in files_df.collect() if row.value.endswith('.csv')]\n",
    "    \n",
    "    # filter to audio datasets we expect\n",
    "    expected_prefixes = ['msd-jmir-area-of-moments', 'msd-jmir-lpc', 'msd-jmir-methods-of-moments', \n",
    "                        'msd-jmir-mfcc', 'msd-jmir-spectral', 'msd-marsyas-timbral']\n",
    "    \n",
    "    audio_datasets = []\n",
    "    for file in available_files:\n",
    "        if any(file.startswith(prefix) for prefix in expected_prefixes):\n",
    "            audio_datasets.append(file)\n",
    "    \n",
    "    print(f\"[discovered] {len(audio_datasets)} audio datasets dynamically\")\n",
    "except:\n",
    "    # fallback to known good files only\n",
    "    audio_datasets = [\n",
    "        'msd-jmir-area-of-moments-all-v1.0',\n",
    "        'msd-jmir-lpc-all-v1.0',\n",
    "        'msd-jmir-methods-of-moments-all-v1.0',\n",
    "        'msd-jmir-mfcc-all-v1.0',\n",
    "        'msd-jmir-spectral-all-all-v1.0',\n",
    "        'msd-marsyas-timbral-v1.0',\n",
    "    'msd-mvd-v1.0',\n",
    "    'msd-rh-v1.0',\n",
    "    'msd-rp-v1.0',\n",
    "    'msd-ssd-v1.0',\n",
    "    'msd-trh-v1.0',\n",
    "    'msd-tssd-v1.0'\n",
    "]\n",
    "\n",
    "print(f\"[info] Found {len(audio_datasets)} audio feature datasets\\n\")\n",
    "\n",
    "# Function to read and parse attribute file\n",
    "def load_attribute_file(dataset_prefix):\n",
    "    \"\"\"\n",
    "    Load attribute names and types from an attribute CSV file.\n",
    "    \n",
    "    Args:\n",
    "        dataset_prefix: Name of the dataset (e.g., 'msd-jmir-lpc-all-v1.0')\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: [(attribute_name, attribute_type), ...]\n",
    "    \"\"\"\n",
    "    attr_path = f\"{WASBS_DATA}/audio/attributes/{dataset_prefix}.attributes.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Read attribute file as text\n",
    "        attr_rdd = spark.sparkContext.textFile(attr_path)\n",
    "        attributes = []\n",
    "        \n",
    "        for line in attr_rdd.collect():\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) >= 2:\n",
    "                attr_name = parts[0]\n",
    "                attr_type = parts[1].lower()\n",
    "                attributes.append((attr_name, attr_type))\n",
    "            elif len(parts) == 1:\n",
    "                # Some files might have just the attribute name\n",
    "                attributes.append((parts[0], 'string'))\n",
    "        \n",
    "        return attributes\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[error] Failed to load {dataset_prefix}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load attributes for all datasets\n",
    "print(\"[info] Loading attribute files...\")\n",
    "all_attributes = {}\n",
    "\n",
    "for dataset in audio_datasets:\n",
    "    attrs = load_attribute_file(dataset)\n",
    "    all_attributes[dataset] = attrs\n",
    "    print(f\"[loaded] {dataset}: {len(attrs)} attributes\")\n",
    "\n",
    "print(f\"\\n[summary] Loaded attribute information for {len(all_attributes)} datasets\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fafde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) continued - Display sample column names from each dataset\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Sample Column Names from Each Dataset\")\n",
    "\n",
    "print(\"Examining actual column names to understand naming patterns:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset, attributes in all_attributes.items():\n",
    "    if attributes:\n",
    "        print(f\"\\n[dataset] {dataset}\")\n",
    "        print(f\"[count]   {len(attributes)} columns\")\n",
    "        print(f\"[sample]  First 5 columns:\")\n",
    "        \n",
    "        # Show first 5 column names\n",
    "        for i, (name, dtype) in enumerate(attributes[:5], 1):\n",
    "            # Truncate long names for display\n",
    "            display_name = name if len(name) <= 60 else name[:57] + \"...\"\n",
    "            print(f\"  {i}. {display_name:60s} ({dtype})\")\n",
    "        \n",
    "        if len(attributes) > 5:\n",
    "            print(f\"  ... ({len(attributes) - 5} more columns)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) continued - Analyze column name characteristics\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Column Name Characteristics Analysis\")\n",
    "\n",
    "print(\"Analyzing column name patterns for Q2(c) discussion:\\n\")\n",
    "\n",
    "# Collect statistics about column names\n",
    "from collections import Counter\n",
    "\n",
    "all_column_names = []\n",
    "column_lengths = []\n",
    "data_types_used = Counter()\n",
    "\n",
    "for dataset, attributes in all_attributes.items():\n",
    "    for name, dtype in attributes:\n",
    "        all_column_names.append(name)\n",
    "        column_lengths.append(len(name))\n",
    "        data_types_used[dtype] += 1\n",
    "\n",
    "# Calculate statistics\n",
    "avg_length = sum(column_lengths) / len(column_lengths) if column_lengths else 0\n",
    "max_length = max(column_lengths) if column_lengths else 0\n",
    "min_length = min(column_lengths) if column_lengths else 0\n",
    "\n",
    "print(f\"[total columns] {len(all_column_names)} across all datasets\")\n",
    "print(f\"\\n[column name length statistics]\")\n",
    "print(f\"  Average: {avg_length:.1f} characters\")\n",
    "print(f\"  Maximum: {max_length} characters\")\n",
    "print(f\"  Minimum: {min_length} characters\")\n",
    "\n",
    "print(f\"\\n[data types used]\")\n",
    "for dtype, count in sorted(data_types_used.items()):\n",
    "    print(f\"  {dtype:10s}: {count:4d} columns ({count/len(all_column_names)*100:.1f}%)\")\n",
    "\n",
    "# Find longest column names\n",
    "print(f\"\\n[longest column names (top 10)]\")\n",
    "name_length_pairs = [(name, len(name)) for name in all_column_names]\n",
    "name_length_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (name, length) in enumerate(name_length_pairs[:10], 1):\n",
    "    display_name = name if length <= 70 else name[:67] + \"...\"\n",
    "    print(f\"  {i:2d}. {display_name:70s} ({length} chars)\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) continued - Check for column name collisions\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Column Name Collision Detection\")\n",
    "\n",
    "print(\"Checking if any column names appear in multiple datasets...\")\n",
    "print(\"(CRITICAL for Q2(c) discussion and Q2(d) renaming strategy)\\n\")\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Build a dictionary: column_name -> [list of datasets that have it]\n",
    "column_to_datasets = defaultdict(list)\n",
    "\n",
    "for dataset, attributes in all_attributes.items():\n",
    "    for name, dtype in attributes:\n",
    "        column_to_datasets[name].append(dataset)\n",
    "\n",
    "# Find columns that appear in multiple datasets\n",
    "collisions = {name: datasets for name, datasets in column_to_datasets.items() \n",
    "              if len(datasets) > 1}\n",
    "\n",
    "if collisions:\n",
    "    print(f\"[ALERT] Found {len(collisions)} column names that appear in multiple datasets!\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sort by number of occurrences (most common first)\n",
    "    sorted_collisions = sorted(collisions.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    # Show top 20 most common collisions\n",
    "    print(f\"\\nTop {min(20, len(sorted_collisions))} most common colliding column names:\\n\")\n",
    "    \n",
    "    for i, (col_name, datasets) in enumerate(sorted_collisions[:20], 1):\n",
    "        print(f\"{i:2d}. '{col_name}' appears in {len(datasets)} datasets:\")\n",
    "        for ds in datasets:\n",
    "            # Abbreviate dataset name for display\n",
    "            ds_abbr = ds.replace('msd-', '').replace('-all-v1.0', '')\n",
    "            print(f\"    - {ds_abbr}\")\n",
    "        print()\n",
    "        \n",
    "    if len(sorted_collisions) > 20:\n",
    "        print(f\"... and {len(sorted_collisions) - 20} more colliding column names\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n[CONCLUSION] WITHOUT renaming, merging datasets would cause column name conflicts!\")\n",
    "    print(f\"[CONCLUSION] This demonstrates the NEED for systematic column renaming (Q2d)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"[OK] No column name collisions detected\")\n",
    "    print(f\"[OK] However, descriptive prefixes would still improve clarity when merging\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) continued - Focus on the 4 required datasets for Audio Similarity\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Required Datasets Analysis (for Audio Similarity)\")\n",
    "\n",
    "# These are the 4 datasets required for Audio Similarity Q1\n",
    "required_datasets = [\n",
    "    'msd-jmir-area-of-moments-all-v1.0',\n",
    "    'msd-jmir-lpc-all-v1.0',\n",
    "    'msd-jmir-spectral-all-all-v1.0',\n",
    "    'msd-marsyas-timbral-v1.0'\n",
    "]\n",
    "\n",
    "print(\"The Audio Similarity section requires merging these 4 specific datasets:\\n\")\n",
    "\n",
    "total_columns = 0\n",
    "for dataset in required_datasets:\n",
    "    if dataset in all_attributes:\n",
    "        col_count = len(all_attributes[dataset])\n",
    "        total_columns += col_count\n",
    "        print(f\"[{dataset}]\")\n",
    "        print(f\"  Columns: {col_count}\")\n",
    "        print()\n",
    "\n",
    "print(f\"[TOTAL] {total_columns} columns after merging (excluding MSD_TRACKID)\")\n",
    "print(f\"[NOTE]  Plus 1 MSD_TRACKID column = {total_columns + 1} total columns\\n\")\n",
    "\n",
    "# Check for collisions among just these 4 datasets\n",
    "print(\"Checking for collisions among these 4 required datasets...\")\n",
    "required_column_to_datasets = defaultdict(list)\n",
    "\n",
    "for dataset in required_datasets:\n",
    "    if dataset in all_attributes:\n",
    "        for name, dtype in all_attributes[dataset]:\n",
    "            required_column_to_datasets[name].append(dataset)\n",
    "\n",
    "required_collisions = {name: datasets for name, datasets in required_column_to_datasets.items() \n",
    "                       if len(datasets) > 1}\n",
    "\n",
    "if required_collisions:\n",
    "    print(f\"\\n[ALERT] Found {len(required_collisions)} collisions among the 4 required datasets:\")\n",
    "    for col_name, datasets in sorted(required_collisions.items()):\n",
    "        print(f\"  - '{col_name}' in: {', '.join([d.replace('msd-', '').replace('-all-v1.0', '').replace('-all-all-v1.0', '') for d in datasets])}\")\n",
    "else:\n",
    "    print(f\"\\n[OK] No collisions among the 4 required datasets\")\n",
    "    print(f\"[INFO] However, renaming is still recommended for clarity and consistency\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) continued - Create summary table for report\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Summary Table - Audio Feature Datasets\")\n",
    "\n",
    "# Create a summary DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for dataset in audio_datasets:\n",
    "    if dataset in all_attributes:\n",
    "        attrs = all_attributes[dataset]\n",
    "        col_count = len(attrs)\n",
    "        \n",
    "        # Calculate average column name length for this dataset\n",
    "        lengths = [len(name) for name, _ in attrs]\n",
    "        avg_len = sum(lengths) / len(lengths) if lengths else 0\n",
    "        max_len = max(lengths) if lengths else 0\n",
    "        \n",
    "        # Count data types\n",
    "        types = [dtype for _, dtype in attrs]\n",
    "        type_counts = Counter(types)\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Dataset': dataset.replace('msd-', '').replace('-v1.0', ''),\n",
    "            'Columns': col_count,\n",
    "            'Avg Name Length': f\"{avg_len:.0f}\",\n",
    "            'Max Name Length': max_len,\n",
    "            'String': type_counts.get('string', 0),\n",
    "            'Real': type_counts.get('real', 0),\n",
    "            'Numeric': type_counts.get('numeric', 0)\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Columns', ascending=False)\n",
    "\n",
    "print(\"\\nAudio Feature Dataset Summary:\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n[KEY OBSERVATIONS for Q2(c)]\")\n",
    "print(f\"  1. Column names average {avg_length:.0f} characters - quite long for practical use\")\n",
    "print(f\"  2. Longest column name is {max_length} characters - very cumbersome\")\n",
    "print(f\"  3. Total of {len(all_column_names)} columns across 13 datasets\")\n",
    "if collisions:\n",
    "    print(f\"  4. {len(collisions)} column names appear in multiple datasets - collision risk!\")\n",
    "print(f\"  5. Dominant data types: {', '.join([f'{k}({v})' for k,v in data_types_used.most_common(3)])}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfeb305",
   "metadata": {},
   "source": [
    "## Q2(b) - Automatic StructType Creation\n",
    "\n",
    "In this section, we implement a function to automatically generate Spark `StructType` schemas from attribute files. This eliminates manual schema definition and ensures consistency across all 13 audio feature datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(b) - Implement automatic StructType generation from attributes\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Q2(b) - Automatic StructType Generation\")\n",
    "\n",
    "# note: create_struct_type_from_attributes() defined in helper functions cell above\n",
    "print(\"[info] using function: create_struct_type_from_attributes()\")\n",
    "print(\"[info] type mapping: string → StringType(), real/numeric → DoubleType()\")\n",
    "\n",
    "# test on one dataset\n",
    "print(\"\\n[test] generating schema for msd-jmir-area-of-moments-all-v1.0:\")\n",
    "aom_schema = create_struct_type_from_attributes(all_attributes['msd-jmir-area-of-moments-all-v1.0'])\n",
    "print(f\"[result] schema with {len(aom_schema.fields)} fields created successfully\")\n",
    "print(\"\\n[sample] first 5 fields:\")\n",
    "for i, field in enumerate(aom_schema.fields[:5], 1):\n",
    "    print(f\"  {i}. {field.name} ({field.dataType})\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62af268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(b) continued - Generate schemas for all 4 required datasets\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n____Generating Schemas for 4 Required Datasets____\\n\")\n",
    "\n",
    "# dataset names for audio similarity section\n",
    "required_datasets = [\n",
    "    'msd-jmir-area-of-moments-all-v1.0',\n",
    "    'msd-jmir-lpc-all-v1.0',\n",
    "    'msd-jmir-spectral-all-all-v1.0',\n",
    "    'msd-marsyas-timbral-v1.0'\n",
    "]\n",
    "\n",
    "# generate schemas\n",
    "schemas = {}\n",
    "for dataset_name in required_datasets:\n",
    "    schema = create_struct_type_from_attributes(all_attributes[dataset_name])\n",
    "    schemas[dataset_name] = schema\n",
    "    short_name = dataset_name.replace('msd-jmir-', '').replace('msd-marsyas-', '')\n",
    "    print(f\"[generated] {short_name}: {len(schema.fields)} fields\")\n",
    "\n",
    "print(f\"\\n[summary] successfully generated schemas for all 4 required datasets\")\n",
    "print(f\"[total fields] {sum(len(s.fields) for s in schemas.values())} across all datasets\")\n",
    "\n",
    "# display example schema structure\n",
    "print(\"\\n[example] msd-marsyas-timbral-v1.0 schema (first 10 fields):\")\n",
    "timb_schema = schemas['msd-marsyas-timbral-v1.0']\n",
    "for i, field in enumerate(timb_schema.fields[:10], 1):\n",
    "    print(f\"  {i:2d}. StructField('{field.name}', {field.dataType}, True)\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56614a34",
   "metadata": {},
   "source": [
    "## Q2(c) - Discussion: Column Naming Advantages and Disadvantages\n",
    "\n",
    "### Current Column Naming Characteristics\n",
    "\n",
    "Based on our analysis in Q2(a), the audio feature datasets use descriptive, self-documenting column names that follow a consistent pattern. For example, names like `Method_of_Moments_Overall_Standard_Deviation_1` and `Spectral_Centroid_Overall_Average` clearly indicate the feature type, statistical measure, and variant. Our analysis revealed:\n",
    "\n",
    "- **Average column name length:** 18 characters\n",
    "- **Maximum column name length:** 108 characters\n",
    "- **Total columns across 13 datasets:** 3,929 columns\n",
    "- **Columns in 4 required datasets:** 184 columns (plus 1 MSD_TRACKID)\n",
    "- **Column name collision detected:** `MSD_TRACKID` appears in 3 different datasets\n",
    "\n",
    "### Advantages of Current Column Names\n",
    "\n",
    "The existing descriptive naming convention offers several important benefits:\n",
    "\n",
    "1. **Self-Documenting:** Names like `Zero_Crossings_Overall_Standard_Deviation` immediately convey the feature's meaning without requiring external documentation. This transparency helps researchers understand what each feature represents and how it was calculated.\n",
    "\n",
    "2. **Traceability:** The naming convention maintains clear links to the original research papers and feature extraction methods. For example, the `MoM_` prefix directly references \"Method of Moments\" calculations, allowing researchers to trace features back to specific audio analysis techniques.\n",
    "\n",
    "3. **Prevents Ambiguity:** The detailed names eliminate confusion when working with multiple datasets. Different statistical measures (mean, standard deviation, minimum, maximum) and different feature types are clearly distinguished, reducing the risk of accidentally using the wrong feature in analysis.\n",
    "\n",
    "4. **Dataset Integrity:** The descriptive names preserve the original research context, ensuring that feature interpretations remain consistent with the published methodologies used to create the Million Song Dataset.\n",
    "\n",
    "### Disadvantages of Current Column Names\n",
    "\n",
    "Despite these advantages, the descriptive naming convention presents significant practical challenges for machine learning workflows:\n",
    "\n",
    "1. **Excessive Length:** With an average of 18 characters and maximum of 108 characters, column names become unwieldy. The longest names like `Mean_Acc5_Mean_Mem20_PeakRatio_Average_Chroma_A_Power_powerFFT_WinH...` (truncated at 108 chars) are impractical for typing, displaying in tables, and referencing in code. This verbosity slows development and makes code harder to read.\n",
    "\n",
    "2. **Not ML-Friendly:** Machine learning libraries often work more efficiently with shorter, consistent column identifiers. Long names increase memory overhead in model metadata, complicate feature importance visualizations, and make serialized models larger. Many ML tools expect compact feature names for optimal performance.\n",
    "\n",
    "3. **Collision Risk When Merging:** Our analysis identified that `MSD_TRACKID` appears in multiple datasets (area-of-moments, lpc, and spectral-all). When merging these datasets for the Audio Similarity section, we must handle this collision explicitly. While this is the only collision detected among the 4 required datasets, it demonstrates the risk of assuming uniqueness with descriptive names.\n",
    "\n",
    "4. **Inconsistent Patterns Across Datasets:** While individual datasets follow internal conventions, the 13 datasets use varying naming patterns. The `marsyas-timbral` dataset uses different conventions than the `jmir` datasets, making it difficult to write generic processing code that works uniformly across all datasets.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While the descriptive column names preserve valuable semantic information and research context, their excessive length and collision potential make them impractical for machine learning workflows. A systematic renaming strategy is essential to balance interpretability with usability. The ideal solution should: (1) create unique, short identifiers suitable for ML algorithms, (2) eliminate collision risks when merging datasets, (3) maintain traceability through a mapping table that preserves the original descriptive names, and (4) apply consistently across all datasets. This approach allows us to work efficiently with compact column names while preserving the ability to interpret results using the original descriptive terminology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956aa32",
   "metadata": {},
   "source": [
    "## Q2(d) - Systematic Column Renaming Implementation\n",
    "\n",
    "### Renaming Strategy: 2-Letter + 3-Digit Convention\n",
    "\n",
    "To address the limitations identified in Q2(c), we implement a systematic renaming convention using **2-letter dataset codes + 3-digit zero-padded numbers** (format: `{AA}{NNN}`). This approach provides:\n",
    "\n",
    "- **Fixed length:** All feature names are exactly 5 characters\n",
    "- **Uniqueness:** Each dataset receives a distinct prefix (AO, LP, SP, TI)\n",
    "- **Collision elimination:** The prefix system ensures no overlapping names across datasets\n",
    "- **ML-friendly:** Short, consistent identifiers optimize performance\n",
    "- **Scalability:** 3 digits support up to 999 features per dataset (current max is 125)\n",
    "\n",
    "**Dataset Code Mapping:**\n",
    "- `AO` = Area-Of-moments (21 features)\n",
    "- `LP` = LPC (21 features)  \n",
    "- `SP` = SPectral-all (17 features)\n",
    "- `TI` = TImbral/marsyas (125 features)\n",
    "\n",
    "**Special handling:** `MSD_TRACKID` remains unchanged as it serves as the common join key across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(d) - Column Renaming Implementation (uses helper function from Cell 8)\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Q2(d) - Column Renaming Implementation\")\n",
    "\n",
    "# rename_audio_columns() already defined in Cell 8 helper functions section\n",
    "# function signature: rename_audio_columns(df, dataset_code, keep_msd_trackid=True)\n",
    "# returns: (renamed_df, mapping_dict)\n",
    "\n",
    "print(\"[info] using helper function: rename_audio_columns()\")\n",
    "print(\"[info] naming convention: {AA}{NNN} (2 letters + 3 zero-padded digits)\")\n",
    "print(\"[info] dataset codes: AO=area-of-moments, LP=lpc, SP=spectral-all, TI=timbral\")\n",
    "print(\"[info] MSD_TRACKID preserved as common join key\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(d) continued - Load and rename the 4 required datasets\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n____Loading and Renaming 4 Required Datasets____\\n\")\n",
    "\n",
    "# config for each dataset\n",
    "datasets_config = [\n",
    "    {\n",
    "        'name': 'msd-jmir-area-of-moments-all-v1.0',\n",
    "        'code': 'AO',\n",
    "        'path': f\"{WASBS_DATA}/audio/features/msd-jmir-area-of-moments-all-v1.0.csv/\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'msd-jmir-lpc-all-v1.0',\n",
    "        'code': 'LP',\n",
    "        'path': f\"{WASBS_DATA}/audio/features/msd-jmir-lpc-all-v1.0.csv/\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'msd-jmir-spectral-all-all-v1.0',\n",
    "        'code': 'SP',\n",
    "        'path': f\"{WASBS_DATA}/audio/features/msd-jmir-spectral-all-all-v1.0.csv/\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'msd-marsyas-timbral-v1.0',\n",
    "        'code': 'TI',\n",
    "        'path': f\"{WASBS_DATA}/audio/features/msd-marsyas-timbral-v1.0.csv/\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# storage for renamed dataframes and mappings\n",
    "renamed_dfs = {}\n",
    "all_mappings = {}\n",
    "\n",
    "for config in datasets_config:\n",
    "    print(f\"[loading] {config['name']}...\")\n",
    "    \n",
    "    # load with schema\n",
    "    schema = schemas[config['name']]\n",
    "    df = spark.read.csv(config['path'], header=True, schema=schema)\n",
    "    \n",
    "    # count before\n",
    "    col_count_before = len(df.columns)\n",
    "    \n",
    "    # rename columns\n",
    "    renamed_df, mapping = rename_audio_columns(df, config['code'])\n",
    "    \n",
    "    # count after\n",
    "    col_count_after = len(renamed_df.columns)\n",
    "    \n",
    "    # store results\n",
    "    renamed_dfs[config['code']] = renamed_df\n",
    "    all_mappings[config['code']] = mapping\n",
    "    \n",
    "    print(f\"[renamed] {config['code']}: {col_count_before} columns → {col_count_after} columns\")\n",
    "    print(f\"[sample] {list(renamed_df.columns)[:5]}...\\n\")\n",
    "\n",
    "print(f\"[summary] successfully renamed all 4 datasets\")\n",
    "print(f\"[total mappings] {sum(len(m) for m in all_mappings.values())} column name mappings created\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2248bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(d) continued - Display before/after examples\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n____Before/After Column Naming Examples____\\n\")\n",
    "\n",
    "# show examples from each dataset\n",
    "examples = [\n",
    "    ('AO', 'msd-jmir-area-of-moments-all-v1.0'),\n",
    "    ('LP', 'msd-jmir-lpc-all-v1.0'),\n",
    "    ('SP', 'msd-jmir-spectral-all-all-v1.0'),\n",
    "    ('TI', 'msd-marsyas-timbral-v1.0')\n",
    "]\n",
    "\n",
    "for code, dataset_name in examples:\n",
    "    print(f\"[{code}] {dataset_name}:\")\n",
    "    mapping = all_mappings[code]\n",
    "    \n",
    "    # get first 5 feature columns (skip MSD_TRACKID if present)\n",
    "    feature_mappings = [(old, new) for old, new in mapping.items() if old != 'MSD_TRACKID'][:5]\n",
    "    \n",
    "    for old_name, new_name in feature_mappings:\n",
    "        # truncate long names for display\n",
    "        display_old = old_name if len(old_name) <= 60 else old_name[:57] + '...'\n",
    "        print(f\"  {display_old:60s} → {new_name}\")\n",
    "    print()\n",
    "\n",
    "# show length comparison\n",
    "print(\"[length comparison]\")\n",
    "old_lengths = []\n",
    "new_lengths = []\n",
    "for mapping in all_mappings.values():\n",
    "    for old, new in mapping.items():\n",
    "        if old != 'MSD_TRACKID':  # exclude join key\n",
    "            old_lengths.append(len(old))\n",
    "            new_lengths.append(len(new))\n",
    "\n",
    "print(f\"  original names: avg={sum(old_lengths)/len(old_lengths):.1f} chars, max={max(old_lengths)} chars\")\n",
    "print(f\"  new names:      avg={sum(new_lengths)/len(new_lengths):.1f} chars, max={max(new_lengths)} chars\")\n",
    "print(f\"  reduction:      {100*(1 - sum(new_lengths)/sum(old_lengths)):.1f}% fewer characters overall\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19780daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(d) continued - Create comprehensive mapping table (translation table)\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n____Creating Column Name Mapping Table____\\n\")\n",
    "\n",
    "# build comprehensive mapping dataframe\n",
    "mapping_rows = []\n",
    "for dataset_code, mapping in all_mappings.items():\n",
    "    # get full dataset name\n",
    "    dataset_full_names = {\n",
    "        'AO': 'msd-jmir-area-of-moments-all-v1.0',\n",
    "        'LP': 'msd-jmir-lpc-all-v1.0',\n",
    "        'SP': 'msd-jmir-spectral-all-all-v1.0',\n",
    "        'TI': 'msd-marsyas-timbral-v1.0'\n",
    "    }\n",
    "    full_name = dataset_full_names[dataset_code]\n",
    "    \n",
    "    # add each mapping\n",
    "    for original, new in mapping.items():\n",
    "        mapping_rows.append({\n",
    "            'dataset_code': dataset_code,\n",
    "            'dataset_name': full_name,\n",
    "            'original_column_name': original,\n",
    "            'new_column_name': new,\n",
    "            'original_length': len(original),\n",
    "            'new_length': len(new),\n",
    "            'is_join_key': 'Yes' if original == 'MSD_TRACKID' else 'No'\n",
    "        })\n",
    "\n",
    "# create pandas dataframe\n",
    "import pandas as pd\n",
    "mapping_df = pd.DataFrame(mapping_rows)\n",
    "\n",
    "print(f\"[created] mapping table with {len(mapping_df)} rows\")\n",
    "print(f\"[datasets] {mapping_df['dataset_code'].nunique()} datasets\")\n",
    "print(f\"[columns] {mapping_df.groupby('dataset_code')['new_column_name'].count().to_dict()}\")\n",
    "\n",
    "# display sample\n",
    "print(\"\\n[sample] first 10 rows of mapping table:\")\n",
    "display(mapping_df.head(10))\n",
    "\n",
    "# save to csv for supplementary materials\n",
    "csv_output_path = '../report/supplementary/audio_column_name_mapping.csv'\n",
    "mapping_df.to_csv(csv_output_path, index=False)\n",
    "print(f\"\\n[saved] mapping table to: {csv_output_path}\")\n",
    "print(\"[info] this csv file can be used to translate between original and new column names\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(d) continued - Verify renaming success and show final statistics\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n____Renaming Verification and Final Statistics____\\n\")\n",
    "\n",
    "# verify all renamed dataframes\n",
    "print(\"[verification] checking renamed dataframes:\")\n",
    "for code, df in renamed_dfs.items():\n",
    "    print(f\"  {code}: {len(df.columns)} columns, {df.count()} rows\")\n",
    "\n",
    "# check for any column name collisions after renaming\n",
    "print(\"\\n[collision check] verifying uniqueness across all datasets:\")\n",
    "all_new_columns = []\n",
    "for code, mapping in all_mappings.items():\n",
    "    all_new_columns.extend([new for old, new in mapping.items() if old != 'MSD_TRACKID'])\n",
    "\n",
    "unique_columns = set(all_new_columns)\n",
    "if len(all_new_columns) == len(unique_columns):\n",
    "    print(f\"  ✓ all {len(all_new_columns)} feature columns are unique across datasets\")\n",
    "    print(f\"  ✓ no collisions detected\")\n",
    "else:\n",
    "    print(f\"  ✗ warning: {len(all_new_columns) - len(unique_columns)} collision(s) found\")\n",
    "\n",
    "# verify MSD_TRACKID preserved\n",
    "print(\"\\n[join key check] verifying MSD_TRACKID preservation:\")\n",
    "for code, df in renamed_dfs.items():\n",
    "    has_trackid = 'MSD_TRACKID' in df.columns\n",
    "    status = '✓' if has_trackid else '✗'\n",
    "    print(f\"  {status} {code}: MSD_TRACKID {'present' if has_trackid else 'MISSING'}\")\n",
    "\n",
    "# final summary\n",
    "print(\"\\n[summary] Q2(d) systematic column renaming complete:\")\n",
    "print(f\"  • renamed {sum(len(m) for m in all_mappings.values())} columns across 4 datasets\")\n",
    "print(f\"  • naming convention: 2-letter code + 3-digit number (e.g., AO001, LP001)\")\n",
    "print(f\"  • MSD_TRACKID preserved as common join key\")\n",
    "print(f\"  • mapping table saved to: report/supplementary/audio_column_name_mapping.csv\")\n",
    "print(f\"  • average name length reduced from {sum(old_lengths)/len(old_lengths):.1f} to {sum(new_lengths)/len(new_lengths):.1f} characters\")\n",
    "print(f\"  • ready for audio similarity section (binary and multiclass classification)\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aaf82e",
   "metadata": {},
   "source": [
    "## Processing Section Completion\n",
    "\n",
    "The following cells complete the Processing section requirements by generating AI-readable artifacts and validation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce30f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Infrastructure: Local Paths\")\n",
    "# supports: all processing completion cells\n",
    "# does: defines local supplementary path and ensures directory exists for ai-readable outputs\n",
    "\n",
    "# local supplementary folder (relative from code/ directory)\n",
    "LOCAL_SUPPLEMENTARY = '../report/supplementary/'\n",
    "\n",
    "# ensure local directory exists\n",
    "os.makedirs(LOCAL_SUPPLEMENTARY, exist_ok=True)\n",
    "\n",
    "print(f\"[paths] WASBS_DATA: {WASBS_DATA}\")\n",
    "print(f\"[paths] WASBS_USER: {WASBS_USER}\")\n",
    "print(f\"[paths] LOCAL_SUPPLEMENTARY: {LOCAL_SUPPLEMENTARY}\")\n",
    "print(f\"[check] supplementary folder exists: {os.path.exists(LOCAL_SUPPLEMENTARY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47832e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Processing: Q1(b)01\")\n",
    "# supports: Q1(b) — compute dataset statistics (names, sizes, formats, row counts)\n",
    "# does: extracts hdfs directory sizes, creates comprehensive statistics dataframe, saves as csv/json/png\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n[Q1(b)] extracting dataset statistics from WASBS_DATA...\")\n",
    "\n",
    "# extract directory sizes using hdfs dfs -du\n",
    "def get_directory_size(path):\n",
    "    \"\"\"get size of hdfs directory in bytes\"\"\"\n",
    "    try:\n",
    "        result = get_ipython().getoutput(f'hdfs dfs -du -s {path}')\n",
    "        if result:\n",
    "            size_bytes = int(result[0].split()[0])\n",
    "            return size_bytes\n",
    "    except Exception as e:\n",
    "        print(f\"[warning] failed to get size for {path}: {e}\")\n",
    "        return 0\n",
    "    return 0\n",
    "\n",
    "# define dataset paths and extract sizes\n",
    "datasets = [\n",
    "    ('audio-features', f\"{WASBS_DATA}audio/features/\"),\n",
    "    ('audio-attributes', f\"{WASBS_DATA}audio/attributes/\"),\n",
    "    ('genre', f\"{WASBS_DATA}genre/\"),\n",
    "    ('main', f\"{WASBS_DATA}main/\"),\n",
    "    ('tasteprofile', f\"{WASBS_DATA}tasteprofile/\"),\n",
    "    ('tasteprofile-triplets', f\"{WASBS_DATA}tasteprofile/triplets.tsv/\"),\n",
    "    ('tasteprofile-mismatches', f\"{WASBS_DATA}tasteprofile/mismatches/\")\n",
    "]\n",
    "\n",
    "print(\"[info] extracting sizes for each dataset directory...\")\n",
    "stats_data = []\n",
    "for name, path in datasets:\n",
    "    size_bytes = get_directory_size(path)\n",
    "    size_gb = size_bytes / (1024**3)\n",
    "    stats_data.append({\n",
    "        'dataset': name,\n",
    "        'path': path,\n",
    "        'size_bytes': size_bytes,\n",
    "        'size_gb': round(size_gb, 3)\n",
    "    })\n",
    "    print(f\"  • {name}: {size_gb:.3f} GB ({size_bytes:,} bytes)\")\n",
    "\n",
    "# create statistics dataframe\n",
    "import pandas as pd\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "\n",
    "# add row counts (will be populated after loading data)\n",
    "stats_df['row_count'] = 0  # placeholder, will update in next cell\n",
    "stats_df['column_count'] = 0  # placeholder, will update in next cell\n",
    "\n",
    "print(f\"\\n[info] dataset statistics summary:\")\n",
    "print(stats_df.to_string(index=False))\n",
    "\n",
    "# save as csv\n",
    "csv_path = f\"{LOCAL_SUPPLEMENTARY}dataset_statistics.csv\"\n",
    "stats_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n[saved] csv: {csv_path}\")\n",
    "\n",
    "# save as json\n",
    "json_path = f\"{LOCAL_SUPPLEMENTARY}dataset_statistics.json\"\n",
    "stats_dict = {\n",
    "    'total_size_gb': round(stats_df['size_gb'].sum(), 3),\n",
    "    'total_size_bytes': int(stats_df['size_bytes'].sum()),\n",
    "    'dataset_count': len(stats_df),\n",
    "    'datasets': stats_df.to_dict('records')\n",
    "}\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(stats_dict, f, indent=2)\n",
    "print(f\"[saved] json: {json_path}\")\n",
    "\n",
    "# save as png table image\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.axis('off')\n",
    "\n",
    "# create table with formatted data\n",
    "table_data = []\n",
    "for _, row in stats_df.iterrows():\n",
    "    table_data.append([\n",
    "        row['dataset'],\n",
    "        f\"{row['size_gb']:.3f} GB\",\n",
    "        f\"{row['size_bytes']:,}\",\n",
    "        str(row['row_count']) if row['row_count'] > 0 else 'TBD',\n",
    "        str(row['column_count']) if row['column_count'] > 0 else 'TBD'\n",
    "    ])\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=table_data,\n",
    "    colLabels=['Dataset', 'Size (GB)', 'Size (Bytes)', 'Rows', 'Columns'],\n",
    "    cellLoc='left',\n",
    "    loc='center',\n",
    "    colWidths=[0.25, 0.15, 0.25, 0.15, 0.15]\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# style header row\n",
    "for i in range(5):\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "plt.title('Dataset Statistics Summary', fontsize=14, weight='bold', pad=20)\n",
    "png_path = f\"{LOCAL_SUPPLEMENTARY}dataset_statistics.png\"\n",
    "plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "print(f\"[saved] png: {png_path}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Processing: Q2(validation)01\")\n",
    "# supports: Q2(b) — validate generated schemas against actual data with inferschema\n",
    "# does: loads 4 audio feature files with inferschema=true, compares to generated schemas, handles track_id column\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n[Q2(validation)] validating generated schemas against actual data...\")\n",
    "\n",
    "# dynamically discover available audio feature files (reference approach)\n",
    "# instead of hardcoding filenames, discover what actually exists\n",
    "try:\n",
    "    # list all files in audio features directory\n",
    "    files_df = spark.read.text(f\"{WASBS_DATA}audio/features/\")\n",
    "    available_files = [row.value.split('/')[-1] for row in files_df.collect() if row.value.endswith('.csv')]\n",
    "    \n",
    "    # build audio_files dictionary from available files\n",
    "    audio_files = {}\n",
    "    \n",
    "    # area of moments\n",
    "    aom_files = [f for f in available_files if 'area-of-moments' in f]\n",
    "    if aom_files:\n",
    "        audio_files['AO'] = (aom_files[0], 'area-of-moments')\n",
    "    \n",
    "    # lpc\n",
    "    lpc_files = [f for f in available_files if 'lpc' in f and 'area-of-moments' not in f]\n",
    "    if lpc_files:\n",
    "        audio_files['LP'] = (lpc_files[0], 'lpc')\n",
    "    \n",
    "    # spectral (discover what spectral files actually exist)\n",
    "    spectral_files = [f for f in available_files if 'spectral' in f]\n",
    "    if spectral_files:\n",
    "        audio_files['SP'] = (spectral_files[0], 'spectral')\n",
    "    else:\n",
    "        print(\"[warning] no spectral files found - skipping SP validation\")\n",
    "    \n",
    "    # timbral\n",
    "    timbral_files = [f for f in available_files if 'timbral' in f]\n",
    "    if timbral_files:\n",
    "        audio_files['TI'] = (timbral_files[0], 'timbral')\n",
    "    \n",
    "    print(f\"[discovered] {len(audio_files)} available audio feature files:\")\n",
    "    for code, (filename, desc) in audio_files.items():\n",
    "        print(f\"  {code}: {filename} ({desc})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"[error] could not discover files dynamically: {e}\")\n",
    "    # fallback to known good files only\n",
    "    audio_files = {\n",
    "        'AO': ('msd-jmir-area-of-moments-all-v1.0.csv', 'area-of-moments'),\n",
    "        'LP': ('msd-jmir-lpc-all-v1.0.csv', 'lpc'),\n",
    "        'TI': ('msd-marsyas-timbral-v1.0.csv', 'timbral')\n",
    "    }\n",
    "    print(\"[fallback] using known existing files only (no spectral)\")\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "for code, (filename, attr_key) in audio_files.items():\n",
    "    print(f\"\\n[validate] {code}: {filename}\")\n",
    "    \n",
    "    # load with inferschema\n",
    "    file_path = f\"{WASBS_DATA}audio/features/{filename}\"\n",
    "    df_inferred = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "    inferred_schema = df_inferred.schema\n",
    "    \n",
    "    print(f\"  • inferred columns: {len(inferred_schema.fields)}\")\n",
    "    print(f\"  • inferred types: {[str(f.dataType) for f in inferred_schema.fields[:3]]} ...\")\n",
    "    \n",
    "    # get generated schema\n",
    "    if attr_key in all_attributes:\n",
    "        generated_schema = schemas[attr_key]\n",
    "        print(f\"  • generated columns: {len(generated_schema.fields)}\")\n",
    "        print(f\"  • generated types: {[str(f.dataType) for f in generated_schema.fields[:3]]} ...\")\n",
    "        \n",
    "        # compare column counts\n",
    "        count_match = len(inferred_schema.fields) == len(generated_schema.fields)\n",
    "        \n",
    "        # check for extra track_id column\n",
    "        extra_column = len(inferred_schema.fields) == len(generated_schema.fields) + 1\n",
    "        \n",
    "        validation_results[code] = {\n",
    "            'filename': filename,\n",
    "            'inferred_columns': len(inferred_schema.fields),\n",
    "            'generated_columns': len(generated_schema.fields),\n",
    "            'count_match': count_match,\n",
    "            'extra_track_id': extra_column,\n",
    "            'status': 'PASS' if (count_match or extra_column) else 'MISMATCH'\n",
    "        }\n",
    "        \n",
    "        print(f\"  • validation: {validation_results[code]['status']}\")\n",
    "        if extra_column:\n",
    "            print(f\"  • note: detected extra track_id column (expected behaviour)\")\n",
    "    else:\n",
    "        print(f\"  • warning: no generated schema found for {attr_key}\")\n",
    "        validation_results[code] = {\n",
    "            'filename': filename,\n",
    "            'status': 'MISSING_SCHEMA'\n",
    "        }\n",
    "\n",
    "print(\"\\n[info] schema validation summary:\")\n",
    "for code, result in validation_results.items():\n",
    "    print(f\"  • {code}: {result['status']}\")\n",
    "\n",
    "# save validation results as json\n",
    "json_path = f\"{LOCAL_SUPPLEMENTARY}schema_validation.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(validation_results, f, indent=2)\n",
    "print(f\"\\n[saved] json: {json_path}\")\n",
    "\n",
    "# create validation comparison table as png\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = []\n",
    "for code, result in validation_results.items():\n",
    "    if result['status'] != 'MISSING_SCHEMA':\n",
    "        table_data.append([\n",
    "            code,\n",
    "            result['filename'],\n",
    "            result['inferred_columns'],\n",
    "            result['generated_columns'],\n",
    "            'Yes' if result['extra_track_id'] else 'No',\n",
    "            result['status']\n",
    "        ])\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=table_data,\n",
    "    colLabels=['Dataset', 'Filename', 'Inferred Cols', 'Generated Cols', 'Extra track_id', 'Status'],\n",
    "    cellLoc='left',\n",
    "    loc='center',\n",
    "    colWidths=[0.08, 0.35, 0.12, 0.12, 0.13, 0.10]\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# style header\n",
    "for i in range(6):\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# colour status cells\n",
    "for i, (code, result) in enumerate(validation_results.items(), 1):\n",
    "    if result['status'] == 'PASS':\n",
    "        table[(i, 5)].set_facecolor('#C6EFCE')\n",
    "    elif result['status'] == 'MISMATCH':\n",
    "        table[(i, 5)].set_facecolor('#FFC7CE')\n",
    "\n",
    "plt.title('Schema Validation Results', fontsize=14, weight='bold', pad=20)\n",
    "png_path = f\"{LOCAL_SUPPLEMENTARY}schema_comparison.png\"\n",
    "plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "print(f\"[saved] png: {png_path}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af305b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Processing: Q2(counts)01\")\n",
    "# supports: Q2 — document row counts for all datasets\n",
    "# does: counts rows in all dataframes (4 audio + genre + main + tasteprofile), saves as json/png\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n[Q2(counts)] counting rows in all datasets...\")\n",
    "\n",
    "row_counts = {}\n",
    "\n",
    "# count rows in audio feature datasets (using renamed dataframes if available)\n",
    "print(\"\\n[info] audio feature datasets:\")\n",
    "if 'renamed_dfs' in dir() and renamed_dfs:\n",
    "    for code, df in renamed_dfs.items():\n",
    "        count = df.count()\n",
    "        row_counts[f'audio_{code.lower()}'] = count\n",
    "        print(f\"  • {code}: {count:,} rows\")\n",
    "else:\n",
    "    print(\"  • warning: renamed_dfs not found, loading from original files...\")\n",
    "    for code, (filename, _) in audio_files.items():\n",
    "        file_path = f\"{WASBS_DATA}audio/features/{filename}\"\n",
    "        df = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "        count = df.count()\n",
    "        row_counts[f'audio_{code.lower()}'] = count\n",
    "        print(f\"  • {code}: {count:,} rows\")\n",
    "\n",
    "# count rows in other datasets (load if not already in memory)\n",
    "print(\"\\n[info] other datasets:\")\n",
    "\n",
    "# genre dataset\n",
    "try:\n",
    "    genre_path = f\"{WASBS_DATA}genre/\"\n",
    "    df_genre = spark.read.parquet(genre_path)\n",
    "    count = df_genre.count()\n",
    "    row_counts['genre'] = count\n",
    "    print(f\"  • genre: {count:,} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"  • genre: unable to load ({e})\")\n",
    "    row_counts['genre'] = 0\n",
    "\n",
    "# main dataset\n",
    "try:\n",
    "    main_path = f\"{WASBS_DATA}main/\"\n",
    "    df_main = spark.read.parquet(main_path)\n",
    "    count = df_main.count()\n",
    "    row_counts['main'] = count\n",
    "    print(f\"  • main: {count:,} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"  • main: unable to load ({e})\")\n",
    "    row_counts['main'] = 0\n",
    "\n",
    "# tasteprofile dataset\n",
    "try:\n",
    "    taste_path = f\"{WASBS_DATA}tasteprofile/\"\n",
    "    df_taste = spark.read.parquet(taste_path)\n",
    "    count = df_taste.count()\n",
    "    row_counts['tasteprofile'] = count\n",
    "    print(f\"  • tasteprofile: {count:,} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"  • tasteprofile: unable to load ({e})\")\n",
    "    row_counts['tasteprofile'] = 0\n",
    "\n",
    "print(f\"\\n[info] total datasets counted: {len(row_counts)}\")\n",
    "print(f\"[info] total rows across all datasets: {sum(row_counts.values()):,}\")\n",
    "\n",
    "# save as json\n",
    "json_path = f\"{LOCAL_SUPPLEMENTARY}row_counts.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(row_counts, f, indent=2)\n",
    "print(f\"\\n[saved] json: {json_path}\")\n",
    "\n",
    "# create row counts table as png\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = [[k.replace('_', ' ').title(), f\"{v:,}\"] for k, v in sorted(row_counts.items())]\n",
    "table_data.append(['TOTAL', f\"{sum(row_counts.values()):,}\"])\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=table_data,\n",
    "    colLabels=['Dataset', 'Row Count'],\n",
    "    cellLoc='left',\n",
    "    loc='center',\n",
    "    colWidths=[0.6, 0.4]\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# style header\n",
    "for i in range(2):\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# style total row\n",
    "last_row = len(table_data)\n",
    "for i in range(2):\n",
    "    table[(last_row, i)].set_facecolor('#E7E6E6')\n",
    "    table[(last_row, i)].set_text_props(weight='bold')\n",
    "\n",
    "plt.title('Row Counts by Dataset', fontsize=14, weight='bold', pad=20)\n",
    "png_path = f\"{LOCAL_SUPPLEMENTARY}row_counts.png\"\n",
    "plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "print(f\"[saved] png: {png_path}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Processing: Q2(d)visualise01\")\n",
    "# supports: Q2(d) — visualise renamed schemas for documentation\n",
    "# does: generates 4 png schema diagrams with column mappings, checks existence first to avoid regeneration\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n[Q2(d)] generating schema visualisation images...\")\n",
    "\n",
    "# define schema visualisations to create\n",
    "schema_vis = {\n",
    "    'AO': ('aom_schema.png', 'Area of Moments Schema'),\n",
    "    'LP': ('lpc_schema.png', 'Linear Predictive Coding Schema'),\n",
    "    'SP': ('spectral_schema.png', 'Spectral Features Schema'),\n",
    "    'TI': ('timbral_schema.png', 'Timbral Features Schema')\n",
    "}\n",
    "\n",
    "images_created = 0\n",
    "images_skipped = 0\n",
    "\n",
    "for code, (filename, title) in schema_vis.items():\n",
    "    png_path = f\"{LOCAL_SUPPLEMENTARY}{filename}\"\n",
    "    \n",
    "    # check if image already exists\n",
    "    if os.path.exists(png_path):\n",
    "        print(f\"  • {code}: skipped (already exists) - {png_path}\")\n",
    "        images_skipped += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"  • {code}: generating {filename}...\")\n",
    "    \n",
    "    # get renamed dataframe\n",
    "    if 'renamed_dfs' in dir() and code in renamed_dfs:\n",
    "        df = renamed_dfs[code]\n",
    "        \n",
    "        # extract schema information\n",
    "        schema_info = []\n",
    "        for i, field in enumerate(df.schema.fields, 1):\n",
    "            type_str = str(field.dataType).replace('Type()', '')\n",
    "            schema_info.append(f\"{field.name:<15} {type_str:<10}\")\n",
    "        \n",
    "        # create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, max(8, len(schema_info) * 0.3)))\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # add title\n",
    "        ax.text(0.5, 0.98, title, fontsize=14, weight='bold', ha='center', va='top')\n",
    "        ax.text(0.5, 0.95, f\"Total Columns: {len(schema_info)}\", fontsize=10, ha='center', va='top')\n",
    "        \n",
    "        # add schema text in columns (split if too many)\n",
    "        if len(schema_info) <= 30:\n",
    "            # single column\n",
    "            schema_text = '\\n'.join(schema_info)\n",
    "            ax.text(0.1, 0.90, schema_text, fontsize=8, family='monospace', va='top')\n",
    "        else:\n",
    "            # two columns\n",
    "            mid = len(schema_info) // 2\n",
    "            col1 = '\\n'.join(schema_info[:mid])\n",
    "            col2 = '\\n'.join(schema_info[mid:])\n",
    "            ax.text(0.05, 0.90, col1, fontsize=8, family='monospace', va='top')\n",
    "            ax.text(0.52, 0.90, col2, fontsize=8, family='monospace', va='top')\n",
    "        \n",
    "        plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"    ✓ saved: {png_path}\")\n",
    "        images_created += 1\n",
    "    else:\n",
    "        print(f\"    ✗ warning: renamed dataframe for {code} not found\")\n",
    "\n",
    "print(f\"\\n[summary] images created: {images_created}, skipped: {images_skipped}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Processing: Q2(schemas)save01\")\n",
    "# supports: Q2(b) — persist schemas and data samples for ai analysis\n",
    "# does: converts structtype schemas to json format, saves 10-row samples of each dataset as csv, creates metadata\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n[Q2(schemas)] saving schemas and data samples for ai analysis...\")\n",
    "\n",
    "# collect all schemas in json format\n",
    "all_schemas_json = {}\n",
    "\n",
    "for code, (filename, attr_key) in audio_files.items():\n",
    "    print(f\"\\n[save] {code}: {filename}\")\n",
    "    \n",
    "    # get renamed dataframe\n",
    "    if 'renamed_dfs' in dir() and code in renamed_dfs:\n",
    "        df = renamed_dfs[code]\n",
    "        \n",
    "        # convert schema to json-serialisable format\n",
    "        schema_json = {\n",
    "            'fields': [\n",
    "                {\n",
    "                    'name': f.name,\n",
    "                    'type': str(f.dataType),\n",
    "                    'nullable': f.nullable\n",
    "                }\n",
    "                for f in df.schema.fields\n",
    "            ],\n",
    "            'column_count': len(df.schema.fields)\n",
    "        }\n",
    "        all_schemas_json[code] = schema_json\n",
    "        print(f\"  • schema: {len(df.schema.fields)} fields\")\n",
    "        \n",
    "        # save sample data as csv (10 rows)\n",
    "        sample_csv_path = f\"{LOCAL_SUPPLEMENTARY}{code.lower()}_sample.csv\"\n",
    "        df.limit(10).toPandas().to_csv(sample_csv_path, index=False)\n",
    "        print(f\"  • sample csv: {sample_csv_path}\")\n",
    "        \n",
    "        # save individual statistics as json\n",
    "        stats = {\n",
    "            'dataset_code': code,\n",
    "            'filename': filename,\n",
    "            'columns': len(df.schema.fields),\n",
    "            'sample_rows': 10,\n",
    "            'column_names': [f.name for f in df.schema.fields],\n",
    "            'column_types': [str(f.dataType) for f in df.schema.fields]\n",
    "        }\n",
    "        stats_json_path = f\"{LOCAL_SUPPLEMENTARY}{code.lower()}_stats.json\"\n",
    "        with open(stats_json_path, 'w') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "        print(f\"  • stats json: {stats_json_path}\")\n",
    "    else:\n",
    "        print(f\"  • warning: renamed dataframe for {code} not found\")\n",
    "\n",
    "# save combined schemas json\n",
    "schemas_json_path = f\"{LOCAL_SUPPLEMENTARY}audio_schemas.json\"\n",
    "with open(schemas_json_path, 'w') as f:\n",
    "    json.dump(all_schemas_json, f, indent=2)\n",
    "print(f\"\\n[saved] combined schemas json: {schemas_json_path}\")\n",
    "print(f\"[info] schemas saved for {len(all_schemas_json)} datasets\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b263848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File sizes bar chart with logarithmic scale\n",
    "\n",
    "hprint(\"File Sizes Bar Chart Generation\")\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "# define the image path\n",
    "chart_path = f\"{LOCAL_SUPPLEMENTARY}file_sizes_chart.png\"\n",
    "\n",
    "# check if chart already exists\n",
    "if os.path.exists(chart_path):\n",
    "    print(f\"[info] file sizes chart exists, displaying from disk\")\n",
    "    print(f\"[display] reading from: {chart_path}\")\n",
    "else:\n",
    "    print(f\"[info] file sizes chart not found, generating...\")\n",
    "    \n",
    "    # read the dataset statistics\n",
    "    stats_path = f\"{LOCAL_SUPPLEMENTARY}dataset_statistics.csv\"\n",
    "    \n",
    "    if os.path.exists(stats_path):\n",
    "        # load data from CSV\n",
    "        stats_df = pd.read_csv(stats_path)\n",
    "        \n",
    "        # filter out datasets with zero size and prepare data\n",
    "        size_data = stats_df[stats_df['size_bytes'] > 0].copy()\n",
    "        \n",
    "        # calculate size in MB for better readability\n",
    "        size_data['size_mb'] = size_data['size_bytes'] / (1024**2)\n",
    "        \n",
    "        # sort by size for better visualization\n",
    "        size_data = size_data.sort_values('size_bytes', ascending=True)\n",
    "        \n",
    "        # create the bar chart\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # create bars with logarithmic scale\n",
    "        bars = ax.barh(range(len(size_data)), size_data['size_mb'], \n",
    "                      color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2'])\n",
    "        \n",
    "        # set logarithmic scale for x-axis\n",
    "        ax.set_xscale('log')\n",
    "        \n",
    "        # customize the plot\n",
    "        ax.set_yticks(range(len(size_data)))\n",
    "        ax.set_yticklabels(size_data['dataset'], fontsize=10)\n",
    "        ax.set_xlabel('Size (MB) - Logarithmic Scale', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Dataset', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Dataset File Sizes (Logarithmic Scale)', fontsize=14, fontweight='bold', pad=20)\n",
    "        \n",
    "        # add size labels on bars\n",
    "        for i, (idx, row) in enumerate(size_data.iterrows()):\n",
    "            size_mb = row['size_mb']\n",
    "            size_gb = row['size_gb']\n",
    "            \n",
    "            # position label based on bar length\n",
    "            label_x = size_mb * 1.1 if size_mb > 1 else size_mb + 0.1\n",
    "            \n",
    "            if size_gb >= 1.0:\n",
    "                label_text = f\"{size_gb:.2f} GB\"\n",
    "            else:\n",
    "                label_text = f\"{size_mb:.1f} MB\"\n",
    "                \n",
    "            ax.text(label_x, i, label_text, va='center', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        # add grid for better readability\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # adjust layout and save\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(chart_path, bbox_inches='tight', dpi=150, facecolor='white')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"[saved] file sizes chart: {chart_path}\")\n",
    "        print(f\"[info] processed {len(size_data)} datasets\")\n",
    "        print(f\"[info] size range: {size_data['size_mb'].min():.2f} MB to {size_data['size_mb'].max():.2f} MB\")\n",
    "        \n",
    "    else:\n",
    "        # create fallback chart if statistics file is missing\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.text(0.5, 0.5, 'Dataset statistics file not available\\nRun previous cells to generate file sizes data', \n",
    "                ha='center', va='center', fontsize=14,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightcoral\", alpha=0.7))\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "        plt.title('File Sizes Chart - Data Not Available', fontsize=14, fontweight='bold')\n",
    "        plt.savefig(chart_path, bbox_inches='tight', dpi=150, facecolor='white')\n",
    "        plt.close()\n",
    "        print(f\"[warning] statistics file not found: {stats_path}\")\n",
    "        print(f\"[saved] fallback chart: {chart_path}\")\n",
    "\n",
    "# always display the chart from disk\n",
    "if os.path.exists(chart_path):\n",
    "    print(f\"[display] showing file sizes chart\")\n",
    "    display(Image(filename=chart_path))\n",
    "else:\n",
    "    print(f\"[error] chart file not available: {chart_path}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c70b30",
   "metadata": {},
   "source": [
    "## Processing Section Complete\n",
    "\n",
    "All required artifacts have been generated:\n",
    "\n",
    "**Dataset Documentation:**\n",
    "- `dataset_statistics.csv/json/png` - Complete dataset inventory with sizes\n",
    "- `row_counts.json/png` - Row counts for all datasets\n",
    "\n",
    "**Schema Validation:**\n",
    "- `schema_validation.json` - Validation results comparing generated vs inferred schemas\n",
    "- `schema_comparison.png` - Visual comparison table\n",
    "\n",
    "**Schema Documentation:**\n",
    "- `audio_schemas.json` - All 4 audio feature schemas in JSON format\n",
    "- `aom_schema.png` - Area of Moments schema diagram\n",
    "- `lpc_schema.png` - Linear Predictive Coding schema diagram\n",
    "- `spectral_schema.png` - Spectral features schema diagram\n",
    "- `timbral_schema.png` - Timbral features schema diagram\n",
    "\n",
    "**Data Samples (AI-readable):**\n",
    "- `ao_sample.csv`, `lp_sample.csv`, `sp_sample.csv`, `ti_sample.csv` - 10 rows each\n",
    "- `ao_stats.json`, `lp_stats.json`, `sp_stats.json`, `ti_stats.json` - Individual metadata\n",
    "\n",
    "All files saved to: `../report/supplementary/`\n",
    "\n",
    "**Ready for Audio Similarity Section**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
