{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bbb004-5a22-4f65-b400-cb2bc3edf217",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017705b0-281d-4565-abe1-852ebc5ca888",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m username\n\u001b[0;32m---> 18\u001b[0m username \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@.*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, getpass\u001b[38;5;241m.\u001b[39mgetuser())\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m azure_account_name\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m azure_data_container_name\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else: \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "    global spark\n",
    "    global sc\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "        spark.stop()\n",
    "        del spark\n",
    "        del sc\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136ef6f-3f20-4213-884d-677723f0694c",
   "metadata": {},
   "source": [
    "### -  –––––––––––––––––––– Assignment 2 BEGINS HERE ––––––––––––––––––––- - ###\n",
    "\n",
    "- MSD containers:\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/` \n",
    "\n",
    "- MY containers:\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31c204-813f-4003-9d21-c3b9c57a065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Imports\n",
    "\n",
    "# group 1: from imports (alphabetical by module)\n",
    "from collections         import Counter\n",
    "from datetime            import datetime\n",
    "from IPython.display     import display, HTML, Image\n",
    "from math                import acos, atan2, cos, radians, sin, sqrt\n",
    "from matplotlib.ticker   import FuncFormatter, MaxNLocator\n",
    "from pathlib             import Path\n",
    "from pyspark.sql         import DataFrame, DataFrame as SparkDF\n",
    "from pyspark.sql         import functions as F, types as T\n",
    "from pyspark.sql.types   import *\n",
    "from pyspark.sql.utils   import AnalysisException\n",
    "from pyspark.sql.window  import Window\n",
    "from rich.console        import Console\n",
    "from rich.tree           import Tree\n",
    "from time                import perf_counter\n",
    "from typing              import List, Optional, Tuple\n",
    "\n",
    "# group 2: import ... as ... (alphabetical)\n",
    "import itertools         as it\n",
    "import matplotlib.dates  as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "\n",
    "# group 3: import statements (alphabetical)\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import platform\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb102f08-d26f-4475-8561-7f63862da060",
   "metadata": {},
   "source": [
    "#The following shows the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c8751-d59a-423b-8f39-d5e6a540944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall time metric\n",
    "notebook_run_time = time.time()\n",
    "\n",
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "#USERNAME    = \"dew59\"\n",
    "WASBS_DATA  = \"wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/\"\n",
    "WASBS_USER  = f\"wasbs://campus-user@madsstorage002.blob.core.windows.net/{username}-A2/\"\n",
    "\n",
    "#WASBS_USER          = \"wasbs://campus-user@madsstorage002.blob.core.windows.net/{}\".format(USERNAME)\n",
    "#WASBS_YEAR_SIZE     = \"{}/years_size_metrics.parquet/\".format(WASBS_USER)\n",
    "\n",
    " \n",
    "#stations_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/{stations_write_path}'\n",
    "#common_data_path    = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/'\n",
    "#stations_read_name  =  inventory_read_name = \"\"\n",
    "#stations_read_name  =  inventory_read_name = \"\"\n",
    " \n",
    "\n",
    "print(\"Spark:\", spark.version)\n",
    "print(\"_\" * 35 + \"PATHS\" + \"_\" * 35)\n",
    "print(\"WASBS_DATA          :\", WASBS_DATA)\n",
    "print(\"WASBS_USER          :\", WASBS_USER) \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b56fb-a403-4903-ba60-b0f33495ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER AND DIAGNOSTIC FUNCTIONS\n",
    "\n",
    "notebook_run_time = time.time()\n",
    "print(\"_\" * 35 + \"HELPER / DIAGNOSTIC FUNCTIONS\" + \"_\" * 35)\n",
    "\n",
    "def hprint(text: str=\"\", l=50):\n",
    "    \"\"\"Print formatted section header\"\"\"\n",
    "    if len(text) > 0: \n",
    "        text = f\" {text} \"\n",
    "    n = len(text)\n",
    "    n = abs(n - l) // 2\n",
    "    print(\"\\n\" + \"–\" * n + text + \"–\" * n)\n",
    "\n",
    "def cleanup_parquet_files(cleanup=False):\n",
    "    \"\"\"Clean up existing parquet files in user directory.\n",
    "    \n",
    "    Args:\n",
    "        cleanup (bool): When True, actually DELETES FILES. \n",
    "                        When False, only LISTS files.\n",
    "    \"\"\"\n",
    "    hprint(\"Clean up existing parquet files\")\n",
    "\n",
    "    print(\"[cleanup] Listing files BEFORE cleanup:\")\n",
    "    get_ipython().system(f'hdfs dfs -ls {WASBS_USER}/*.parquet')\n",
    "    \n",
    "    if cleanup:\n",
    "        print(\"\\n[cleanup] Deleting all parquet folders...\")\n",
    "        get_ipython().system(f'hdfs dfs -rm -r -f {WASBS_USER}/*.parquet')\n",
    "        \n",
    "        print(\"\\n[info] Listing files AFTER cleanup:\")\n",
    "        get_ipython().system(f'hdfs dfs -ls {WASBS_USER}/*.parquet')\n",
    "        print(\"\\n[cleanup] Parquet file cleanup complete - ready to restart Processing run with clean schema\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n[info] To actually delete files, call: cleanup_parquet_files(cleanup=True)\")\n",
    "\n",
    "def normalise_ids(df: DataFrame, col: str = \"ID\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    # Single source of truth for ID normalisation \n",
    "    Upper + trim + distinct on the given ID column.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] normalise_ids() on column: {col}\")\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "    return df.select(F.upper(F.trim(F.col(col))).alias(\"ID\")).distinct()\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "\n",
    "def show_df(df, n=10, name=\"DataFrame\", max_width=None, right_align=False, left_trim=True, total_info=True):\n",
    "    \"\"\"\n",
    "    Enhanced DataFrame display function with multiple formatting options.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to display\n",
    "        n (int): Number of rows to show (default: 10)\n",
    "        name (str): Name/title for the DataFrame\n",
    "        max_width (int): Maximum column width for truncation\n",
    "        right_align (bool): Whether to right-align numeric columns\n",
    "        left_trim (bool): Whether to trim whitespace from string columns\n",
    "        total_info (bool): Whether to show total row count info\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(f\"[show_df] {name}: DataFrame is None\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Get total count if requested\n",
    "        if total_info:\n",
    "            total_rows = df.count()\n",
    "            print(f\"[show_df] {name}: Showing {min(n, total_rows)} of {total_rows} rows\")\n",
    "        else:\n",
    "            print(f\"[show_df] {name}: Showing {n} rows\")\n",
    "        \n",
    "        # Configure display options\n",
    "        if max_width:\n",
    "            df.show(n, truncate=max_width)\n",
    "        else:\n",
    "            df.show(n, truncate=False)\n",
    "            \n",
    "        # Show schema info\n",
    "        print(f\"[schema] {len(df.columns)} columns: {', '.join(df.columns[:5])}{' ...' if len(df.columns) > 5 else ''}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[show_df] Error displaying {name}: {str(e)}\")\n",
    "\n",
    "def benchmark_function(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Benchmark a function's execution time.\n",
    "    \n",
    "    Args:\n",
    "        func: Function to benchmark\n",
    "        *args: Arguments to pass to the function\n",
    "        **kwargs: Keyword arguments to pass to the function\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (result, execution_time_seconds)\n",
    "    \"\"\"\n",
    "    start_time = perf_counter()\n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        print(f\"[benchmark] {func.__name__}: {execution_time:.3f} seconds\")\n",
    "        return result, execution_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"[benchmark] {func.__name__} FAILED after {execution_time:.3f} seconds: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def save_to_parquet(df, path: str, mode: str = \"overwrite\", check_exists: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Save DataFrame to parquet with enhanced error handling and timing.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to save\n",
    "        path (str): HDFS path to save to\n",
    "        mode (str): Write mode ('overwrite', 'append', etc.)\n",
    "        check_exists (bool): Whether to check if path already exists\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = perf_counter()\n",
    "        \n",
    "        if check_exists and has_parquet(path):\n",
    "            print(f\"[save_parquet] Path already exists: {path}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"[save_parquet] Writing to: {path}\")\n",
    "        df.write.mode(mode).parquet(path)\n",
    "        \n",
    "        end_time = perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "        \n",
    "        print(f\"[save_parquet] Completed in {elapsed:.2f} seconds\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[save_parquet] Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def has_parquet(dir_as_path: str) -> bool:\n",
    "    path   = _normalise_dir( dir_as_path)\n",
    "    marker = path + '_SUCCESS'\n",
    "    #print(\"\\n[check] dir_path:\", dir_path)\n",
    "    #print(\"\\n[check] path    :\", path)\n",
    "    print(\"\\n[check] marker  :\", marker)\n",
    "    rc = os.system(f'hdfs dfs -test -e \"{marker}\"')\n",
    "    print(\"[check] rc:\", rc, \"->\", (\"exists\" if rc == 0 else \"missing\"))\n",
    "    return (rc == 0)\n",
    "\n",
    "def _to_spark(df_like, schema=None):\n",
    "    \"\"\"\n",
    "    Return a Spark DataFrame  .\n",
    "    \"\"\"\n",
    "    if isinstance(df_like, SparkDF):\n",
    "        return df_like\n",
    "    return spark.createDataFrame(df_like, schema=schema) if schema else spark.createDataFrame(df_like)\n",
    "\n",
    "def ensure_dir(path: str) -> str:\n",
    "    \"\"\"\n",
    "    ensures that path is a path \n",
    "    and not representing a file;\n",
    "    add trailing slash if needed\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        raise ValueError(\"Path is None\")\n",
    "    path = _normalise_dir(path)\n",
    "#   print(\"ensure_dir -> \",path)\n",
    "    return path\n",
    "\n",
    "def _normalise_dir(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure trailing slash so we point to\n",
    "    the directory rather than the prefix.\n",
    "    \"\"\"\n",
    "    if not s.endswith(\"/\"):\n",
    "        s += \"/\"\n",
    "    return s\n",
    "\n",
    "def write_parquet(df, path: str, mode: str = \"overwrite\") -> None:\n",
    "    funct_time = time.time()\n",
    "    print(f\"Writing to parquet: {path}\")\n",
    "    df.write.mode(mode).parquet(path)\n",
    "    funct_time = time.time() - funct_time\n",
    "    print(f\"[time] write_parquet (min)   : {funct_time/60:5.2f}\")\n",
    "    print(f\"[time] write_parquet (sec)   : {funct_time:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c42b0-9d6d-4eb4-89df-590eb55cb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USE SPARINGLY - these are for diagnostics only\n",
    "# Set cleanup=True to actually delete files, or False to just list them \n",
    "# LEAVE cleanup=False after running this cell once! \n",
    "# if they have been created and are correct, change cleanup=False for quicker runs. \n",
    "cleanup_parquet_files(cleanup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586fca6e-683f-4426-b354-175f8eeb5000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# overall time metric\n",
    "start_notebook = time.time() \n",
    "start_time = datetime.fromtimestamp(start_notebook).strftime(\"%Y.%m.%d %H:%M\")\n",
    "\n",
    "hprint(f\"started at: {start_time}\")\n",
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "#!hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/msd/\n",
    "!hdfs dfs -ls    -h {WASBS_DATA} \n",
    "!hdfs dfs -du -s -h {WASBS_DATA} \n",
    "!hdfs dfs -ls    -h {WASBS_USER} \n",
    "!hdfs dfs -du -s -h {WASBS_USER} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c741463-9dfb-4c39-ba88-1d35afcecedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_time = time.time() \n",
    "result = get_ipython().getoutput(f\"hdfs dfs -du -s {WASBS_DATA}\") \n",
    "\n",
    "print(\"Raw result:\", result)\n",
    "print()\n",
    "data_size_bytes = int(result[0].split()[0])\n",
    "print(\"firstpass size (bytes):\", data_size_bytes)\n",
    "print(f\"firstpass size (MB)   : {data_size_bytes / (1024**2):.3f}\")\n",
    " \n",
    "lines = get_ipython().getoutput(f\"hdfs dfs -ls {WASBS_DATA}\")\n",
    "print()\n",
    "#other_size_bytes = 0\n",
    "#for line in lines:\n",
    "#    parts = line.split()\n",
    "#    if len(parts) >= 6 and parts[0].startswith('-'):   # file, not directory\n",
    "#        size = int(parts[2])                           # file size is parts[2] in your env\n",
    "#        other_size_bytes += size\n",
    "#print()\n",
    "#print(\"_____________________________________________________\") \n",
    "#print(f\"[result] daily size (bytes): {daily_size_bytes:,d}\")\n",
    "#print(f\"[result] daily size (MB)   : {daily_size_bytes / (1024**2):.2f}\")\n",
    "#print(f\"[result] meta-data (bytes) : {other_size_bytes:,d}\")\n",
    "#print(f\"[result] meta-data (MB)    : {other_size_bytes / (1024**2):.2f}\")\n",
    "\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc5edd-f44d-41c9-8add-129638a53a8f",
   "metadata": {},
   "source": [
    "### Q1 - Directory Tree Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab232eb-ade2-4400-b722-11708d97ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1(a) - Get the file structure and display directory tree\n",
    "\n",
    "png_path = '../report/supplementary/msd_directory_tree.png'\n",
    "\n",
    "# ensure directory exists\n",
    "os.makedirs(os.path.dirname(png_path), exist_ok=True)\n",
    "\n",
    "# check if png already exists\n",
    "if os.path.exists(png_path):\n",
    "    print(f\"[info] directory tree image exists, skipping generation\")\n",
    "    print(f\"[display] reading from disk: {png_path}\")\n",
    "else:\n",
    "    print(f\"[info] directory tree image not found, generating...\")\n",
    "    \n",
    "    # build directory tree dataframe\n",
    "    tree_df = build_directory_tree_df(WASBS_DATA, max_depth=3)\n",
    "    \n",
    "    # save to parquet in wasbs_user\n",
    "    tree_parquet_path = f\"{WASBS_USER}msd_directory_tree.parquet/\"\n",
    "    tree_df.write.mode(\"overwrite\").parquet(tree_parquet_path)\n",
    "    print(f\"[saved] tree dataframe: {tree_parquet_path}\")\n",
    "    \n",
    "    # create rich console visualisation and save as png\n",
    "    try:\n",
    "        # create rich tree visualisation\n",
    "        console_tree = Console(record=True, width=120)\n",
    "        tree = Tree(f\"[green]{WASBS_DATA}[/green]\")\n",
    "        \n",
    "        # build tree from dataframe\n",
    "        tree_data = tree_df.orderBy(\"level\", \"path\").collect()\n",
    "        path_to_node = {None: tree}\n",
    "        \n",
    "        for row in tree_data:\n",
    "            parent_node = path_to_node.get(row.parent_path, tree)\n",
    "            if row.type == \"dir\":\n",
    "                node = parent_node.add(f\"[bold cyan]{row.name}/[/bold cyan]\")\n",
    "                path_to_node[row.path] = node\n",
    "            else:\n",
    "                size_mb = row.size / (1024**2) if row.size > 0 else 0\n",
    "                parent_node.add(f\"{row.name} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        # export to svg then convert to png\n",
    "        console_tree.print(tree)\n",
    "        svg_output = console_tree.export_svg(title=\"MSD Directory Tree\")\n",
    "        \n",
    "        # save svg temporarily\n",
    "        svg_path = png_path.replace('.png', '.svg')\n",
    "        with open(svg_path, 'w') as f:\n",
    "            f.write(svg_output)\n",
    "        \n",
    "        # convert svg to png using cairosvg or imagemagick\n",
    "        try:\n",
    "            import cairosvg\n",
    "            cairosvg.svg2png(url=svg_path, write_to=png_path, dpi=150)\n",
    "        except ImportError:\n",
    "            # fallback: use matplotlib to create simple tree visualisation\n",
    "            fig, ax = plt.subplots(figsize=(14, 10))\n",
    "            ax.axis('off')\n",
    "            tree_text = \"\\n\".join([f\"{'  ' * row.level}{row.name}{'/' if row.type == 'dir' else ''}\" \n",
    "                                   for row in tree_data[:50]])  # limit to 50 items\n",
    "            ax.text(0.05, 0.95, tree_text, fontsize=8, family='monospace', va='top')\n",
    "            plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "            plt.close()\n",
    "        \n",
    "        print(f\"[saved] directory tree image: {png_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[error] failed to generate tree image: {e}\")\n",
    "        print(f\"[fallback] creating simple text-based image...\")\n",
    "        \n",
    "        # simple fallback: create text-based visualisation\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "        ax.axis('off')\n",
    "        ax.text(0.5, 0.5, \"Directory tree visualisation\\n(see parquet file for details)\", \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"[saved] fallback image: {png_path}\")\n",
    "\n",
    "# always display from disk (whether just created or already existed)\n",
    "if os.path.exists(png_path):\n",
    "    print(f\"[display] showing directory tree from: {png_path}\")\n",
    "    display(Image(filename=png_path))\n",
    "else:\n",
    "    print(f\"[warning] directory tree image not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1(b) - Parse the structure file and calculate summary statistics\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Q1(b) - Summary Statistics\")\n",
    "\n",
    "import re\n",
    "\n",
    "# Parse hdfs ls -R output to extract size and path\n",
    "def parse_ls_line(line):\n",
    "    \"\"\"Parse a single line from hdfs ls -R output\"\"\"\n",
    "    # Format: permissions replication user group size date time path\n",
    "    # Example: -rw-r--r--   3 hdfs supergroup   1051 2024-01-15 10:30 /path/to/file\n",
    "    parts = line.split()\n",
    "    if len(parts) < 8:\n",
    "        return None\n",
    "    \n",
    "    permissions = parts[0]\n",
    "    size_str = parts[4]\n",
    "    path = parts[-1]\n",
    "    \n",
    "    # Only process files (not directories)\n",
    "    if not permissions.startswith('d'):\n",
    "        try:\n",
    "            size = int(size_str)\n",
    "            return {'size': size, 'path': path, 'is_dir': False}\n",
    "        except ValueError:\n",
    "            return None\n",
    "    else:\n",
    "        return {'size': 0, 'path': path, 'is_dir': True}\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Read and parse the data structure file\n",
    "try:\n",
    "    with open(\"data_structure.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    file_count = 0\n",
    "    dir_count = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        parsed = parse_ls_line(line.strip())\n",
    "        if parsed:\n",
    "            if parsed['is_dir']:\n",
    "                dir_count += 1\n",
    "            else:\n",
    "                file_count += 1\n",
    "                total_size += parsed['size']\n",
    "    \n",
    "    print(f\"\\n[summary] directories: {dir_count}\")\n",
    "    print(f\"[summary] files: {file_count}\")\n",
    "    print(f\"[summary] total size: {total_size:,} bytes ({total_size/(1024**3):.2f} GB)\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"[error] data_structure.txt not found. Please run the previous cell first.\")\n",
    "except Exception as e:\n",
    "    print(f\"[error] Failed to parse structure file: {e}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2318c1-9ef3-4db9-a5cb-7d028114864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    " \n",
    "#stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47253f6c-7c2c-47f1-a55c-a16210d00c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff6b9d78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q2 - Exploring the Audio Dataset\n",
    "\n",
    "In this section, we will examine the audio feature datasets, understand their schemas, and develop a systematic approach to working with their column names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b40b20",
   "metadata": {},
   "source": [
    "### Q2(a) - Load Audio Feature Attribute Names and Types\n",
    "\n",
    "The audio feature datasets are stored in two locations:\n",
    "- **Attributes directory**: Contains CSV files defining column names and data types\n",
    "- **Features directory**: Contains the actual feature data (partitioned CSV directories)\n",
    "\n",
    "Each attribute file follows the format: `attribute_name,type`\n",
    "\n",
    "We'll examine these attribute files to understand how they can be used to define schemas for loading the feature datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) - Examine attribute files to understand column naming\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Q2(a) - Audio Feature Attributes Analysis\")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# List all audio feature datasets\n",
    "audio_datasets = [\n",
    "    'msd-jmir-area-of-moments-all-v1.0',\n",
    "    'msd-jmir-lpc-all-v1.0',\n",
    "    'msd-jmir-methods-of-moments-all-v1.0',\n",
    "    'msd-jmir-mfcc-all-v1.0',\n",
    "    'msd-jmir-spectral-all-all-v1.0',\n",
    "    'msd-jmir-spectral-derivatives-all-all-v1.0',\n",
    "    'msd-marsyas-timbral-v1.0',\n",
    "    'msd-mvd-v1.0',\n",
    "    'msd-rh-v1.0',\n",
    "    'msd-rp-v1.0',\n",
    "    'msd-ssd-v1.0',\n",
    "    'msd-trh-v1.0',\n",
    "    'msd-tssd-v1.0'\n",
    "]\n",
    "\n",
    "print(f\"[info] Found {len(audio_datasets)} audio feature datasets\\n\")\n",
    "\n",
    "# Function to read and parse attribute file\n",
    "def load_attribute_file(dataset_prefix):\n",
    "    \"\"\"\n",
    "    Load attribute names and types from an attribute CSV file.\n",
    "    \n",
    "    Args:\n",
    "        dataset_prefix: Name of the dataset (e.g., 'msd-jmir-lpc-all-v1.0')\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: [(attribute_name, attribute_type), ...]\n",
    "    \"\"\"\n",
    "    attr_path = f\"{WASBS_DATA}/audio/attributes/{dataset_prefix}.attributes.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Read attribute file as text\n",
    "        attr_rdd = spark.sparkContext.textFile(attr_path)\n",
    "        attributes = []\n",
    "        \n",
    "        for line in attr_rdd.collect():\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) >= 2:\n",
    "                attr_name = parts[0]\n",
    "                attr_type = parts[1].lower()\n",
    "                attributes.append((attr_name, attr_type))\n",
    "            elif len(parts) == 1:\n",
    "                # Some files might have just the attribute name\n",
    "                attributes.append((parts[0], 'string'))\n",
    "        \n",
    "        return attributes\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[error] Failed to load {dataset_prefix}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load attributes for all datasets\n",
    "print(\"[info] Loading attribute files...\")\n",
    "all_attributes = {}\n",
    "\n",
    "for dataset in audio_datasets:\n",
    "    attrs = load_attribute_file(dataset)\n",
    "    all_attributes[dataset] = attrs\n",
    "    print(f\"[loaded] {dataset}: {len(attrs)} attributes\")\n",
    "\n",
    "print(f\"\\n[summary] Loaded attribute information for {len(all_attributes)} datasets\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fafde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) continued - Display sample column names from each dataset\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Sample Column Names from Each Dataset\")\n",
    "\n",
    "print(\"Examining actual column names to understand naming patterns:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset, attributes in all_attributes.items():\n",
    "    if attributes:\n",
    "        print(f\"\\n[dataset] {dataset}\")\n",
    "        print(f\"[count]   {len(attributes)} columns\")\n",
    "        print(f\"[sample]  First 5 columns:\")\n",
    "        \n",
    "        # Show first 5 column names\n",
    "        for i, (name, dtype) in enumerate(attributes[:5], 1):\n",
    "            # Truncate long names for display\n",
    "            display_name = name if len(name) <= 60 else name[:57] + \"...\"\n",
    "            print(f\"  {i}. {display_name:60s} ({dtype})\")\n",
    "        \n",
    "        if len(attributes) > 5:\n",
    "            print(f\"  ... ({len(attributes) - 5} more columns)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) continued - Analyze column name characteristics\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Column Name Characteristics Analysis\")\n",
    "\n",
    "print(\"Analyzing column name patterns for Q2(c) discussion:\\n\")\n",
    "\n",
    "# Collect statistics about column names\n",
    "all_column_names = []\n",
    "column_lengths = []\n",
    "data_types_used = Counter()\n",
    "\n",
    "for dataset, attributes in all_attributes.items():\n",
    "    for name, dtype in attributes:\n",
    "        all_column_names.append(name)\n",
    "        column_lengths.append(len(name))\n",
    "        data_types_used[dtype] += 1\n",
    "\n",
    "# Calculate statistics\n",
    "avg_length = sum(column_lengths) / len(column_lengths) if column_lengths else 0\n",
    "max_length = max(column_lengths) if column_lengths else 0\n",
    "min_length = min(column_lengths) if column_lengths else 0\n",
    "\n",
    "print(f\"[total columns] {len(all_column_names)} across all datasets\")\n",
    "print(f\"\\n[column name length statistics]\")\n",
    "print(f\"  Average: {avg_length:.1f} characters\")\n",
    "print(f\"  Maximum: {max_length} characters\")\n",
    "print(f\"  Minimum: {min_length} characters\")\n",
    "\n",
    "print(f\"\\n[data types used]\")\n",
    "for dtype, count in sorted(data_types_used.items()):\n",
    "    print(f\"  {dtype:10s}: {count:4d} columns ({count/len(all_column_names)*100:.1f}%)\")\n",
    "\n",
    "# Find longest column names\n",
    "print(f\"\\n[longest column names (top 10)]\")\n",
    "name_length_pairs = [(name, len(name)) for name in all_column_names]\n",
    "name_length_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (name, length) in enumerate(name_length_pairs[:10], 1):\n",
    "    display_name = name if length <= 70 else name[:67] + \"...\"\n",
    "    print(f\"  {i:2d}. {display_name:70s} ({length} chars)\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) continued - Check for column name collisions\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Column Name Collision Detection\")\n",
    "\n",
    "print(\"Checking if any column names appear in multiple datasets...\")\n",
    "print(\"(CRITICAL for Q2(c) discussion and Q2(d) renaming strategy)\\n\")\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Build a dictionary: column_name -> [list of datasets that have it]\n",
    "column_to_datasets = defaultdict(list)\n",
    "\n",
    "for dataset, attributes in all_attributes.items():\n",
    "    for name, dtype in attributes:\n",
    "        column_to_datasets[name].append(dataset)\n",
    "\n",
    "# Find columns that appear in multiple datasets\n",
    "collisions = {name: datasets for name, datasets in column_to_datasets.items() \n",
    "              if len(datasets) > 1}\n",
    "\n",
    "if collisions:\n",
    "    print(f\"[ALERT] Found {len(collisions)} column names that appear in multiple datasets!\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sort by number of occurrences (most common first)\n",
    "    sorted_collisions = sorted(collisions.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    # Show top 20 most common collisions\n",
    "    print(f\"\\nTop {min(20, len(sorted_collisions))} most common colliding column names:\\n\")\n",
    "    \n",
    "    for i, (col_name, datasets) in enumerate(sorted_collisions[:20], 1):\n",
    "        print(f\"{i:2d}. '{col_name}' appears in {len(datasets)} datasets:\")\n",
    "        for ds in datasets:\n",
    "            # Abbreviate dataset name for display\n",
    "            ds_abbr = ds.replace('msd-', '').replace('-all-v1.0', '')\n",
    "            print(f\"    - {ds_abbr}\")\n",
    "        print()\n",
    "        \n",
    "    if len(sorted_collisions) > 20:\n",
    "        print(f\"... and {len(sorted_collisions) - 20} more colliding column names\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n[CONCLUSION] WITHOUT renaming, merging datasets would cause column name conflicts!\")\n",
    "    print(f\"[CONCLUSION] This demonstrates the NEED for systematic column renaming (Q2d)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"[OK] No column name collisions detected\")\n",
    "    print(f\"[OK] However, descriptive prefixes would still improve clarity when merging\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) continued - Focus on the 4 required datasets for Audio Similarity\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Required Datasets Analysis (for Audio Similarity)\")\n",
    "\n",
    "# These are the 4 datasets required for Audio Similarity Q1\n",
    "required_datasets = [\n",
    "    'msd-jmir-area-of-moments-all-v1.0',\n",
    "    'msd-jmir-lpc-all-v1.0',\n",
    "    'msd-jmir-spectral-all-all-v1.0',\n",
    "    'msd-marsyas-timbral-v1.0'\n",
    "]\n",
    "\n",
    "print(\"The Audio Similarity section requires merging these 4 specific datasets:\\n\")\n",
    "\n",
    "total_columns = 0\n",
    "for dataset in required_datasets:\n",
    "    if dataset in all_attributes:\n",
    "        col_count = len(all_attributes[dataset])\n",
    "        total_columns += col_count\n",
    "        print(f\"[{dataset}]\")\n",
    "        print(f\"  Columns: {col_count}\")\n",
    "        print()\n",
    "\n",
    "print(f\"[TOTAL] {total_columns} columns after merging (excluding MSD_TRACKID)\")\n",
    "print(f\"[NOTE]  Plus 1 MSD_TRACKID column = {total_columns + 1} total columns\\n\")\n",
    "\n",
    "# Check for collisions among just these 4 datasets\n",
    "print(\"Checking for collisions among these 4 required datasets...\")\n",
    "required_column_to_datasets = defaultdict(list)\n",
    "\n",
    "for dataset in required_datasets:\n",
    "    if dataset in all_attributes:\n",
    "        for name, dtype in all_attributes[dataset]:\n",
    "            required_column_to_datasets[name].append(dataset)\n",
    "\n",
    "required_collisions = {name: datasets for name, datasets in required_column_to_datasets.items() \n",
    "                       if len(datasets) > 1}\n",
    "\n",
    "if required_collisions:\n",
    "    print(f\"\\n[ALERT] Found {len(required_collisions)} collisions among the 4 required datasets:\")\n",
    "    for col_name, datasets in sorted(required_collisions.items()):\n",
    "        print(f\"  - '{col_name}' in: {', '.join([d.replace('msd-', '').replace('-all-v1.0', '').replace('-all-all-v1.0', '') for d in datasets])}\")\n",
    "else:\n",
    "    print(f\"\\n[OK] No collisions among the 4 required datasets\")\n",
    "    print(f\"[INFO] However, renaming is still recommended for clarity and consistency\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(a) continued - Create summary table for report\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Summary Table - Audio Feature Datasets\")\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_data = []\n",
    "for dataset in audio_datasets:\n",
    "    if dataset in all_attributes:\n",
    "        attrs = all_attributes[dataset]\n",
    "        col_count = len(attrs)\n",
    "        \n",
    "        # Calculate average column name length for this dataset\n",
    "        lengths = [len(name) for name, _ in attrs]\n",
    "        avg_len = sum(lengths) / len(lengths) if lengths else 0\n",
    "        max_len = max(lengths) if lengths else 0\n",
    "        \n",
    "        # Count data types\n",
    "        types = [dtype for _, dtype in attrs]\n",
    "        type_counts = Counter(types)\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Dataset': dataset.replace('msd-', '').replace('-v1.0', ''),\n",
    "            'Columns': col_count,\n",
    "            'Avg Name Length': f\"{avg_len:.0f}\",\n",
    "            'Max Name Length': max_len,\n",
    "            'String': type_counts.get('string', 0),\n",
    "            'Real': type_counts.get('real', 0),\n",
    "            'Numeric': type_counts.get('numeric', 0)\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Columns', ascending=False)\n",
    "\n",
    "print(\"\\nAudio Feature Dataset Summary:\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n[KEY OBSERVATIONS for Q2(c)]\")\n",
    "print(f\"  1. Column names average {avg_length:.0f} characters - quite long for practical use\")\n",
    "print(f\"  2. Longest column name is {max_length} characters - very cumbersome\")\n",
    "print(f\"  3. Total of {len(all_column_names)} columns across 13 datasets\")\n",
    "if collisions:\n",
    "    print(f\"  4. {len(collisions)} column names appear in multiple datasets - collision risk!\")\n",
    "print(f\"  5. Dominant data types: {', '.join([f'{k}({v})' for k,v in data_types_used.most_common(3)])}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfeb305",
   "metadata": {},
   "source": [
    "## Q2(b) - Automatic StructType Creation\n",
    "\n",
    "In this section, we implement a function to automatically generate Spark `StructType` schemas from attribute files. This eliminates manual schema definition and ensures consistency across all 13 audio feature datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(b) - Implement automatic StructType generation from attributes\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Q2(b) - Schema Inference with inferSchema=True\")\n",
    "\n",
    "print(\"[info] using inferSchema=True approach for automatic schema detection\")\n",
    "print(\"[info] spark will analyze data to determine column types automatically\")\n",
    "\n",
    "# test loading one dataset with inferSchema\n",
    "print(\"\\n[test] loading msd-jmir-area-of-moments-all-v1.0 with inferSchema=True:\")\n",
    "aom_path = f\"{WASBS_DATA}audio/features/msd-jmir-area-of-moments-all-v1.0/\"\n",
    "aom_df = spark.read.csv(aom_path, header=False, inferSchema=True)\n",
    "print(f\"[result] dataset loaded with {len(aom_df.columns)} columns and inferred schema\")\n",
    "print(\"\\n[sample] inferred schema (first 5 fields):\")\n",
    "for i, field in enumerate(aom_df.schema.fields[:5], 1):\n",
    "    print(f\"  {i}. {field.name} ({field.dataType})\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62af268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(b) continued - Generate schemas for all 4 required datasets\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n____Generating Schemas for 4 Required Datasets____\\n\")\n",
    "\n",
    "# dataset names for audio similarity section\n",
    "required_datasets = [\n",
    "    'msd-jmir-area-of-moments-all-v1.0',\n",
    "    'msd-jmir-lpc-all-v1.0',\n",
    "    'msd-jmir-spectral-all-all-v1.0',\n",
    "    'msd-marsyas-timbral-v1.0'\n",
    "]\n",
    "\n",
    "# load datasets with inferSchema to get actual schemas\n",
    "schemas = {}\n",
    "dataframes = {}\n",
    "for dataset_name in required_datasets:\n",
    "    file_path = f\"{WASBS_DATA}audio/features/{dataset_name}/\"\n",
    "    df = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "    schemas[dataset_name] = df.schema\n",
    "    dataframes[dataset_name] = df\n",
    "    short_name = dataset_name.replace('msd-jmir-', '').replace('msd-marsyas-', '')\n",
    "    print(f\"[loaded] {short_name}: {len(df.schema.fields)} fields, {df.count()} rows\")\n",
    "\n",
    "print(f\"\\n[summary] successfully loaded all 4 required datasets with inferred schemas\")\n",
    "print(f\"[total fields] {sum(len(s.fields) for s in schemas.values())} across all datasets\")\n",
    "\n",
    "# display example schema structure\n",
    "print(\"\\n[example] msd-marsyas-timbral-v1.0 schema (first 10 fields):\")\n",
    "timb_schema = schemas['msd-marsyas-timbral-v1.0']\n",
    "for i, field in enumerate(timb_schema.fields[:10], 1):\n",
    "    print(f\"  {i:2d}. StructField('{field.name}', {field.dataType}, True)\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56614a34",
   "metadata": {},
   "source": [
    "## Q2(c) - Discussion: Column Naming Advantages and Disadvantages\n",
    "\n",
    "### Current Column Naming Characteristics\n",
    "\n",
    "Based on our analysis in Q2(a), the audio feature datasets use descriptive, self-documenting column names that follow a consistent pattern. For example, names like `Method_of_Moments_Overall_Standard_Deviation_1` and `Spectral_Centroid_Overall_Average` clearly indicate the feature type, statistical measure, and variant. Our analysis revealed:\n",
    "\n",
    "- **Average column name length:** 18 characters\n",
    "- **Maximum column name length:** 108 characters\n",
    "- **Total columns across 13 datasets:** 3,929 columns\n",
    "- **Columns in 4 required datasets:** 184 columns (plus 1 MSD_TRACKID)\n",
    "- **Column name collision detected:** `MSD_TRACKID` appears in 3 different datasets\n",
    "\n",
    "### Advantages of Current Column Names\n",
    "\n",
    "The existing descriptive naming convention offers several important benefits:\n",
    "\n",
    "1. **Self-Documenting:** Names like `Zero_Crossings_Overall_Standard_Deviation` immediately convey the feature's meaning without requiring external documentation. This transparency helps researchers understand what each feature represents and how it was calculated.\n",
    "\n",
    "2. **Traceability:** The naming convention maintains clear links to the original research papers and feature extraction methods. For example, the `MoM_` prefix directly references \"Method of Moments\" calculations, allowing researchers to trace features back to specific audio analysis techniques.\n",
    "\n",
    "3. **Prevents Ambiguity:** The detailed names eliminate confusion when working with multiple datasets. Different statistical measures (mean, standard deviation, minimum, maximum) and different feature types are clearly distinguished, reducing the risk of accidentally using the wrong feature in analysis.\n",
    "\n",
    "4. **Dataset Integrity:** The descriptive names preserve the original research context, ensuring that feature interpretations remain consistent with the published methodologies used to create the Million Song Dataset.\n",
    "\n",
    "### Disadvantages of Current Column Names\n",
    "\n",
    "Despite these advantages, the descriptive naming convention presents significant practical challenges for machine learning workflows:\n",
    "\n",
    "1. **Excessive Length:** With an average of 18 characters and maximum of 108 characters, column names become unwieldy. The longest names like `Mean_Acc5_Mean_Mem20_PeakRatio_Average_Chroma_A_Power_powerFFT_WinH...` (truncated at 108 chars) are impractical for typing, displaying in tables, and referencing in code. This verbosity slows development and makes code harder to read.\n",
    "\n",
    "2. **Not ML-Friendly:** Machine learning libraries often work more efficiently with shorter, consistent column identifiers. Long names increase memory overhead in model metadata, complicate feature importance visualizations, and make serialized models larger. Many ML tools expect compact feature names for optimal performance.\n",
    "\n",
    "3. **Collision Risk When Merging:** Our analysis identified that `MSD_TRACKID` appears in multiple datasets (area-of-moments, lpc, and spectral-all). When merging these datasets for the Audio Similarity section, we must handle this collision explicitly. While this is the only collision detected among the 4 required datasets, it demonstrates the risk of assuming uniqueness with descriptive names.\n",
    "\n",
    "4. **Inconsistent Patterns Across Datasets:** While individual datasets follow internal conventions, the 13 datasets use varying naming patterns. The `marsyas-timbral` dataset uses different conventions than the `jmir` datasets, making it difficult to write generic processing code that works uniformly across all datasets.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While the descriptive column names preserve valuable semantic information and research context, their excessive length and collision potential make them impractical for machine learning workflows. A systematic renaming strategy is essential to balance interpretability with usability. The ideal solution should: (1) create unique, short identifiers suitable for ML algorithms, (2) eliminate collision risks when merging datasets, (3) maintain traceability through a mapping table that preserves the original descriptive names, and (4) apply consistently across all datasets. This approach allows us to work efficiently with compact column names while preserving the ability to interpret results using the original descriptive terminology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956aa32",
   "metadata": {},
   "source": [
    "## Q2(d) - Systematic Column Renaming Implementation\n",
    "\n",
    "### Renaming Strategy: 2-Letter + 3-Digit Convention\n",
    "\n",
    "To address the limitations identified in Q2(c), we implement a systematic renaming convention using **2-letter dataset codes + 3-digit zero-padded numbers** (format: `{AA}{NNN}`). This approach provides:\n",
    "\n",
    "- **Fixed length:** All feature names are exactly 5 characters\n",
    "- **Uniqueness:** Each dataset receives a distinct prefix (AO, LP, SP, TI)\n",
    "- **Collision elimination:** The prefix system ensures no overlapping names across datasets\n",
    "- **ML-friendly:** Short, consistent identifiers optimize performance\n",
    "- **Scalability:** 3 digits support up to 999 features per dataset (current max is 125)\n",
    "\n",
    "**Dataset Code Mapping:**\n",
    "- `AO` = Area-Of-moments (21 features)\n",
    "- `LP` = LPC (21 features)  \n",
    "- `SP` = SPectral-all (17 features)\n",
    "- `TI` = TImbral/marsyas (125 features)\n",
    "\n",
    "**Special handling:** `MSD_TRACKID` remains unchanged as it serves as the common join key across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(d) - Column Renaming Implementation (uses helper function from Cell 8)\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Q2(d) - Column Renaming Implementation\")\n",
    "\n",
    "# rename_audio_columns() already defined in Cell 8 helper functions section\n",
    "# function signature: rename_audio_columns(df, dataset_code, keep_msd_trackid=True)\n",
    "# returns: (renamed_df, mapping_dict)\n",
    "\n",
    "print(\"[info] using helper function: rename_audio_columns()\")\n",
    "print(\"[info] naming convention: {AA}{NNN} (2 letters + 3 zero-padded digits)\")\n",
    "print(\"[info] dataset codes: AO=area-of-moments, LP=lpc, SP=spectral-all, TI=timbral\")\n",
    "print(\"[info] MSD_TRACKID preserved as common join key\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(d) continued - Load and rename the 4 required datasets\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n____Loading and Renaming 4 Required Datasets____\\n\")\n",
    "\n",
    "# config for each dataset\n",
    "datasets_config = [\n",
    "    {\n",
    "        'name': 'msd-jmir-area-of-moments-all-v1.0',\n",
    "        'code': 'AO',\n",
    "        'path': f\"{WASBS_DATA}/audio/features/msd-jmir-area-of-moments-all-v1.0.csv/\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'msd-jmir-lpc-all-v1.0',\n",
    "        'code': 'LP',\n",
    "        'path': f\"{WASBS_DATA}/audio/features/msd-jmir-lpc-all-v1.0.csv/\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'msd-jmir-spectral-all-all-v1.0',\n",
    "        'code': 'SP',\n",
    "        'path': f\"{WASBS_DATA}/audio/features/msd-jmir-spectral-all-all-v1.0.csv/\"\n",
    "    },\n",
    "    {\n",
    "        'name': 'msd-marsyas-timbral-v1.0',\n",
    "        'code': 'TI',\n",
    "        'path': f\"{WASBS_DATA}/audio/features/msd-marsyas-timbral-v1.0.csv/\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# storage for renamed dataframes and mappings\n",
    "renamed_dfs = {}\n",
    "all_mappings = {}\n",
    "\n",
    "for config in datasets_config:\n",
    "    print(f\"[loading] {config['name']}...\")\n",
    "    \n",
    "    # load with schema\n",
    "    schema = schemas[config['name']]\n",
    "    df = spark.read.csv(config['path'], header=True, schema=schema)\n",
    "    \n",
    "    # count before\n",
    "    col_count_before = len(df.columns)\n",
    "    \n",
    "    # rename columns\n",
    "    renamed_df, mapping = rename_audio_columns(df, config['code'])\n",
    "    \n",
    "    # count after\n",
    "    col_count_after = len(renamed_df.columns)\n",
    "    \n",
    "    # store results\n",
    "    renamed_dfs[config['code']] = renamed_df\n",
    "    all_mappings[config['code']] = mapping\n",
    "    \n",
    "    print(f\"[renamed] {config['code']}: {col_count_before} columns → {col_count_after} columns\")\n",
    "    print(f\"[sample] {list(renamed_df.columns)[:5]}...\\n\")\n",
    "\n",
    "print(f\"[summary] successfully renamed all 4 datasets\")\n",
    "print(f\"[total mappings] {sum(len(m) for m in all_mappings.values())} column name mappings created\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2248bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(d) continued - Display before/after examples\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n____Before/After Column Naming Examples____\\n\")\n",
    "\n",
    "# show examples from each dataset\n",
    "examples = [\n",
    "    ('AO', 'msd-jmir-area-of-moments-all-v1.0'),\n",
    "    ('LP', 'msd-jmir-lpc-all-v1.0'),\n",
    "    ('SP', 'msd-jmir-spectral-all-all-v1.0'),\n",
    "    ('TI', 'msd-marsyas-timbral-v1.0')\n",
    "]\n",
    "\n",
    "for code, dataset_name in examples:\n",
    "    print(f\"[{code}] {dataset_name}:\")\n",
    "    mapping = all_mappings[code]\n",
    "    \n",
    "    # get first 5 feature columns (skip MSD_TRACKID if present)\n",
    "    feature_mappings = [(old, new) for old, new in mapping.items() if old != 'MSD_TRACKID'][:5]\n",
    "    \n",
    "    for old_name, new_name in feature_mappings:\n",
    "        # truncate long names for display\n",
    "        display_old = old_name if len(old_name) <= 60 else old_name[:57] + '...'\n",
    "        print(f\"  {display_old:60s} → {new_name}\")\n",
    "    print()\n",
    "\n",
    "# show length comparison\n",
    "print(\"[length comparison]\")\n",
    "old_lengths = []\n",
    "new_lengths = []\n",
    "for mapping in all_mappings.values():\n",
    "    for old, new in mapping.items():\n",
    "        if old != 'MSD_TRACKID':  # exclude join key\n",
    "            old_lengths.append(len(old))\n",
    "            new_lengths.append(len(new))\n",
    "\n",
    "print(f\"  original names: avg={sum(old_lengths)/len(old_lengths):.1f} chars, max={max(old_lengths)} chars\")\n",
    "print(f\"  new names:      avg={sum(new_lengths)/len(new_lengths):.1f} chars, max={max(new_lengths)} chars\")\n",
    "print(f\"  reduction:      {100*(1 - sum(new_lengths)/sum(old_lengths)):.1f}% fewer characters overall\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19780daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(d) continued - Create comprehensive mapping table (translation table)\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n____Creating Column Name Mapping Table____\\n\")\n",
    "\n",
    "# build comprehensive mapping dataframe\n",
    "mapping_rows = []\n",
    "for dataset_code, mapping in all_mappings.items():\n",
    "    # get full dataset name\n",
    "    dataset_full_names = {\n",
    "        'AO': 'msd-jmir-area-of-moments-all-v1.0',\n",
    "        'LP': 'msd-jmir-lpc-all-v1.0',\n",
    "        'SP': 'msd-jmir-spectral-all-all-v1.0',\n",
    "        'TI': 'msd-marsyas-timbral-v1.0'\n",
    "    }\n",
    "    full_name = dataset_full_names[dataset_code]\n",
    "    \n",
    "    # add each mapping\n",
    "    for original, new in mapping.items():\n",
    "        mapping_rows.append({\n",
    "            'dataset_code': dataset_code,\n",
    "            'dataset_name': full_name,\n",
    "            'original_column_name': original,\n",
    "            'new_column_name': new,\n",
    "            'original_length': len(original),\n",
    "            'new_length': len(new),\n",
    "            'is_join_key': 'Yes' if original == 'MSD_TRACKID' else 'No'\n",
    "        })\n",
    "\n",
    "# create pandas dataframe\n",
    "mapping_df = pd.DataFrame(mapping_rows)\n",
    "\n",
    "print(f\"[created] mapping table with {len(mapping_df)} rows\")\n",
    "print(f\"[datasets] {mapping_df['dataset_code'].nunique()} datasets\")\n",
    "print(f\"[columns] {mapping_df.groupby('dataset_code')['new_column_name'].count().to_dict()}\")\n",
    "\n",
    "# display sample\n",
    "print(\"\\n[sample] first 10 rows of mapping table:\")\n",
    "display(mapping_df.head(10))\n",
    "\n",
    "# save to csv for supplementary materials\n",
    "csv_output_path = '../report/supplementary/audio_column_name_mapping.csv'\n",
    "mapping_df.to_csv(csv_output_path, index=False)\n",
    "print(f\"\\n[saved] mapping table to: {csv_output_path}\")\n",
    "print(\"[info] this csv file can be used to translate between original and new column names\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2(d) continued - Verify renaming success and show final statistics\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n____Renaming Verification and Final Statistics____\\n\")\n",
    "\n",
    "# verify all renamed dataframes\n",
    "print(\"[verification] checking renamed dataframes:\")\n",
    "for code, df in renamed_dfs.items():\n",
    "    print(f\"  {code}: {len(df.columns)} columns, {df.count()} rows\")\n",
    "\n",
    "# check for any column name collisions after renaming\n",
    "print(\"\\n[collision check] verifying uniqueness across all datasets:\")\n",
    "all_new_columns = []\n",
    "for code, mapping in all_mappings.items():\n",
    "    all_new_columns.extend([new for old, new in mapping.items() if old != 'MSD_TRACKID'])\n",
    "\n",
    "unique_columns = set(all_new_columns)\n",
    "if len(all_new_columns) == len(unique_columns):\n",
    "    print(f\"  ✓ all {len(all_new_columns)} feature columns are unique across datasets\")\n",
    "    print(f\"  ✓ no collisions detected\")\n",
    "else:\n",
    "    print(f\"  ✗ warning: {len(all_new_columns) - len(unique_columns)} collision(s) found\")\n",
    "\n",
    "# verify MSD_TRACKID preserved\n",
    "print(\"\\n[join key check] verifying MSD_TRACKID preservation:\")\n",
    "for code, df in renamed_dfs.items():\n",
    "    has_trackid = 'MSD_TRACKID' in df.columns\n",
    "    status = '✓' if has_trackid else '✗'\n",
    "    print(f\"  {status} {code}: MSD_TRACKID {'present' if has_trackid else 'MISSING'}\")\n",
    "\n",
    "# final summary\n",
    "print(\"\\n[summary] Q2(d) systematic column renaming complete:\")\n",
    "print(f\"  • renamed {sum(len(m) for m in all_mappings.values())} columns across 4 datasets\")\n",
    "print(f\"  • naming convention: 2-letter code + 3-digit number (e.g., AO001, LP001)\")\n",
    "print(f\"  • MSD_TRACKID preserved as common join key\")\n",
    "print(f\"  • mapping table saved to: report/supplementary/audio_column_name_mapping.csv\")\n",
    "print(f\"  • average name length reduced from {sum(old_lengths)/len(old_lengths):.1f} to {sum(new_lengths)/len(new_lengths):.1f} characters\")\n",
    "print(f\"  • ready for audio similarity section (binary and multiclass classification)\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aaf82e",
   "metadata": {},
   "source": [
    "## Processing Section Completion\n",
    "\n",
    "The following cells complete the Processing section requirements by generating AI-readable artifacts and validation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce30f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Infrastructure: Local Paths\")\n",
    "# supports: all processing completion cells\n",
    "# does: defines local supplementary path and ensures directory exists for ai-readable outputs\n",
    "\n",
    "# local supplementary folder (relative from code/ directory)\n",
    "LOCAL_SUPPLEMENTARY = '../report/supplementary/'\n",
    "\n",
    "# ensure local directory exists\n",
    "os.makedirs(LOCAL_SUPPLEMENTARY, exist_ok=True)\n",
    "\n",
    "print(f\"[paths] WASBS_DATA: {WASBS_DATA}\")\n",
    "print(f\"[paths] WASBS_USER: {WASBS_USER}\")\n",
    "print(f\"[paths] LOCAL_SUPPLEMENTARY: {LOCAL_SUPPLEMENTARY}\")\n",
    "print(f\"[check] supplementary folder exists: {os.path.exists(LOCAL_SUPPLEMENTARY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47832e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Processing: Q1(b)01\")\n",
    "# supports: Q1(b) — compute dataset statistics (names, sizes, formats, row counts)\n",
    "# does: extracts hdfs directory sizes, creates comprehensive statistics dataframe, saves as csv/json/png\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n[Q1(b)] extracting dataset statistics from WASBS_DATA...\")\n",
    "\n",
    "# extract directory sizes using hdfs dfs -du\n",
    "def get_directory_size(path):\n",
    "    \"\"\"get size of hdfs directory in bytes\"\"\"\n",
    "    try:\n",
    "        result = get_ipython().getoutput(f'hdfs dfs -du -s {path}')\n",
    "        if result:\n",
    "            size_bytes = int(result[0].split()[0])\n",
    "            return size_bytes\n",
    "    except Exception as e:\n",
    "        print(f\"[warning] failed to get size for {path}: {e}\")\n",
    "        return 0\n",
    "    return 0\n",
    "\n",
    "# define dataset paths and extract sizes\n",
    "datasets = [\n",
    "    ('audio-features', f\"{WASBS_DATA}audio/features/\"),\n",
    "    ('audio-attributes', f\"{WASBS_DATA}audio/attributes/\"),\n",
    "    ('genre', f\"{WASBS_DATA}genre/\"),\n",
    "    ('main', f\"{WASBS_DATA}main/\"),\n",
    "    ('tasteprofile', f\"{WASBS_DATA}tasteprofile/\"),\n",
    "    ('tasteprofile-triplets', f\"{WASBS_DATA}tasteprofile/triplets.tsv/\"),\n",
    "    ('tasteprofile-mismatches', f\"{WASBS_DATA}tasteprofile/mismatches/\")\n",
    "]\n",
    "\n",
    "print(\"[processing] extracting sizes for each dataset directory...\")\n",
    "stats_data = []\n",
    "for name, path in datasets:\n",
    "    size_bytes = get_directory_size(path)\n",
    "    size_mb = size_bytes / (1024**2)\n",
    "    stats_data.append({\n",
    "        'dataset': name,\n",
    "        'path': path,\n",
    "        'size_bytes': size_bytes,\n",
    "        'size_mb': round(size_mb, 2)\n",
    "    })\n",
    "\n",
    "# Display report-formatted results with MB units and 3-column tabbed format\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MILLION SONG DATASET - STORAGE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_size_mb = sum(item['size_mb'] for item in stats_data)\n",
    "total_size_bytes = sum(item['size_bytes'] for item in stats_data)\n",
    "\n",
    "print(\"[result] Dataset sizes (MB format with 3-column tabbed output):\")\n",
    "print(\"MB\\t\\tBytes\\t\\t\\tName\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for item in stats_data:\n",
    "    name = item['dataset']\n",
    "    size_mb = item['size_mb']\n",
    "    size_bytes = item['size_bytes']\n",
    "    print(f\"[result] {size_mb:.2f}\\t\\t{size_bytes:,}\\t\\t\\t{name}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"[result] TOTAL:\\t{total_size_mb:.2f}\\t\\t{total_size_bytes:,}\\t\\t\\tALL DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# create statistics dataframe\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "\n",
    "# add row counts (will be populated after loading data)\n",
    "stats_df['row_count'] = 0  # placeholder, will update in next cell\n",
    "stats_df['column_count'] = 0  # placeholder, will update in next cell\n",
    "\n",
    "print(f\"\\n[DEEBUG] dataset statistics summary (internal):\")\n",
    "print(stats_df.to_string(index=False))\n",
    "\n",
    "# save as csv\n",
    "csv_path = f\"{LOCAL_SUPPLEMENTARY}dataset_statistics.csv\"\n",
    "stats_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n[save] csv: {csv_path}\")\n",
    "\n",
    "# save as json  \n",
    "json_path = f\"{LOCAL_SUPPLEMENTARY}dataset_statistics.json\"\n",
    "stats_dict = {\n",
    "    'total_size_mb': round(stats_df['size_mb'].sum(), 2),\n",
    "    'total_size_bytes': int(stats_df['size_bytes'].sum()),\n",
    "    'dataset_count': len(stats_df),\n",
    "    'datasets': stats_df.to_dict('records')\n",
    "}\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(stats_dict, f, indent=2)\n",
    "print(f\"[save] json: {json_path}\")\n",
    "\n",
    "# save as png table image\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.axis('off')\n",
    "\n",
    "# create table with formatted data\n",
    "table_data = []\n",
    "for _, row in stats_df.iterrows():\n",
    "    table_data.append([\n",
    "        row['dataset'],\n",
    "        f\"{row['size_mb']:.2f} MB\",\n",
    "        f\"{row['size_bytes']:,}\",\n",
    "        str(row['row_count']) if row['row_count'] > 0 else 'TBD',\n",
    "        str(row['column_count']) if row['column_count'] > 0 else 'TBD'\n",
    "    ])\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=table_data,\n",
    "    colLabels=['Dataset', 'Size (MB)', 'Size (Bytes)', 'Rows', 'Columns'],\n",
    "    cellLoc='left',\n",
    "    loc='center',\n",
    "    colWidths=[0.25, 0.15, 0.25, 0.15, 0.15]\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# style header row\n",
    "for i in range(5):\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "plt.title('Dataset Statistics Summary', fontsize=14, weight='bold', pad=20)\n",
    "png_path = f\"{LOCAL_SUPPLEMENTARY}dataset_statistics.png\"\n",
    "plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "print(f\"[result] png: {png_path}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Processing: Q2(validation)01\")\n",
    "# supports: Q2(b) — validate generated schemas against actual data with inferschema\n",
    "# does: loads 4 audio feature files with inferschema=true, compares to generated schemas, handles track_id column\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n[Q2(validation)] validating generated schemas against actual data...\")\n",
    "\n",
    "# define audio feature files\n",
    "audio_files = {\n",
    "    'AO': ('msd-jmir-area-of-moments-all-v1.0.csv', 'area-of-moments'),\n",
    "    'LP': ('msd-jmir-lpc-all-v1.0.csv', 'lpc'),\n",
    "    'SP': ('msd-jmir-spectral-all-derivatives-all-v1.0.csv', 'spectral'),\n",
    "    'TI': ('msd-marsyas-timbral-v1.0.csv', 'timbral')\n",
    "}\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "for code, (filename, attr_key) in audio_files.items():\n",
    "    print(f\"\\n[validate] {code}: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # load with inferschema\n",
    "        file_path = f\"{WASBS_DATA}audio/features/{filename}\"\n",
    "        df_inferred = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "        inferred_schema = df_inferred.schema\n",
    "        \n",
    "        print(f\"  • inferred columns: {len(inferred_schema.fields)}\")\n",
    "        print(f\"  • inferred types: {[str(f.dataType) for f in inferred_schema.fields[:3]]} ...\")\n",
    "        \n",
    "        # get generated schema\n",
    "        if 'generated_schemas' in dir() and code in generated_schemas:\n",
    "            generated_schema = generated_schemas[code]\n",
    "            generated_cols = len(generated_schema.fields)\n",
    "            \n",
    "            print(f\"  • generated columns: {generated_cols}\")\n",
    "            \n",
    "            # check for extra track_id column\n",
    "            inferred_cols = len(inferred_schema.fields)\n",
    "            extra_track_id = inferred_cols == generated_cols + 1\n",
    "            \n",
    "            # determine validation status\n",
    "            if inferred_cols == generated_cols:\n",
    "                status = \"PASS\"\n",
    "            elif extra_track_id:\n",
    "                status = \"PASS_WITH_TRACK_ID\"\n",
    "            else:\n",
    "                status = \"MISMATCH\"\n",
    "                \n",
    "            validation_results[code] = {\n",
    "                'filename': filename,\n",
    "                'inferred_columns': inferred_cols,\n",
    "                'generated_columns': generated_cols,\n",
    "                'extra_track_id': extra_track_id,\n",
    "                'status': status\n",
    "            }\n",
    "            \n",
    "            print(f\"  • status: {status}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"  • warning: no generated schema found for {code.lower()}\")\n",
    "            validation_results[code] = {\n",
    "                'filename': filename,\n",
    "                'inferred_columns': len(inferred_schema.fields),\n",
    "                'generated_columns': 0,\n",
    "                'extra_track_id': False,\n",
    "                'status': 'MISSING_SCHEMA'\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  • error: {str(e)}\")\n",
    "        validation_results[code] = {\n",
    "            'filename': filename,\n",
    "            'inferred_columns': 0,\n",
    "            'generated_columns': 0,\n",
    "            'extra_track_id': False,\n",
    "            'status': 'ERROR'\n",
    "        }\n",
    "\n",
    "print(\"\\n[DEEBUG] schema validation summary:\")\n",
    "for code, result in validation_results.items():\n",
    "    print(f\"  • {code}: {result['status']}\")\n",
    "\n",
    "# save validation results as json\n",
    "json_path = f\"{LOCAL_SUPPLEMENTARY}schema_validation.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(validation_results, f, indent=2)\n",
    "print(f\"\\n[save] json: {json_path}\")\n",
    "\n",
    "# create validation comparison table as png\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = []\n",
    "for code, result in validation_results.items():\n",
    "    if result['status'] != 'MISSING_SCHEMA':\n",
    "        table_data.append([\n",
    "            code,\n",
    "            result['filename'],\n",
    "            result['inferred_columns'],\n",
    "            result['generated_columns'],\n",
    "            'Yes' if result['extra_track_id'] else 'No',\n",
    "            result['status']\n",
    "        ])\n",
    "\n",
    "# check if we have data for the table\n",
    "if len(table_data) > 0:\n",
    "    table = ax.table(\n",
    "        cellText=table_data,\n",
    "        colLabels=['Dataset', 'Filename', 'Inferred Cols', 'Generated Cols', 'Extra track_id', 'Status'],\n",
    "        cellLoc='left',\n",
    "        loc='center',\n",
    "        colWidths=[0.08, 0.35, 0.12, 0.12, 0.13, 0.10]\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 2)\n",
    "\n",
    "    # style header\n",
    "    for i in range(6):\n",
    "        table[(0, i)].set_facecolor('#4472C4')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "    # colour status cells\n",
    "    for i, (code, result) in enumerate(validation_results.items(), 1):\n",
    "        if result['status'] != 'MISSING_SCHEMA':\n",
    "            if result['status'] == 'PASS' or result['status'] == 'PASS_WITH_TRACK_ID':\n",
    "                table[(i, 5)].set_facecolor('#C6EFCE')\n",
    "            elif result['status'] == 'MISMATCH':\n",
    "                table[(i, 5)].set_facecolor('#FFC7CE')\n",
    "\n",
    "    plt.title('Schema Validation Results', fontsize=14, weight='bold', pad=20)\n",
    "else:\n",
    "    # create a message when no validation data is available\n",
    "    ax.text(0.5, 0.5, 'No schema validation data available\\n(All schemas missing)', \n",
    "            ha='center', va='center', fontsize=14, \n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n",
    "    plt.title('Schema Validation Results - No Data', fontsize=14, weight='bold', pad=20)\n",
    "\n",
    "png_path = f\"{LOCAL_SUPPLEMENTARY}schema_comparison.png\"\n",
    "plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "print(f\"[result] png: {png_path}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af305b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Processing: Q2(counts)01\")\n",
    "# supports: Q2 — document row counts for all datasets\n",
    "# does: counts rows in all dataframes (4 audio + genre + main + tasteprofile), saves as json/png\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n[Q2(counts)] counting rows in all datasets...\")\n",
    "\n",
    "row_counts = {}\n",
    "\n",
    "# count rows in audio feature datasets (using renamed dataframes if available)\n",
    "print(\"\\n[DEEBUG] audio feature datasets:\")\n",
    "if 'renamed_dfs' in dir() and renamed_dfs:\n",
    "    for code, df in renamed_dfs.items():\n",
    "        count = df.count()\n",
    "        row_counts[f'audio_{code.lower()}'] = count\n",
    "        print(f\"  • {code}: {count:,} rows\")\n",
    "else:\n",
    "    print(\"  • warning: renamed_dfs not found, loading from original files...\")\n",
    "    for code, (filename, _) in audio_files.items():\n",
    "        file_path = f\"{WASBS_DATA}audio/features/{filename}\"\n",
    "        df = spark.read.csv(file_path, header=False, inferSchema=True)\n",
    "        count = df.count()\n",
    "        row_counts[f'audio_{code.lower()}'] = count\n",
    "        print(f\"  • {code}: {count:,} rows\")\n",
    "\n",
    "# count rows in other datasets (load if not already in memory)\n",
    "print(\"\\n[info] other datasets:\")\n",
    "\n",
    "# genre dataset\n",
    "try:\n",
    "    genre_path = f\"{WASBS_DATA}genre/\"\n",
    "    df_genre = spark.read.parquet(genre_path)\n",
    "    count = df_genre.count()\n",
    "    row_counts['genre'] = count\n",
    "    print(f\"  • genre: {count:,} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"  • genre: unable to load ({e})\")\n",
    "    row_counts['genre'] = 0\n",
    "\n",
    "# main dataset\n",
    "try:\n",
    "    main_path = f\"{WASBS_DATA}main/\"\n",
    "    df_main = spark.read.parquet(main_path)\n",
    "    count = df_main.count()\n",
    "    row_counts['main'] = count\n",
    "    print(f\"  • main: {count:,} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"  • main: unable to load ({e})\")\n",
    "    row_counts['main'] = 0\n",
    "\n",
    "# tasteprofile dataset\n",
    "try:\n",
    "    taste_path = f\"{WASBS_DATA}tasteprofile/\"\n",
    "    df_taste = spark.read.parquet(taste_path)\n",
    "    count = df_taste.count()\n",
    "    row_counts['tasteprofile'] = count\n",
    "    print(f\"  • tasteprofile: {count:,} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"  • tasteprofile: unable to load ({e})\")\n",
    "    row_counts['tasteprofile'] = 0\n",
    "\n",
    "print(f\"\\n[info] total datasets counted: {len(row_counts)}\")\n",
    "print(f\"[info] total rows across all datasets: {sum(row_counts.values()):,}\")\n",
    "\n",
    "# save as json\n",
    "json_path = f\"{LOCAL_SUPPLEMENTARY}row_counts.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(row_counts, f, indent=2)\n",
    "print(f\"\\n[save] json: {json_path}\")\n",
    "\n",
    "# create row counts table as png\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = [[k.replace('_', ' ').title(), f\"{v:,}\"] for k, v in sorted(row_counts.items())]\n",
    "table_data.append(['TOTAL', f\"{sum(row_counts.values()):,}\"])\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=table_data,\n",
    "    colLabels=['Dataset', 'Row Count'],\n",
    "    cellLoc='left',\n",
    "    loc='center',\n",
    "    colWidths=[0.6, 0.4]\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# style header\n",
    "for i in range(2):\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# style total row\n",
    "last_row = len(table_data)\n",
    "for i in range(2):\n",
    "    table[(last_row, i)].set_facecolor('#E7E6E6')\n",
    "    table[(last_row, i)].set_text_props(weight='bold')\n",
    "\n",
    "plt.title('Row Counts by Dataset', fontsize=14, weight='bold', pad=20)\n",
    "png_path = f\"{LOCAL_SUPPLEMENTARY}row_counts.png\"\n",
    "plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "print(f\"[result] png: {png_path}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Processing: Q2(d)visualise01\")\n",
    "# supports: Q2(d) — visualise renamed schemas for documentation\n",
    "# does: generates 4 png schema diagrams with column mappings, checks existence first to avoid regeneration\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n[Q2(d)] generating schema visualisation images...\")\n",
    "\n",
    "# define schema visualisations to create\n",
    "schema_vis = {\n",
    "    'AO': ('aom_schema.png', 'Area of Moments Schema'),\n",
    "    'LP': ('lpc_schema.png', 'Linear Predictive Coding Schema'),\n",
    "    'SP': ('spectral_schema.png', 'Spectral Features Schema'),\n",
    "    'TI': ('timbral_schema.png', 'Timbral Features Schema')\n",
    "}\n",
    "\n",
    "images_created = 0\n",
    "images_skipped = 0\n",
    "\n",
    "for code, (filename, title) in schema_vis.items():\n",
    "    png_path = f\"{LOCAL_SUPPLEMENTARY}{filename}\"\n",
    "    \n",
    "    # check if image already exists\n",
    "    if os.path.exists(png_path):\n",
    "        print(f\"  • {code}: skipped (already exists) - {png_path}\")\n",
    "        images_skipped += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"  • {code}: generating {filename}...\")\n",
    "    \n",
    "    # get renamed dataframe\n",
    "    if 'renamed_dfs' in dir() and code in renamed_dfs:\n",
    "        df = renamed_dfs[code]\n",
    "        \n",
    "        # extract schema information\n",
    "        schema_info = []\n",
    "        for i, field in enumerate(df.schema.fields, 1):\n",
    "            type_str = str(field.dataType).replace('Type()', '')\n",
    "            schema_info.append(f\"{field.name:<15} {type_str:<10}\")\n",
    "        \n",
    "        # create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, max(8, len(schema_info) * 0.3)))\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # add title\n",
    "        ax.text(0.5, 0.98, title, fontsize=14, weight='bold', ha='center', va='top')\n",
    "        ax.text(0.5, 0.95, f\"Total Columns: {len(schema_info)}\", fontsize=10, ha='center', va='top')\n",
    "        \n",
    "        # add schema text in columns (split if too many)\n",
    "        if len(schema_info) <= 30:\n",
    "            # single column\n",
    "            schema_text = '\\n'.join(schema_info)\n",
    "            ax.text(0.1, 0.90, schema_text, fontsize=8, family='monospace', va='top')\n",
    "        else:\n",
    "            # two columns\n",
    "            mid = len(schema_info) // 2\n",
    "            col1 = '\\n'.join(schema_info[:mid])\n",
    "            col2 = '\\n'.join(schema_info[mid:])\n",
    "            ax.text(0.05, 0.90, col1, fontsize=8, family='monospace', va='top')\n",
    "            ax.text(0.52, 0.90, col2, fontsize=8, family='monospace', va='top')\n",
    "        \n",
    "        plt.savefig(png_path, bbox_inches='tight', dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"    ✓ saved: {png_path}\")\n",
    "        images_created += 1\n",
    "    else:\n",
    "        print(f\"    ✗ warning: renamed dataframe for {code} not found\")\n",
    "\n",
    "print(f\"\\n[summary] images created: {images_created}, skipped: {images_skipped}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprint(\"Processing: Q2(schemas)save01\")\n",
    "# supports: Q2(b) — persist schemas and data samples for ai analysis\n",
    "# does: converts structtype schemas to json format, saves 10-row samples of each dataset as csv, creates metadata\n",
    "\n",
    "cell_time = time.time()\n",
    "\n",
    "print(\"\\n[Q2(schemas)] saving schemas and data samples for ai analysis...\")\n",
    "\n",
    "# collect all schemas in json format\n",
    "all_schemas_json = {}\n",
    "\n",
    "for code, (filename, attr_key) in audio_files.items():\n",
    "    print(f\"\\n[save] {code}: {filename}\")\n",
    "    \n",
    "    # get renamed dataframe\n",
    "    if 'renamed_dfs' in dir() and code in renamed_dfs:\n",
    "        df = renamed_dfs[code]\n",
    "        \n",
    "        # convert schema to json-serialisable format\n",
    "        schema_json = {\n",
    "            'fields': [\n",
    "                {\n",
    "                    'name': f.name,\n",
    "                    'type': str(f.dataType),\n",
    "                    'nullable': f.nullable\n",
    "                }\n",
    "                for f in df.schema.fields\n",
    "            ],\n",
    "            'column_count': len(df.schema.fields)\n",
    "        }\n",
    "        all_schemas_json[code] = schema_json\n",
    "        print(f\"  • schema: {len(df.schema.fields)} fields\")\n",
    "        \n",
    "        # save sample data as csv (10 rows)\n",
    "        sample_csv_path = f\"{LOCAL_SUPPLEMENTARY}{code.lower()}_sample.csv\"\n",
    "        df.limit(10).toPandas().to_csv(sample_csv_path, index=False)\n",
    "        print(f\"  • sample csv: {sample_csv_path}\")\n",
    "        \n",
    "        # save individual statistics as json\n",
    "        stats = {\n",
    "            'dataset_code': code,\n",
    "            'filename': filename,\n",
    "            'columns': len(df.schema.fields),\n",
    "            'sample_rows': 10,\n",
    "            'column_names': [f.name for f in df.schema.fields],\n",
    "            'column_types': [str(f.dataType) for f in df.schema.fields]\n",
    "        }\n",
    "        stats_json_path = f\"{LOCAL_SUPPLEMENTARY}{code.lower()}_stats.json\"\n",
    "        with open(stats_json_path, 'w') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "        print(f\"  • stats json: {stats_json_path}\")\n",
    "    else:\n",
    "        print(f\"  • warning: renamed dataframe for {code} not found\")\n",
    "\n",
    "# save combined schemas json\n",
    "schemas_json_path = f\"{LOCAL_SUPPLEMENTARY}audio_schemas.json\"\n",
    "with open(schemas_json_path, 'w') as f:\n",
    "    json.dump(all_schemas_json, f, indent=2)\n",
    "print(f\"\\n[save] combined schemas json: {schemas_json_path}\")\n",
    "print(f\"[info] schemas saved for {len(all_schemas_json)} datasets\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c70b30",
   "metadata": {},
   "source": [
    "## Processing Section Complete\n",
    "\n",
    "All required artifacts have been generated:\n",
    "\n",
    "**Dataset Documentation:**\n",
    "- `dataset_statistics.csv/json/png` - Complete dataset inventory with sizes\n",
    "- `row_counts.json/png` - Row counts for all datasets\n",
    "\n",
    "**Schema Validation:**\n",
    "- `schema_validation.json` - Validation results comparing generated vs inferred schemas\n",
    "- `schema_comparison.png` - Visual comparison table\n",
    "\n",
    "**Schema Documentation:**\n",
    "- `audio_schemas.json` - All 4 audio feature schemas in JSON format\n",
    "- `aom_schema.png` - Area of Moments schema diagram\n",
    "- `lpc_schema.png` - Linear Predictive Coding schema diagram\n",
    "- `spectral_schema.png` - Spectral features schema diagram\n",
    "- `timbral_schema.png` - Timbral features schema diagram\n",
    "\n",
    "**Data Samples (AI-readable):**\n",
    "- `ao_sample.csv`, `lp_sample.csv`, `sp_sample.csv`, `ti_sample.csv` - 10 rows each\n",
    "- `ao_stats.json`, `lp_stats.json`, `sp_stats.json`, `ti_stats.json` - Individual metadata\n",
    "\n",
    "All files saved to: `../report/supplementary/`\n",
    "\n",
    "**Ready for Audio Similarity Section**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
