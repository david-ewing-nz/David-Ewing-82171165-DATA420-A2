{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bbb004-5a22-4f65-b400-cb2bc3edf217",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter notebook or Jupyter lab session running on the cluster master node in the cloud.\n",
    "\n",
    "Follow the instructions on the computing resources page to start a cluster and open this notebook.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Connect to the Windows server using Windows App.\n",
    "2. Connect to Kubernetes.\n",
    "3. Start Jupyter and open this notebook from Jupyter in order to connect to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017705b0-281d-4565-abe1-852ebc5ca888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Constants used to interact with Azure Blob Storage using the hdfs command or Spark\n",
    "\n",
    "global username\n",
    "\n",
    "username = re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "global azure_account_name\n",
    "global azure_data_container_name\n",
    "global azure_user_container_name\n",
    "global azure_user_token\n",
    "\n",
    "azure_account_name = \"madsstorage002\"\n",
    "azure_data_container_name = \"campus-data\"\n",
    "azure_user_container_name = \"campus-user\"\n",
    "azure_user_token = r\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://localhost:{sc.uiWebUrl.split(\":\")[-1]}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else: \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username} (notebook)</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{username}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.driver.memory\", f'{master_memory}g')\n",
    "        .config(\"spark.executor.memory\", f'{worker_memory}g')\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.kubernetes.container.image\", \"madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\n",
    "        .config(\"spark.memory.fraction\", \"0.1\")\n",
    "        .config(f\"fs.azure.sas.{azure_user_container_name}.{azure_account_name}.blob.core.windows.net\",  azure_user_token)\n",
    "        .config(\"spark.app.name\", f\"{username} (notebook)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "    global spark\n",
    "    global sc\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "        spark.stop()\n",
    "        del spark\n",
    "        del sc\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e2f6d-f8f0-4150-8eec-e5bb2b0471e4",
   "metadata": {},
   "source": [
    "### Assignment 2 ###\n",
    "\n",
    "- MSD containers:\n",
    "  - `wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/` \n",
    "\n",
    "- MY containers:\n",
    "  - `wasbs://campus-user@madsstorage002.blob.core.windows.net/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06343f5-a638-4f7f-ade5-5f564a068bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net\n",
      "Warning: Ignoring non-Spark config property: SPARK_DRIVER_BIND_ADDRESS\n",
      "25/10/08 08:43:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dew59 (notebook)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://localhost:4040\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.driver.pod.name</td><td>spark-master-driver</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>4</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.namespace</td><td>dew59</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1759866199141</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image.pullPolicy</td><td>IfNotPresent</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>32</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dderby.system.home=/tmp/dew59/spark/</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podNamePrefix</td><td>dew59-notebook-b1e07899c0331fa6</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>k8s://https://kubernetes.default.svc.cluster.local:443</td></tr><tr><td style=\"text-align:left;\">spark.fs.azure</td><td>org.apache.hadoop.fs.azure.NativeAzureFileSystem</td></tr><tr><td style=\"text-align:left;\">spark.app.submitTime</td><td>1759866198979</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dew59 (notebook)</td></tr><tr><td style=\"text-align:left;\">spark.memory.fraction</td><td>0.1</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>4g</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8-1.0.16</td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.memoryOverheadFactor</td><td>0.3</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>spark-master-svc</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>${env:SPARK_UI_PORT}</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.container.image</td><td>madsregistry001.azurecr.io/hadoop-spark:v3.3.5-openjdk-8</td></tr><tr><td style=\"text-align:left;\">spark.kubernetes.executor.podTemplateFile</td><td>/opt/spark/conf/executor-pod-template.yaml</td></tr><tr><td style=\"text-align:left;\">fs.azure.sas.campus-user.madsstorage002.blob.core.windows.net</td><td>sp=racwdl&st=2025-08-01T09:41:33Z&se=2026-12-30T16:56:33Z&spr=https&sv=2024-11-04&sr=c&sig=GzR1hq7EJ0lRHj92oDO1MBNjkc602nrpfB5H8Cl7FFY%3D</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.extraJavaOptions</td><td>-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>spark-7454e372d39543c59fe417a5008ffe57</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>7077</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.fs.azure.sas.uco-user.madsstorage002.blob.core.windows.net</td><td>\"sp=racwdl&st=2025-09-21T20:54:03Z&se=2026-12-31T04:09:03Z&spr=https&sv=2024-11-04&sr=c&sig=5V91JeJe9mD%2FuPKUQ3LCErJh%2FwP0gNYoyl8MMx5pdkM%3D\"</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dew59 (notebook)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f31c204-813f-4003-9d21-c3b9c57a065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Imports\n",
    " \n",
    "from IPython.display     import display  # calls between environments\n",
    "from math                import acos, atan2, cos, radians, sin, sqrt\n",
    "from matplotlib.ticker   import FuncFormatter, MaxNLocator\n",
    "from pathlib             import Path\n",
    "from pyspark.sql         import DataFrame\n",
    "from pyspark.sql         import DataFrame as SparkDF\n",
    "from pyspark.sql         import functions as F, types as T\n",
    "from pyspark.sql.types   import *\n",
    "from pyspark.sql.utils   import AnalysisException\n",
    "from pyspark.sql.window  import Window\n",
    "from time                import perf_counter  # Add this line for benchmark functions\n",
    "from typing              import List, Optional, Tuple\n",
    "from rich.tree           import Tree\n",
    "from rich.console        import Console\n",
    "from datetime            import datetime\n",
    "\n",
    "\n",
    "import itertools         as it\n",
    "import matplotlib.dates  as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import warnings\n",
    "\n",
    "import math, os, platform, re\n",
    "import subprocess, sys, time\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "console = Console()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb102f08-d26f-4475-8561-7f63862da060",
   "metadata": {},
   "source": [
    "#The following shows the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812c8751-d59a-423b-8f39-d5e6a540944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark: 3.5.1\n",
      "___________________________________PATHS___________________________________\n",
      "WASBS_DATA          : wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/\n",
      "WASBS_USER          : wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overall time metric\n",
    "notebook_run_time = time.time()\n",
    "\n",
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "#USERNAME    = \"dew59\"\n",
    "WASBS_DATA  = \"wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/\"\n",
    "WASBS_USER  = f\"wasbs://campus-user@madsstorage002.blob.core.windows.net/{username}-A2/\"\n",
    "\n",
    "#WASBS_USER          = \"wasbs://campus-user@madsstorage002.blob.core.windows.net/{}\".format(USERNAME)\n",
    "#WASBS_YEAR_SIZE     = \"{}/years_size_metrics.parquet/\".format(WASBS_USER)\n",
    "\n",
    " \n",
    "#stations_path = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/{stations_write_path}'\n",
    "#common_data_path    = f'wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/'\n",
    "#stations_read_name  =  inventory_read_name = \"\"\n",
    "#stations_read_name  =  inventory_read_name = \"\"\n",
    " \n",
    "\n",
    "print(\"Spark:\", spark.version)\n",
    "print(\"_\" * 35 + \"PATHS\" + \"_\" * 35)\n",
    "print(\"WASBS_DATA          :\", WASBS_DATA)\n",
    "print(\"WASBS_USER          :\", WASBS_USER) \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "839b56fb-a403-4903-ba60-b0f33495ff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________HELPER / DIAGNOSTIC FUNCTIONS___________________________________\n"
     ]
    }
   ],
   "source": [
    "# HELPER AND DIAGNOSTIC FUNCTIONS\n",
    "\n",
    "notebook_run_time = time.time()\n",
    "print(\"_\" * 35 + \"HELPER / DIAGNOSTIC FUNCTIONS\" + \"_\" * 35)\n",
    "\n",
    "def hprint(text: str=\"\", l=50):\n",
    "    \"\"\"Print formatted section header\"\"\"\n",
    "    n = len(text)\n",
    "    n = abs(n - l) // 2\n",
    "    print(\"\\n\" + \"_\" * n + text + \"_\" * n)\n",
    "\n",
    "def cleanup_parquet_files(cleanup=False):\n",
    "    \"\"\"Clean up existing parquet files in user directory.\n",
    "    \n",
    "    Args:\n",
    "        cleanup (bool): When True, actually DELETES FILES. \n",
    "                        When False, only LISTS files.\n",
    "    \"\"\"\n",
    "    hprint(\"Clean up existing parquet files\")\n",
    "\n",
    "    print(\"[cleanup] Listing files BEFORE cleanup:\")\n",
    "    get_ipython().system(f'hdfs dfs -ls {WASBS_USER}/*.parquet')\n",
    "    \n",
    "    if cleanup:\n",
    "        print(\"\\n[cleanup] Deleting all parquet folders...\")\n",
    "        get_ipython().system(f'hdfs dfs -rm -r -f {WASBS_USER}/*.parquet')\n",
    "        \n",
    "        print(\"\\n[info] Listing files AFTER cleanup:\")\n",
    "        get_ipython().system(f'hdfs dfs -ls {WASBS_USER}/*.parquet')\n",
    "        print(\"\\n[cleanup] Parquet file cleanup complete - ready to restart Processing run with clean schema\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n[info] To actually delete files, call: cleanup_parquet_files(cleanup=True)\")\n",
    "\n",
    "def normalise_ids(df: DataFrame, col: str = \"ID\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    # Single source of truth for ID normalisation \n",
    "    Upper + trim + distinct on the given ID column.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] normalise_ids() on column: {col}\")\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "    return df.select(F.upper(F.trim(F.col(col))).alias(\"ID\")).distinct()\n",
    "    df.printSchema()\n",
    "    df.show(20)\n",
    "\n",
    "def df_as_html(df, n: int = 5, right_align: bool = False, show_index: bool = False):\n",
    "    \"\"\"\n",
    "    HTML preview via pandas with no truncation. If right_align=True,\n",
    "    only numeric columns are right-justified; everything else is \n",
    "    explicitly left-aligned.\n",
    "    \"\"\"\n",
    "    pdf = df.limit(n).toPandas()\n",
    "    print(\"[INFO] Converting Spark → pandas for HTML display (rows:\", len(pdf), \")\")\n",
    "    print(\"[INFO] right_align (numeric columns):\", right_align)\n",
    "\n",
    "    with pd.option_context(\n",
    "        \"display.max_colwidth\", None,   \n",
    "        \"display.max_columns\", None,    \n",
    "        \"display.width\", None            \n",
    "    ):\n",
    "        styler = pdf.style if show_index else pdf.style.hide(axis=\"index\")\n",
    "\n",
    "        #   table alignment: left for both headers and cells\n",
    "        styler = styler.set_table_styles(\n",
    "            [\n",
    "                {\"selector\": \"th\", \"props\": [(\"text-align\", \"left\")]},\n",
    "                {\"selector\": \"td\", \"props\": [(\"text-align\", \"left\")]},\n",
    "            ],\n",
    "            overwrite=True,  # make this the baseline\n",
    "        )\n",
    "         \n",
    "        if right_align:\n",
    "            numeric_cols = list(pdf.select_dtypes(include=[\"number\"]).columns)\n",
    "            print(\"[INFO] Right-aligning numeric columns:\", numeric_cols)\n",
    "            if numeric_cols:\n",
    "                styler = styler.set_properties(subset=numeric_cols,\n",
    "                                               **{\"text-align\": \"right\"})\n",
    "        display(styler)\n",
    "\n",
    "def show_df(df, n: int = 10, name: str = \"\", right_align: bool = False):\n",
    "    \"\"\"\n",
    "    Print schema, \n",
    "    show an HTML sample,\n",
    "    and row count.\n",
    "    \"\"\"\n",
    "    hprint()\n",
    "    print(\"name : \",name)\n",
    "    df.printSchema()\n",
    "    print(\"[check] sample:\")\n",
    "    df_as_html(df, n=n, right_align=right_align)\n",
    "\n",
    "def write_parquet(df, dir_as_path: str, df_name:str = \"\"):    \n",
    "    funct_time = time.time()\n",
    "    path = _normalise_dir(dir_as_path)\n",
    "    print(f\"[file] write_parquet  : {path}\")\n",
    "    try:      \n",
    "        show_df(df,df_name)\n",
    "    except Exception as e:\n",
    "        print(\"[catch] sample failed:\", e)\n",
    "        os.system(f'hdfs dfs -rm -r -f \"{path}\"')   # idempotent cleanup\n",
    "    df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
    "    os.system(f'hdfs dfs -ls -R \"{path}\"')\n",
    "    funct_time = time.time() - funct_time \n",
    "    print(f\"[time] write_parquet (min)   : {funct_time/60:5.2f}\")\n",
    "    print(f\"[time] write_parquet (sec)   : {funct_time:5.2f}\")\n",
    "\n",
    "def has_parquet(dir_as_path: str) -> bool:\n",
    "    path   = _normalise_dir( dir_as_path)\n",
    "    marker = path + '_SUCCESS'\n",
    "    #print(\"\\n[check] dir_path:\", dir_path)\n",
    "    #print(\"\\n[check] path    :\", path)\n",
    "    print(\"\\n[check] marker  :\", marker)\n",
    "    rc = os.system(f'hdfs dfs -test -e \"{marker}\"')\n",
    "    print(\"[check] rc:\", rc, \"->\", (\"exists\" if rc == 0 else \"missing\"))\n",
    "    return (rc == 0)\n",
    "\n",
    "def _to_spark(df_like, schema=None):\n",
    "    \"\"\"\n",
    "    Return a Spark DataFrame  .\n",
    "    \"\"\"\n",
    "    if isinstance(df_like, SparkDF):\n",
    "        return df_like\n",
    "    return spark.createDataFrame(df_like, schema=schema) if schema else spark.createDataFrame(df_like)\n",
    "\n",
    "def ensure_dir(path: str) -> str:\n",
    "    \"\"\"\n",
    "    ensures that path is a path \n",
    "    and not representing a file;\n",
    "    add trailing slash if needed\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        raise ValueError(\"Path is None\")\n",
    "    path = _normalise_dir(path)\n",
    "#   print(\"ensure_dir -> \",path)\n",
    "    return path\n",
    "\n",
    "def _normalise_dir(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensure trailing slash so we point to\n",
    "    the dataset directory (not a file)\n",
    "    \"\"\"\n",
    "    return s if s.endswith(\"/\") else s + \"/\"\n",
    "\n",
    "def _success_exists(target_dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check for the Hadoop/Spark _SUCCESS marker;  \n",
    "    \"\"\"\n",
    "    jvm = spark._jvm\n",
    "    hconf = spark._jsc.hadoopConfiguration()\n",
    "    try:\n",
    "        uri = jvm.java.net.URI(target_dir)\n",
    "        fs = jvm.org.apache.hadoop.fs.FileSystem.get(uri, hconf)\n",
    "        success = jvm.org.apache.hadoop.fs.Path(target_dir + \"_SUCCESS\")\n",
    "        exists = fs.exists(success)\n",
    "        print(f\"[status] _SUCCESS check at: {target_dir}_SUCCESS -> {exists}\")\n",
    "        return bool(exists)\n",
    "    except Exception as e:\n",
    "        print(f\"[status] _SUCCESS check failed ({e}); attempting read-probe …\")\n",
    "        try:\n",
    "            spark.read.parquet(target_dir).limit(1).count()\n",
    "            print(f\"[dewstatus59] read-probe succeeded at: {target_dir}\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"[status] read-probe failed ({e2}); treating as not existing.\")\n",
    "            return False\n",
    "\n",
    "def _count_unique_ids(df: DataFrame) -> int:\n",
    "    return normalise_ids(df).count()\n",
    "\n",
    " \n",
    "# Where to save diagnostics (use your username as requested)\n",
    "\n",
    "# Back-compat aliases hack to account for non-disciplined naming un-convention\n",
    "# hack \n",
    "_ids       = normalise_ids\n",
    "canon_ids  = normalise_ids\n",
    "_canon_ids = normalise_ids\n",
    "\n",
    "#print(\"[TEST] Using _canon_ids:\", _canon_ids(stations).count())\n",
    "#print(\"[TEST] Using canon_ids :\", canon_ids(stations).count())\n",
    "#print(\"[TEST] Using _ids      :\", _ids(stations).count())\n",
    "\n",
    "# : pairwise city distances in km using Spark built-ins \n",
    "def pairwise_city_distances_spark(cities, radius_km=6371.0):\n",
    "    \"\"\"\n",
    "    cities: list[tuple[str, float, float]] -> [(name, lat_deg, lon_deg), ...]\n",
    "    returns: Spark DataFrame with columns:\n",
    "             city_a, city_b, haversine_km, slc_km, delta_km, delta_pct\n",
    "    \"\"\"\n",
    "  #  from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        raise RuntimeError(\"No active Spark session.\")\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "        ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.city\").alias(\"city_a\"),\n",
    "                       F.col(\"b.city\").alias(\"city_b\"),\n",
    "                       F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\")))\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\"));  lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav_km = R * c_term\n",
    "\n",
    "    cos_val = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cos_val = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cos_val))\n",
    "    slc_km = R * F.acos(cos_val)\n",
    "\n",
    "    delta_km  = F.abs(hav_km - slc_km)\n",
    "    delta_pct = F.when(hav_km == 0, F.lit(0.0)).otherwise(delta_km / hav_km * 100.0)\n",
    "\n",
    "    out_df = (pairs\n",
    "              .withColumn(\"haversine_km\", F.round(hav_km, 2))\n",
    "              .withColumn(\"slc_km\",       F.round(slc_km, 2))\n",
    "              .withColumn(\"delta_km\",     F.round(delta_km, 4))\n",
    "              .withColumn(\"delta_pct\",    F.round(delta_pct, 6))\n",
    "              .select(\"city_a\", \"city_b\", \"haversine_km\", \"slc_km\", \"delta_km\", \"delta_pct\")\n",
    "              .orderBy(\"haversine_km\"))\n",
    "    return out_df\n",
    "\n",
    "# --- Timing helpers for Spark & pure Python (no extra deps)\n",
    "\n",
    "def benchmark_python_distances(cities, radius_km=6371.0, repeats=50000):\n",
    "    \"\"\"\n",
    "    cities: [(name, lat_deg, lon_deg), ...]  (3 cities => 3 pairs)\n",
    "    repeats: loop count to make timings stable\n",
    "    returns: dict with seconds for haversine/slc\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(cities)):\n",
    "        for j in range(i+1, len(cities)):\n",
    "            (_, lat1, lon1), (_, lat2, lon2) = cities[i], cities[j]\n",
    "            pairs.append((lat1, lon1, lat2, lon2))\n",
    "\n",
    "    # haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            dφ, dλ = (φ2 - φ1), (λ2 - λ1)\n",
    "            a = sin(dφ/2)**2 + cos(φ1)*cos(φ2)*sin(dλ/2)**2\n",
    "            c = 2*atan2(sqrt(a), sqrt(1 - a))\n",
    "            _ = radius_km * c\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # spherical law of cosines (SLC)\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        for lat1, lon1, lat2, lon2 in pairs:\n",
    "            φ1, λ1, φ2, λ2 = map(radians, (lat1, lon1, lat2, lon2))\n",
    "            cosv = sin(φ1)*sin(φ2) + cos(φ1)*cos(φ2)*cos(λ2 - λ1)\n",
    "            cosv = max(-1.0, min(1.0, cosv))\n",
    "            _ = radius_km * acos(cosv)\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"python_haversine_sec\": t1 - t0,\n",
    "        \"python_slc_sec\":       t3 - t2,\n",
    "        \"repeats\": repeats,\n",
    "        \"pairs\": len(pairs),\n",
    "    }\n",
    "\n",
    "def _parse_ls_bytes(line): \n",
    "    parts = line.split()\n",
    "    if len(parts) < 8:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[4])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def _parse_du_bytes(line):\n",
    "    parts = line.split()\n",
    "    if len(parts) < 2:\n",
    "        return None, None\n",
    "    try:\n",
    "        size = int(parts[0])\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "    return size, parts[-1]\n",
    "\n",
    "def du_bytes(path):\n",
    "    lines = get_ipython().getoutput(f'hdfs dfs -du \"{path}\"')\n",
    "    total = 0\n",
    "    for ln in lines:\n",
    "        parts = ln.split()\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                total += int(parts[0])\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return total\n",
    "    \n",
    "def benchmark_spark_distances(cities, radius_km=6368.6, repeats=3):\n",
    "    \"\"\"\n",
    "    Uses Spark built-ins only. Measures full execution\n",
    "    time by forcing an action.\n",
    "    \n",
    "    returns: dict with seconds for haversine/slc and\n",
    "    row counts used.\n",
    "    \n",
    "    For the radius:\n",
    "    \n",
    "    The Earth is slightly flattened, so the geocentric \n",
    "    radius depends on latitude.  For context: \n",
    "    \n",
    "    * equatorial radius = 6,378.137 km; \n",
    "    * polar radius      = 6,356.752 km \n",
    "    \n",
    "    Across New Zealand's latitudes (≈36–47°S), using the\n",
    "    WGS-84 ellipsoid, you get roughly:\n",
    "\n",
    "    Auckland (37°S):       ~6,370.4 km\n",
    "    Christchurch (43.5°S): ~6,368.0 km\n",
    "    Dunedin (45.9°S):      ~6,367.2 km\n",
    "    __________________________________\n",
    "    Wellington (41°S):     ~6,369.0 km\n",
    "    mean                  ≈ 6,368.6 km\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        from pyspark.sql import SparkSession, functions as F, types as T\n",
    "    except Exception:\n",
    "        return None  # no Spark therefore save cannot run in vs code\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        return None\n",
    "\n",
    "    # build pairs once and cache\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"city\", T.StringType(), False),\n",
    "        T.StructField(\"lat\",  T.DoubleType(), False),\n",
    "        T.StructField(\"lon\",  T.DoubleType(), False),\n",
    "    ])\n",
    "    df = spark.createDataFrame(cities, schema)\n",
    "    a, b = df.alias(\"a\"), df.alias(\"b\")\n",
    "    pairs = (a.join(b, F.col(\"a.city\") < F.col(\"b.city\"))\n",
    "               .select(F.col(\"a.lat\").alias(\"lat1\"),\n",
    "                       F.col(\"a.lon\").alias(\"lon1\"),\n",
    "                       F.col(\"b.lat\").alias(\"lat2\"),\n",
    "                       F.col(\"b.lon\").alias(\"lon2\"))\n",
    "               .cache())\n",
    "    _ = pairs.count()\n",
    "\n",
    "    R = F.lit(float(radius_km))\n",
    "    lat1 = F.radians(F.col(\"lat1\")); lat2 = F.radians(F.col(\"lat2\"))\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = F.radians(F.col(\"lon2\") - F.col(\"lon1\"))\n",
    "\n",
    "    # Haversine expr\n",
    "    a_term = F.sin(dlat/2)**2 + F.cos(lat1)*F.cos(lat2)*F.sin(dlon/2)**2\n",
    "    c_term = 2*F.atan2(F.sqrt(a_term), F.sqrt(1 - a_term))\n",
    "    hav    = R * c_term\n",
    "\n",
    "    # SLC expr\n",
    "    cosv = F.sin(lat1)*F.sin(lat2) + F.cos(lat1)*F.cos(lat2)*F.cos(dlon)\n",
    "    cosv = F.greatest(F.lit(-1.0), F.least(F.lit(1.0), cosv))\n",
    "    slc = R * F.acos(cosv)\n",
    "\n",
    "    # time Haversine\n",
    "    t0 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(hav.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t1 = perf_counter()\n",
    "\n",
    "    # time SLC\n",
    "    t2 = perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = pairs.select(slc.alias(\"d\")).agg(F.sum(\"d\")).collect()\n",
    "    t3 = perf_counter()\n",
    "\n",
    "    return {\n",
    "        \"spark_pairs\": pairs.count(),\n",
    "        \"spark_repeats\": repeats,\n",
    "        \"spark_haversine_sec\": t1 - t0,\n",
    "        \"spark_slc_sec\":       t3 - t2,\n",
    "    }\n",
    "\n",
    "\n",
    "def list_hdfs_csvgz_files(hdfs_path = WASBS_DATA, debug=False):\n",
    "    \"\"\"\n",
    "    Lists .csv.gz files from an HDFS directory, extracting year and file size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hdfs_path : str\n",
    "        The HDFS path to list, e.g. 'wasbs://campus-data@...'\n",
    "    debug : bool, optional\n",
    "        If True, prints intermediate parsing steps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuple\n",
    "        A list of (year, size) tuples for each .csv.gz file.\n",
    "    \"\"\"\n",
    "    cmd = f\"hdfs dfs -ls {hdfs_path}\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    lines = result.stdout.strip().split(\"\\n\")\n",
    "    rows = []\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if debug:\n",
    "            print(\"Parts:\", parts)\n",
    "        if len(parts) < 6:\n",
    "            continue\n",
    "        try:\n",
    "            size = int(parts[2])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        path = parts[-1]\n",
    "        if path.endswith(\".csv.gz\"):\n",
    "            try:\n",
    "                year = int(path.split(\"/\")[-1].replace(\".csv.gz\", \"\"))\n",
    "                rows.append((year, size))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    if debug:\n",
    "        print(\"_____________________________________________________\")\n",
    "        print(\"Sample parsed rows:\", rows[:5])\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def explore_hdfs_directory_tree(root_path, max_depth=2, show_sizes=True):\n",
    "    \"\"\"\n",
    "    Explore and visualise any HDFS or WASBS directory tree.\n",
    "    Works with any file types (not just .parquet).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_path : str\n",
    "        HDFS/WASBS path to explore.\n",
    "    max_depth : int\n",
    "        Maximum depth to traverse.\n",
    "    show_sizes : bool\n",
    "        Whether to display file sizes in MB.\n",
    "    \"\"\"\n",
    "\n",
    "    console = Console()\n",
    "\n",
    "    def build_tree(path, tree, depth=0):\n",
    "        if depth >= max_depth:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Run the HDFS ls command\n",
    "            cmd = ['hdfs', 'dfs', '-ls', path]\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "\n",
    "            lines = result.stdout.strip().split('\\n')\n",
    "            if not lines:\n",
    "                tree.add(\"[dim]Empty directory[/dim]\")\n",
    "                return\n",
    "\n",
    "            # Skip 'Found N items' header\n",
    "            if lines[0].startswith(\"Found\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.split()\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "\n",
    "                permissions, _, _, size, date, time_str, _, name = parts[-8:]\n",
    "                item_name = name.split(\"/\")[-1] or name.split(\"/\")[-2]\n",
    "\n",
    "                if permissions.startswith(\"d\"):\n",
    "                    # Directory node\n",
    "                    subtree = tree.add(f\"[bold cyan]{item_name}/[/bold cyan]\")\n",
    "                    if depth + 1 < max_depth:\n",
    "                        build_tree(name, subtree, depth + 1)\n",
    "                else:\n",
    "                    # File node\n",
    "                    display_name = item_name\n",
    "                    if show_sizes and size.isdigit():\n",
    "                        size_mb = int(size) / (1024 ** 2)\n",
    "                        display_name += f\" ({size_mb:.2f} MB)\"\n",
    "                    tree.add(display_name)\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            tree.add(f\"[red]Error accessing {path}: {e}[/red]\")\n",
    "        except Exception as e:\n",
    "            tree.add(f\"[red]Unexpected error: {e}[/red]\")\n",
    "\n",
    "    # Start visualisation\n",
    "    console.print(\"=\" * 60)\n",
    "    console.print(f\"[bold white]DIRECTORY TREE FOR:[/bold white] [cyan]{root_path}[/cyan]\")\n",
    "    console.print(\"=\" * 60)\n",
    "\n",
    "    tree = Tree(f\"[green]{root_path}[/green]\")\n",
    "    build_tree(root_path, tree)\n",
    "    console.print(tree)\n",
    "    console.print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "def explore_hdfs_directory_tree(root_path, max_depth=3, show_sizes=True):\n",
    "    console = Console()\n",
    "\n",
    "    def build_tree(path, tree, depth=0):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"hdfs\", \"dfs\", \"-ls\", path],\n",
    "                capture_output=True, text=True, check=True\n",
    "            )\n",
    "            lines = [ln for ln in result.stdout.strip().split(\"\\n\") if ln and not ln.startswith(\"Found\")]\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.split()\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "\n",
    "                perms, size, name = parts[0], parts[4], parts[-1]\n",
    "                item_name = name.split(\"/\")[-1] or name.split(\"/\")[-2]\n",
    "\n",
    "                if perms.startswith(\"d\"):\n",
    "                    subtree = tree.add(f\"[bold cyan]{item_name}/[/bold cyan]\")\n",
    "                    build_tree(name, subtree, depth + 1)\n",
    "                else:\n",
    "                    size_mb = int(size)/(1024*1024) if size.isdigit() else 0\n",
    "                    label = f\"{item_name} ({size_mb:.2f} MB)\" if show_sizes else item_name\n",
    "                    tree.add(label)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            tree.add(f\"[red]Error accessing {path}: {e}[/red]\")\n",
    "\n",
    "    # ✅ Header and recursive tree printing belong *inside* the function\n",
    "    console.print(\"=\" * 60)\n",
    "    console.print(f\"[bold white]DIRECTORY TREE FOR:[/bold white] [cyan]{root_path}[/cyan]\")\n",
    "    console.print(\"=\" * 60)\n",
    "    tree = Tree(f\"[green]{root_path}[/green]\")\n",
    "    build_tree(root_path, tree)\n",
    "    console.print(tree)\n",
    "    console.print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def list_hdfs_all(hdfs_path):\n",
    "    \"\"\"List all files and directories under a given HDFS/WASBS path.\"\"\"\n",
    "    cmd = f\"hdfs dfs -ls -R {hdfs_path}\"  # -R for recursive\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    output = result.stdout.strip()\n",
    "    \n",
    "    if not output:\n",
    "        print(f\"[INFO] No files or directories found in {hdfs_path}\")\n",
    "    else:\n",
    "        print(f\"Listing for {hdfs_path}:\\n\")\n",
    "        print(output)\n",
    "\n",
    "\n",
    "def build_directory_tree_df(root_path=None, max_depth=3):\n",
    "    \"\"\"\n",
    "    build directory tree from hdfs/wasbs path and return as spark dataframe.\n",
    "    \n",
    "    parameters:\n",
    "        root_path (str): wasbs path to explore (defaults to WASBS_DATA)\n",
    "        max_depth (int): maximum depth to traverse\n",
    "        \n",
    "    returns:\n",
    "        spark dataframe with columns: level, path, name, type, size, parent_path\n",
    "    \"\"\"\n",
    "    if root_path is None:\n",
    "        root_path = WASBS_DATA\n",
    "        \n",
    "    print(f\"[info] building directory tree from: {root_path}\")\n",
    "    print(f\"[info] max depth: {max_depth}\")\n",
    "    \n",
    "    tree_data = []\n",
    "    \n",
    "    def explore_path(current_path, current_level, parent_path):\n",
    "        if current_level > max_depth:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"hdfs\", \"dfs\", \"-ls\", current_path],\n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            lines = result.stdout.strip().split(\"\\n\")\n",
    "            if lines and lines[0].startswith(\"Found\"):\n",
    "                lines = lines[1:]\n",
    "                \n",
    "            for line in lines:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                    \n",
    "                parts = line.split()\n",
    "                if len(parts) < 8:\n",
    "                    continue\n",
    "                    \n",
    "                permissions = parts[0]\n",
    "                size_str = parts[4]\n",
    "                full_path = parts[-1]\n",
    "                \n",
    "                # extract item name\n",
    "                item_name = full_path.rstrip('/').split('/')[-1]\n",
    "                if not item_name:\n",
    "                    item_name = full_path.split('/')[-2]\n",
    "                \n",
    "                # determine type and size\n",
    "                is_dir = permissions.startswith('d')\n",
    "                item_type = \"dir\" if is_dir else \"file\"\n",
    "                size_bytes = 0 if is_dir else (int(size_str) if size_str.isdigit() else 0)\n",
    "                \n",
    "                # add to tree data\n",
    "                tree_data.append({\n",
    "                    \"level\": current_level,\n",
    "                    \"path\": full_path,\n",
    "                    \"name\": item_name,\n",
    "                    \"type\": item_type,\n",
    "                    \"size\": size_bytes,\n",
    "                    \"parent_path\": parent_path\n",
    "                })\n",
    "                \n",
    "                # recurse into directories\n",
    "                if is_dir and current_level < max_depth:\n",
    "                    explore_path(full_path, current_level + 1, current_path)\n",
    "                    \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"[error] failed to access {current_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[error] unexpected error at {current_path}: {e}\")\n",
    "    \n",
    "    # start exploration from root\n",
    "    explore_path(root_path, 0, None)\n",
    "    \n",
    "    print(f\"[info] collected {len(tree_data)} items from directory tree\")\n",
    "    \n",
    "    # convert to spark dataframe\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"level\", T.IntegerType(), False),\n",
    "        T.StructField(\"path\", T.StringType(), False),\n",
    "        T.StructField(\"name\", T.StringType(), False),\n",
    "        T.StructField(\"type\", T.StringType(), False),\n",
    "        T.StructField(\"size\", T.LongType(), False),\n",
    "        T.StructField(\"parent_path\", T.StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = spark.createDataFrame(tree_data, schema=schema)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_tree_to_parquet(df, output_path):\n",
    "    \"\"\"\n",
    "    save directory tree dataframe to parquet.\n",
    "    \n",
    "    parameters:\n",
    "        df: spark dataframe with tree structure\n",
    "        output_path: wasbs path for output (should be in WASBS_USER)\n",
    "    \"\"\"\n",
    "    print(f\"[info] saving tree to: {output_path}\")\n",
    "    \n",
    "    # ensure trailing slash\n",
    "    if not output_path.endswith('/'):\n",
    "        output_path += '/'\n",
    "    \n",
    "    try:\n",
    "        df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        print(f\"[info] tree saved successfully to: {output_path}\")\n",
    "        \n",
    "        # verify with hdfs ls\n",
    "        result = subprocess.run(\n",
    "            [\"hdfs\", \"dfs\", \"-ls\", output_path],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        print(f\"[info] parquet contents:\\n{result.stdout}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[error] failed to save tree: {e}\")\n",
    "\n",
    "\n",
    "def display_tree_as_text(df, show_sizes=True):\n",
    "    \"\"\"\n",
    "    display directory tree dataframe in text format matching reference pdf.\n",
    "    \n",
    "    parameters:\n",
    "        df: spark dataframe with tree structure\n",
    "        show_sizes: whether to show file sizes in bytes\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DIRECTORY TREE STRUCTURE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # collect data sorted by level and path\n",
    "    tree_data = df.orderBy(\"level\", \"path\").collect()\n",
    "    \n",
    "    # build hierarchical display\n",
    "    path_to_children = {}\n",
    "    for row in tree_data:\n",
    "        parent = row.parent_path\n",
    "        if parent not in path_to_children:\n",
    "            path_to_children[parent] = []\n",
    "        path_to_children[parent].append(row)\n",
    "    \n",
    "    def print_tree(path, level=0, prefix=\"\", is_last=True):\n",
    "        \"\"\"recursively print tree structure\"\"\"\n",
    "        children = path_to_children.get(path, [])\n",
    "        \n",
    "        for i, child in enumerate(children):\n",
    "            is_last_child = (i == len(children) - 1)\n",
    "            \n",
    "            # determine connector characters\n",
    "            if level == 0:\n",
    "                connector = \"└── \" if is_last_child else \"├── \"\n",
    "                extension = \"    \" if is_last_child else \"│   \"\n",
    "            else:\n",
    "                connector = prefix + (\"└── \" if is_last_child else \"├── \")\n",
    "                extension = prefix + (\"    \" if is_last_child else \"│   \")\n",
    "            \n",
    "            # format item name\n",
    "            item_display = child.name\n",
    "            if child.type == \"dir\":\n",
    "                item_display += \"/\"\n",
    "            elif show_sizes and child.size > 0:\n",
    "                item_display += f\" ({child.size})\"\n",
    "            \n",
    "            # print the item\n",
    "            print(connector + item_display)\n",
    "            \n",
    "            # recurse for directories\n",
    "            if child.type == \"dir\":\n",
    "                print_tree(child.path, level + 1, extension, is_last_child)\n",
    "    \n",
    "    # start from root (items with no parent)\n",
    "    root_items = path_to_children.get(None, [])\n",
    "    for i, root_item in enumerate(root_items):\n",
    "        is_last = (i == len(root_items) - 1)\n",
    "        print(\"└── \" + root_item.name + (\"/\" if root_item.type == \"dir\" else \"\"))\n",
    "        if root_item.type == \"dir\":\n",
    "            print_tree(root_item.path, 1, \"    \" if is_last else \"│   \", is_last)\n",
    "    \n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a36c42b0-9d6d-4eb4-89df-590eb55cb051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________Clean up existing parquet files_________\n",
      "[cleanup] Listing files BEFORE cleanup:\n",
      "Found 2 items\n",
      "-rw-r--r--   1 dew59 supergroup          0 2025-10-08 08:29 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2/msd_directory_tree.parquet/_SUCCESS\n",
      "-rw-r--r--   1 dew59 supergroup        727 2025-10-08 08:29 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2/msd_directory_tree.parquet/part-00000-7069e22c-78cd-449b-967d-81d921730363-c000.snappy.parquet\n",
      "\n",
      "[info] To actually delete files, call: cleanup_parquet_files(cleanup=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# USE SPARINGLY - these are for diagnostics only\n",
    "# Set cleanup=True to actually delete files, or False to just list them \n",
    "# LEAVE cleanup=False after running this cell once! \n",
    "# if they have been created and are correct, change cleanup=False for quicker runs. \n",
    "cleanup_parquet_files(cleanup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586fca6e-683f-4426-b354-175f8eeb5000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___________started at: 2025.10.08 08:43___________\n",
      "Found 4 items\n",
      "drwxrwxrwx   -          0 1970-01-01 12:00 wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/audio\n",
      "drwxrwxrwx   -          0 1970-01-01 12:00 wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/genre\n",
      "drwxrwxrwx   -          0 1970-01-01 12:00 wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/main\n",
      "drwxrwxrwx   -          0 1970-01-01 12:00 wasbs://campus-data@madsstorage002.blob.core.windows.net/msd/tasteprofile\n",
      "12.9 G  12.9 G  wasbs://campus-data@madsstorage002.blob.core.windows.net/msd\n",
      "Found 1 items\n",
      "drwxr-xr-x   - dew59 supergroup          0 2025-10-08 08:29 wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2/msd_directory_tree.parquet\n",
      "727  727  wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2\n"
     ]
    }
   ],
   "source": [
    "# overall time metric\n",
    "start_notebook = time.time() \n",
    "start_time = datetime.fromtimestamp(start_notebook).strftime(\"%Y.%m.%d %H:%M\")\n",
    "\n",
    "hprint(f\"started at: {start_time}\")\n",
    "# Use the hdfs command to explore the data in Azure Blob Storage\n",
    "#!hdfs dfs -ls wasbs://{azure_data_container_name}@{azure_account_name}.blob.core.windows.net/msd/\n",
    "!hdfs dfs -ls    -h {WASBS_DATA} \n",
    "!hdfs dfs -du -s -h {WASBS_DATA} \n",
    "!hdfs dfs -ls    -h {WASBS_USER} \n",
    "!hdfs dfs -du -s -h {WASBS_USER} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c741463-9dfb-4c39-ba88-1d35afcecedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw result: ['13854359805  13854359805  wasbs://campus-data@madsstorage002.blob.core.windows.net/msd']\n",
      "\n",
      "firstpass size (bytes): 13854359805\n",
      "firstpass size (MB)   : 13212.547\n",
      "\n",
      "[time]   Cell time (sec)   :  4.83\n",
      "[time]   Cell time (min)   :  0.08\n"
     ]
    }
   ],
   "source": [
    "cell_time = time.time() \n",
    "result = get_ipython().getoutput(f\"hdfs dfs -du -s {WASBS_DATA}\") \n",
    "\n",
    "print(\"Raw result:\", result)\n",
    "print()\n",
    "data_size_bytes = int(result[0].split()[0])\n",
    "print(\"firstpass size (bytes):\", data_size_bytes)\n",
    "print(f\"firstpass size (MB)   : {data_size_bytes / (1024**2):.3f}\")\n",
    " \n",
    "lines = get_ipython().getoutput(f\"hdfs dfs -ls {WASBS_DATA}\")\n",
    "print()\n",
    "#other_size_bytes = 0\n",
    "#for line in lines:\n",
    "#    parts = line.split()\n",
    "#    if len(parts) >= 6 and parts[0].startswith('-'):   # file, not directory\n",
    "#        size = int(parts[2])                           # file size is parts[2] in your env\n",
    "#        other_size_bytes += size\n",
    "#print()\n",
    "#print(\"_____________________________________________________\") \n",
    "#print(f\"[result] daily size (bytes): {daily_size_bytes:,d}\")\n",
    "#print(f\"[result] daily size (MB)   : {daily_size_bytes / (1024**2):.2f}\")\n",
    "#print(f\"[result] meta-data (bytes) : {other_size_bytes:,d}\")\n",
    "#print(f\"[result] meta-data (MB)    : {other_size_bytes / (1024**2):.2f}\")\n",
    "\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"[time]   Cell time (sec)   : {cell_time:5.2f}\") \n",
    "print(f\"[time]   Cell time (min)   : {cell_time/60:5.2f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc5edd-f44d-41c9-8add-129638a53a8f",
   "metadata": {},
   "source": [
    "### Q1 - Directory Tree Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab232eb-ade2-4400-b722-11708d97ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_______________Q1 - Directory Tree_______________\n",
      "[status] _SUCCESS check at: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2/msd_directory_tree.parquet/_SUCCESS -> True\n",
      "[info] directory tree parquet exists in WASBS_USER\n",
      "[info] path: wasbs://campus-user@madsstorage002.blob.core.windows.net/dew59-A2/msd_directory_tree.parquet/\n",
      "[info] reading directory tree from parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] loaded 0 items from directory tree\n",
      "\n",
      "======================================================================\n",
      "DIRECTORY TREE STRUCTURE\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "\n",
      "\n",
      "__________________________________________________\n",
      "name :  MSD Directory Structure\n",
      "root\n",
      " |-- level: integer (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- size: long (nullable = true)\n",
      " |-- parent_path: string (nullable = true)\n",
      "\n",
      "[check] sample:\n",
      "[INFO] Converting Spark → pandas for HTML display (rows: 0 )\n",
      "[INFO] right_align (numeric columns): True\n",
      "[INFO] Right-aligning numeric columns: ['level', 'size']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ed44a th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_ed44a td {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ed44a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_ed44a_level0_col0\" class=\"col_heading level0 col0\" >level</th>\n",
       "      <th id=\"T_ed44a_level0_col1\" class=\"col_heading level0 col1\" >path</th>\n",
       "      <th id=\"T_ed44a_level0_col2\" class=\"col_heading level0 col2\" >name</th>\n",
       "      <th id=\"T_ed44a_level0_col3\" class=\"col_heading level0 col3\" >type</th>\n",
       "      <th id=\"T_ed44a_level0_col4\" class=\"col_heading level0 col4\" >size</th>\n",
       "      <th id=\"T_ed44a_level0_col5\" class=\"col_heading level0 col5\" >parent_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff7c585760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________Example Queries_________________\n",
      "\n",
      "[query] all .csv.gz files:\n",
      "+----+----+----+\n",
      "|name|size|path|\n",
      "+----+----+----+\n",
      "+----+----+----+\n",
      "\n",
      "\n",
      "[query] audio folder contents:\n",
      "+-----+----+----+----+\n",
      "|level|name|type|size|\n",
      "+-----+----+----+----+\n",
      "+-----+----+----+----+\n",
      "\n",
      "[time] cell time (sec): 11.81\n",
      "[time] cell time (min):  0.20\n"
     ]
    }
   ],
   "source": [
    "# Q1(a) - Get the file structure of the whole MSD dataset\n",
    "# Save hdfs ls -R output to a text file for inspection and parsing\n",
    "get_ipython().system(f'hdfs dfs -ls -R {WASBS_DATA} > data_structure.txt')\n",
    "print(f\"[info] Saved directory structure to: data_structure.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410f2b5",
   "metadata": {},
   "source": [
    "### Q1(a) - Directory Tree Structure\n",
    "\n",
    "The MSD dataset has the following structure:\n",
    "\n",
    "```\n",
    "└── msd/\n",
    "    ├── audio/\n",
    "    │   ├── attributes/\n",
    "    │   │   ├── msd-jmir-area-of-moments-all-v1.0.attributes.csv\n",
    "    │   │   ├── msd-jmir-lpc-all-v1.0.attributes.csv\n",
    "    │   │   ├── msd-jmir-methods-of-moments-all-v1.0.attributes.csv\n",
    "    │   │   ├── msd-jmir-mfcc-all-v1.0.attributes.csv\n",
    "    │   │   ├── msd-jmir-spectral-all-v1.0.attributes.csv\n",
    "    │   │   ├── msd-jmir-spectral-derivatives-all-v1.0.attributes.csv\n",
    "    │   │   ├── msd-marsyas-timbral-v1.0.attributes.csv\n",
    "    │   │   ├── msd-mvd-v1.0.attributes.csv\n",
    "    │   │   ├── msd-rh-v1.0.attributes.csv\n",
    "    │   │   ├── msd-rp-v1.0.attributes.csv\n",
    "    │   │   ├── msd-ssd-v1.0.attributes.csv\n",
    "    │   │   ├── msd-trh-v1.0.attributes.csv\n",
    "    │   │   └── msd-tssd-v1.0.attributes.csv\n",
    "    │   ├── features/\n",
    "    │   │   ├── msd-jmir-area-of-moments-all-v1.0.csv/\n",
    "    │   │   ├── msd-jmir-lpc-all-v1.0.csv/\n",
    "    │   │   ├── msd-jmir-methods-of-moments-all-v1.0.csv/\n",
    "    │   │   ├── msd-jmir-mfcc-all-v1.0.csv/\n",
    "    │   │   ├── msd-jmir-spectral-all-v1.0.csv/\n",
    "    │   │   ├── msd-jmir-spectral-derivatives-all-v1.0.csv/\n",
    "    │   │   ├── msd-marsyas-timbral-v1.0.csv/\n",
    "    │   │   ├── msd-mvd-v1.0.csv/\n",
    "    │   │   ├── msd-rh-v1.0.csv/\n",
    "    │   │   ├── msd-rp-v1.0.csv/\n",
    "    │   │   ├── msd-ssd-v1.0.csv/\n",
    "    │   │   ├── msd-trh-v1.0.csv/\n",
    "    │   │   └── msd-tssd-v1.0.csv/\n",
    "    │   └── statistics/\n",
    "    │       └── sample_properties.csv.gz\n",
    "    ├── genre/\n",
    "    │   ├── msd-MAGD-genreAssignment.tsv\n",
    "    │   ├── msd-MASD-styleAssignment.tsv\n",
    "    │   └── msd-topMAGD-genreAssignment.tsv\n",
    "    ├── main/\n",
    "    │   └── summary/\n",
    "    │       ├── analysis.csv.gz\n",
    "    │       └── metadata.csv.gz\n",
    "    └── tasteprofile/\n",
    "        ├── mismatches/\n",
    "        │   ├── sid_matches_manually_accepted.txt\n",
    "        │   └── sid_mismatches.txt\n",
    "        └── triplets.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1(a) continued - Generate and display directory tree PNG\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Add code directory to path so we can import the module\n",
    "if '../code' not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath('../code'))\n",
    "\n",
    "from generate_tree_png import create_tree_png\n",
    "\n",
    "png_path = 'report/supplementary/msd_directory_tree.png'\n",
    "\n",
    "# Test if PNG exists, generate if missing\n",
    "if not os.path.exists(png_path):\n",
    "    print(f\"[info] PNG not found at {png_path}, generating it...\")\n",
    "    create_tree_png()\n",
    "else:\n",
    "    print(f\"[info] PNG already exists at: {png_path}\")\n",
    "\n",
    "# Display the PNG\n",
    "print(f\"\\n[info] Displaying: {png_path}\")\n",
    "display(Image(filename=png_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1(b) - Parse the structure file and calculate summary statistics\n",
    "cell_time = time.time()\n",
    "\n",
    "hprint(\"Q1(b) - Summary Statistics\")\n",
    "\n",
    "import re\n",
    "\n",
    "# Parse hdfs ls -R output to extract size and path\n",
    "def parse_ls_line(line):\n",
    "    \"\"\"Parse a single line from hdfs ls -R output\"\"\"\n",
    "    # Format: permissions replication user group size date time path\n",
    "    # Example: -rw-r--r--   3 hdfs supergroup   1051 2024-01-15 10:30 /path/to/file\n",
    "    parts = line.split()\n",
    "    if len(parts) < 8:\n",
    "        return None\n",
    "    \n",
    "    permissions = parts[0]\n",
    "    size_str = parts[4]\n",
    "    path = parts[-1]\n",
    "    \n",
    "    # Only process files (not directories)\n",
    "    if not permissions.startswith('d'):\n",
    "        try:\n",
    "            size = int(size_str)\n",
    "            return {'size': size, 'path': path, 'is_dir': False}\n",
    "        except ValueError:\n",
    "            return None\n",
    "    else:\n",
    "        return {'size': 0, 'path': path, 'is_dir': True}\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Read and parse the data structure file\n",
    "try:\n",
    "    with open(\"data_structure.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    file_count = 0\n",
    "    dir_count = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        parsed = parse_ls_line(line.strip())\n",
    "        if parsed:\n",
    "            if parsed['is_dir']:\n",
    "                dir_count += 1\n",
    "            else:\n",
    "                file_count += 1\n",
    "                total_size += parsed['size']\n",
    "    \n",
    "    print(f\"\\n[summary] directories: {dir_count}\")\n",
    "    print(f\"[summary] files: {file_count}\")\n",
    "    print(f\"[summary] total size: {total_size:,} bytes ({total_size/(1024**3):.2f} GB)\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"[error] data_structure.txt not found. Please run the previous cell first.\")\n",
    "except Exception as e:\n",
    "    print(f\"[error] Failed to parse structure file: {e}\")\n",
    "\n",
    "cell_time = time.time() - cell_time\n",
    "print(f\"\\n[time] cell time (sec): {cell_time:5.2f}\")\n",
    "print(f\"[time] cell time (min): {cell_time/60:5.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2318c1-9ef3-4db9-a5cb-7d028114864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    " \n",
    "stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47253f6c-7c2c-47f1-a55c-a16210d00c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
